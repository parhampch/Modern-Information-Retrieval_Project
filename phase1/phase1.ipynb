{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIR_phase1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5O3xz7oPosg"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;font-family:BNazanin,\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=5>\n",
        "<p></p><p></p>\n",
        "بسمه تعالی\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<font>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر فاطمه لشکری\n",
        "</font>\n",
        "<p></p>\n",
        "<br>\n",
        "<font>\n",
        "<b>فاز اول پروژه</b>\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "موعد تحویل: ۸ آبان ۱۴۰۰\n",
        "<br>\n",
        "<font size=4.8>\n",
        "دستیاران آموزشی مربوط به این فاز: سینا کاظمی - پارسا اسکندر\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<font>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "<br>\n",
        "<br>\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i47HeS9NRU7B"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "مقدمه\n",
        "</h2>\n",
        "<p>\n",
        "هدف فاز اول پروژه پیاده سازی یک سیستم بازیابی اطلاعات است.\n",
        "این فاز پروژه از ۴ مرحله تشکیل شده است.\n",
        "بخش اول به پیش پردازش متنی داده ها می پردازد که شامل نرمال سازی متن (Normalization) ، جداسازی لغات (Tokenization) ، حذف لغات پرتکرار و ... می باشد. بخش دوم مربوط به نمایه سازی (indexing) خواهد بود. در بخش سوم باید روی این نمایه فشرده سازی صورت بگیرد. در بخش چهارم باید پرسمان ورودی کاربر را تصحیح کنید.\n",
        "</p>\n",
        "<p>\n",
        "۲ سری مجموعه دادگان در اختیار شما قرار گرفته است که یکی مربوط به زبان فارسی ( persian_dataset.csv ) و دیگری مربوط به زبان انگلیسی ( english_dataset.csv ) می باشد.\n",
        "مجموعه فارسی بخشی از مستندات ویکی پدیای فارسی می باشد و مجموعه انگلیسی مربوط به اخبار است.\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi2P02O_RviE"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "بخش اول ( پیش پردازش اولیه متن )\n",
        "</h2>\n",
        "<p>\n",
        "در این بخش از پروژه باید ابتدا مجموعه فایل هایی که در اختیارتان قرار گرفته است را بخوانید، برای خواندن و کار کردن با فایل‌ها پیشنهاد می‌شود از کتابخانه‌ی \n",
        "<a href=\"https://pandas.pydata.org/\">\n",
        "pandas\n",
        "</a> \n",
        "استفاده کنید.\n",
        "این کتابخانه توابعی برای \n",
        "<a href=\"https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\">\n",
        "خواندن\n",
        "</a> \n",
        "و \n",
        "<a href=\"https://towardsdatascience.com/pandas-dataframe-basics-3c16eb35c4f3\">\n",
        "کار کردن\n",
        "</a> \n",
        "راحت‌تر با دیتاها در اختیار شما قرار می‌دهد .\n",
        "\n",
        "\n",
        " سپس به ترتیب مراحل پیش پردازش متنی که در ادامه آمده است را روی آنها اعمال کنید. برای کار های پیش رو می توانید از کتابخانه های \n",
        "<a href=\"https://www.nltk.org/\">\n",
        "NLTK\n",
        "</a>\n",
        " برای زبان انگلیسی و هضم ( \n",
        "   <a href=\"http://www.sobhe.ir/hazm/\">\n",
        "hazm\n",
        "</a> \n",
        " ) برای زبان فارسی استفاده کنید\n",
        "</p>\n",
        "<ul>\n",
        "<li>\n",
        "نرمال سازی متنی (normalization): برای یکسان سازی متون می توانید از توابع معرفی شده استفاده کنید. اما در صورتی که میخواهید خودتان پیاده سازی کنید باید پیاده \n",
        "سازیتان شامل برگرداندن لغات به ریشه (stemming)\n",
        "، case folding ( برای یکسان سازی متون انگلیسی ) و بقیه موارد مطرح شده در درس باشد.\n",
        "</li>\n",
        "<li>\n",
        "جداسازی (tokenization): برای اینکار میتوانید از کتابخانه های معرفی شده در بالا استفاده نمایید\n",
        "</li>\n",
        "<li>\n",
        "حذف علائم نگارشی: هر متنی در هر زبانی یه سری علائم نگارشی دارد که می بایست حذف شوند مانند: ویرگول ، دونقطه و ...\n",
        "</li>\n",
        "<li>\n",
        "یافتن و حذف لغات پرتکرار (stopwords): در این بخش، حذف درصد معقولی از لغات پرتکرار مورد نظر است. برای این منظور لازم است تا همه متن را پردازش کنید و نسبت به حجم متن، کلماتی که پرتکرار هستند را نمایش دهید. این نسبت را طوری در نظر بگیرید که کلمات پرتکرار به دست آمده تا حد خوبی منطقی و کافی باشند.\n",
        "</li>\n",
        "<li>\n",
        "برگرداندن کلمات به ریشه (stemming, lemmatizing): در نهایت کلمات را به حالت ساده و پایه آنها تبدیل کنید.\n",
        "</li>\n",
        "</ul>\n",
        "<p>\n",
        "برای پیاده سازی این بخش تابع پیاده سازی prepare_text را پر کنید.\n",
        "برای نمایش لغات پرتکرار می توانید از هیستوگرام و یا لیست ساده ای از کلمات استفاده کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KClkZkGvP9c0",
        "outputId": "59a04cb3-fa03-43b6-8347-3e1fcef5802d"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 316 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 32.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=8b37189d8b7fd886a9bc71c43eaa027f8e5defc043f930484c751ef0836b0dd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154800 sha256=10e330af948e4127eaeb57291dff7706bf033b6ef085ae99d018034a25778b8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02BS2jjM7c4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a6a55c-9a99-46c5-a457-37e69747c7a4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import json\n",
        "import os\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8hkTHeZ74JT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7a264d46-5e2d-4eff-d930-7253729aaaf9"
      },
      "source": [
        "# read dataset\n",
        "english_data = pd.read_csv('/content/sample_data/english_dataset.csv')\n",
        "# persian_data = pd.read_csv('persian_dataset.csv')\n",
        "english_data.head()\n",
        "#english_data.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               text\n",
              "0           0  ...  Daniel Greenfield, a Shillman Journalism Fello...\n",
              "1           1  ...  Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
              "2           2  ...  U.S. Secretary of State John F. Kerry said Mon...\n",
              "3           3  ...  — Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
              "4           4  ...  It's primary day in New York and front-runners...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVEWGRlgQ0PD"
      },
      "source": [
        "persian_data = pd.read_csv('persian_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5udG6tx7ji7"
      },
      "source": [
        "با استفاده از توابع خود پایتون و کتابخانه‌های داده شده، عملیات‌های خواسته شده را انجام می‌دهیم، شرح دقیق عملیات هر بخش، در بالای آن نوشته شده است"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3gsB7gzYIlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742e2647-b215-4433-ef54-06e80ae6b823"
      },
      "source": [
        "def prepare_text(raw_text):\n",
        "    \"\"\"\n",
        "    Preprocesses the text with tokenization, case folding, stemming and lemmatization\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_text : str\n",
        "        The title or plot of a movie\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of tokens\n",
        "    \"\"\"\n",
        "    # remove all of punctuations\n",
        "    tran_table = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
        "    raw_text = raw_text.translate(tran_table)\n",
        "\n",
        "    # case foldig\n",
        "    raw_text = raw_text.lower()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = nltk.word_tokenize(raw_text)\n",
        "\n",
        "    # stemming\n",
        "    # stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
        "    stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
        "    # stemmer = nltk.stem.PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # lemmatize\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='n') for token in tokens]\n",
        "\n",
        "    # TODO: tokenize, case_folding, stem, lemmatize\n",
        "    return tokens\n",
        "\n",
        "prepare_text(\"Edvard was a Runner. He was always running.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['edvard', 'be', 'a', 'run', 'he', 'be', 'alway', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS5kAXEKQ5lg"
      },
      "source": [
        "def prepare_text_persian(raw_text):\n",
        "    # remove all of punctuations\n",
        "    punctuations = list(string.punctuation)\n",
        "    punctuations.extend(['،', '؟', '؛'])\n",
        "    tran_table = str.maketrans(dict.fromkeys(punctuations))\n",
        "    raw_text = raw_text.translate(tran_table)\n",
        "\n",
        "    # convert all of bad spaces to half space\n",
        "    normalizer = Normalizer()\n",
        "    text = normalizer.normalize(raw_text)\n",
        "\n",
        "    # tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # stemming\n",
        "    stemmer = Stemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # lemmatize\n",
        "    lemmatizer = Lemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # TODO: tokenize, case_folding, stem, lemmatize\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrmBZ5Lp4XLJ",
        "outputId": "9494fbd4-6aac-4e2f-a5c3-ac5502211152"
      },
      "source": [
        "a = [1 ,3, 4]\n",
        "b = a.extend([5, 6, 7])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 4, 5, 6, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0joZDmDIYLh8"
      },
      "source": [
        "def get_stop_words():\n",
        "    \"\"\"Detects stop-words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary of terms with their repetition\n",
        "    \"\"\"\n",
        "    count_of_tokens = {}\n",
        "    for index, row in english_data.iterrows():\n",
        "      tokens = prepare_text(row['title'] + ' ' + row['text'])\n",
        "      for token in tokens:\n",
        "        if token in count_of_tokens:\n",
        "          count_of_tokens[token] += 1\n",
        "        else:\n",
        "          count_of_tokens[token] = 1\n",
        "    result = sorted(count_of_tokens.items(), key = lambda kv: kv[1], reverse=True)\n",
        "       \n",
        "    # TODO: find top most repetitive terms and report them\n",
        "    return result\n",
        "\n",
        "get_stop_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzpC6pjdSQ6U"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "بخش دوم ( نمایه سازی )\n",
        "</h2>\n",
        "<p>\n",
        "در این بخش پیاده سازی نمایه جایگاهی (Positional) و نمایه Bigram مطلوب است. برای نمایه جایگاهی باید به ازای هر لغت، لیستی از اسناد شامل آن لغت و جایگاه (ها) هر لغت در آن سند داشته باشید و برای نمایه Bigram نیز ترکیب های دوحرفی تمامی کلمات موجود در لغت نامه که این ترکیب در آنها موجود است را ذخیره کنید. این نمایه برای اصلاح پرسمان مورد استفاده قرار خواهد گرفت. نمایه شما باید پویا باشد یعنی با حذف سند از نمایه نیز حذف شود و با اضافه کردن سند یا اسناد در طول اجرای برنامه به نمایه اضافه شود. همچنین بعد از نمایه سازی باید قادر باشید نمایه را در فایلی ذخیره کرده و از آن بخوانید. پویا بودن نمایه و ذخیره سازی آن را برای هردو نوع نمایه در نظر بگیرید.\n",
        "</p>\n",
        "<p>\n",
        "نکات پیاده سازی:\n",
        "</br>\n",
        "برای سادگی پیاده سازی برای هر کارکرد خواسته شده در توضیحات بالا یک تابع پیاده سازی کنید. برای مثال دوتابع برای حذف و اضافه سند به نمایه و توابعی برای ذخیره سازی و بارگزاری نمایه ها و ... در نظر بگیرید.\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd3C2vpTagke"
      },
      "source": [
        "pos_ind, bigr_ind = {}, {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAde5fVztevI"
      },
      "source": [
        "این دو هر کدام دیکشنری هستند. اولی به ازای هر توکن یک دیکشنری از داک آیدی هایی که دارای این توکن هستند به پوزیشن  هایی در آن داک آیدی که این توکن وجود دارد است. همچنین دومی دیکشنری از ترکیب های دو حرفی به توکن هایی که حاوی این کلمات هستند است."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Mu9wPX6v0P"
      },
      "source": [
        "def get_bi(token):\n",
        "  res = []\n",
        "  bigr_text = token + \"$\"\n",
        "  for i in range(len(bigr_text) - 1):\n",
        "    bi = bigr_text[i] + bigr_text[i + 1]\n",
        "    res.append(bi)\n",
        "  return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagqUsA9wqJn"
      },
      "source": [
        "این تابع یک توکن را ورودی میگیرد و بخش های دو حرفی که در این توکن ظاهر شده است را در یک لیست بر میگرداند."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_yhojV9ZwTl"
      },
      "source": [
        "def insert_doc(doc, docID):\n",
        "  tokens = prepare_text(doc)\n",
        "  for ind, token in enumerate(tokens):\n",
        "    if token in pos_ind:\n",
        "      if docID in pos_ind[token]:\n",
        "        pos_ind[token][docID].add(ind)\n",
        "      else :\n",
        "        pos_ind[token][docID] = {ind}\n",
        "    else: \n",
        "      pos_ind[token] = {docID : {ind}}\n",
        "\n",
        "    bigrams = get_bi(token)\n",
        "    for bi in bigrams:\n",
        "      if not bi in bigr_ind:\n",
        "        bigr_ind[bi] = set()\n",
        "      bigr_ind[bi].add(token)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFWiQbTlxFVP"
      },
      "source": [
        "این تابع وظیفه اضافه کردن محتویات لازم برای یک داک جدید را به pos_ind و bigr_ind  بر عهده دارد.\n",
        "داک و داک آیدی را ورودی میگیرد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKq6c1mrZ4pd"
      },
      "source": [
        "def remove_doc(doc, docID):\n",
        "  tokens = prepare_text(doc)\n",
        "  for token in tokens:\n",
        "    pos_ind[token].pop(docID) # remove by binary search\n",
        "    if len(pos_ind[token]) == 0:\n",
        "      pos_ind.pop(token)\n",
        "      bigrams = get_bi(token)\n",
        "      for bi in bigrams:\n",
        "        bigr_ind[bi].remove(token)\n",
        "        if length(bigr_ind[bi]) == 0:\n",
        "          bigr_ind.pop(bi)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtJUQBTlx5eG"
      },
      "source": [
        "این تابع بر خلاف تابع بالایی یک داک و داک آیدی را خروجی میگیرد و محتویات مربوط به آن داک را از pos_ind و bigr_ind  برمیدارد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCYrCZV_2Zzb"
      },
      "source": [
        "remove_doc(english_data.iloc[3]['title'] + \" \" + english_data.iloc[3]['text'], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxlJtybd_FP7"
      },
      "source": [
        "import json\n",
        "\n",
        "def write_on_file():\n",
        "  with open('pos_ind.json', 'w') as convert_file:\n",
        "     convert_file.write(str(pos_ind))\n",
        "  with open('bigr_ind.json', 'w') as convert_file:\n",
        "     convert_file.write(str(bigr_ind))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l9_iMOHysIL"
      },
      "source": [
        "این تابع دو دیکشنری pos_ind و bigr_ind را تبدیل به استرینگ میکند و بر روی فایل مینویسد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSIXuVIW9b4d"
      },
      "source": [
        "import json\n",
        "import ast\n",
        "\n",
        "def read_file():\n",
        "  with open('pos_ind.json') as json_file:\n",
        "    pos_ind = ast.literal_eval(str(json_file.read()))\n",
        "  with open('bigr_ind.json') as json_file:\n",
        "    bigr_ind = ast.literal_eval(str(json_file.read()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTrEbTyfzOaM"
      },
      "source": [
        "این تابع با استفاده از لایبرری ایمپورت شده دو فایلی که حاوی محتویات دیکشنری ها هستند را میخواند و از روی آن دیکشنری ها را میسازد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Its0B-Ujb4"
      },
      "source": [
        "def initialize():\n",
        "  for ind, row in english_data.iterrows():\n",
        "    insert_doc(row['title'] + ' ' + row['text'], ind)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1dKke-NzjQB"
      },
      "source": [
        "این تابع تابع insert_doc را روی کل داده صدا میزند تا دیکشنری ها ساخته شوند."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z40oST7p1KbQ"
      },
      "source": [
        "read_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTa-OtxmJb_l"
      },
      "source": [
        "initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJBDwr7ZSQw8"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "بخش سوم ( فشرده سازی نمایه ها)\n",
        "</h2>\n",
        "در این بخش هدف، فشرده‌سازی نمایه‌های ساخته‌شده به دو روش \n",
        "variable code \n",
        "و\n",
        "gamma code \n",
        "است.\n",
        "\n",
        "<h3>\n",
        "نکات پیاده‌سازی \n",
        "</h3>\n",
        " هر دو روش پیاده‌سازی شود ولی برای ذخیره‌سازی نمایه‌ها در فایل و استفاده‌ از آن در بخش‌های بعد یکی از دو روش به دلخواه استفاده شود.\n",
        " <br>\n",
        "برای ذخیره نمایه در فایل می‌توانید از JSON \n",
        "یا\n",
        "pickle \n",
        "استفاده کنید.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdBYh7kj4HVo"
      },
      "source": [
        "این تابع با گرفتن یک لیست از نمایه‌ها، آن لیست را به یک لیست از فاصله بین نمایه‌ها تبدیل می‌کند."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQcU7VVrhzY9"
      },
      "source": [
        "def get_gaps_list(pos_ind): \n",
        "  return [pos_ind[0]]+[pos_ind[i]-pos_ind[i-1] for i in range(1,len(pos_ind))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNz6hqkM4Ud5"
      },
      "source": [
        "این بدون هیچ نوع فشرده‌سازی، صرفا نمایه‌ها را در یک فایل جیسون ذخیره می‌کند."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgWCl5RTJ94k"
      },
      "source": [
        "def no_compression_save_func(path, pos_ind):\n",
        "  with open(path, \"w\") as outfile:  \n",
        "    json.dump(pos_ind, outfile) \n",
        "    return os.path.getsize(path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9mGd0GR5q51"
      },
      "source": [
        "تابع نهایی تنها یک عدد را با روش گفته کد می‌کند. با استفاده از این تابع، در تابع میانی، یک لیست از نمایه‌های یک توکن را دریافت، و رشته نهایی که حاصل از فشرده شدن کل این لیست است را خروجی می‌دهیم. در نهایت در تابع اول، کل نمایه‌ها را ورودی گرفته و هر نمایه (پس از تبدیل به یک آرایه از فواصل) را با استفاده از تابع میانی فشرده کرده و در فایل جیسون ذخیره می‌کنیم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8qp9-9RLloz"
      },
      "source": [
        "def variable_byte_save_func(path, pos_ind):\n",
        "  result = {}\n",
        "  for key in pos_ind.keys():\n",
        "    result[key] = encode_vb(get_gaps_list(pos_ind[key]))\n",
        "  with open(path, \"w\") as outfile:  \n",
        "        json.dump(result, outfile) \n",
        "        return os.path.getsize(path)\n",
        "\n",
        "def encode_vb(numbers):\n",
        "    bytes_list = []\n",
        "    for number in numbers:\n",
        "        bytes_list.append(encode_number_vb(number))\n",
        "    return ''.join('{:02x}'.format(x) for x in sum(bytes_list, []))\n",
        "\n",
        "def encode_number_vb(number):\n",
        "    bytes_list = []\n",
        "    while True:\n",
        "        bytes_list.insert(0, number % 128)\n",
        "        if number < 128:\n",
        "            break\n",
        "        number = number // 128\n",
        "    bytes_list[-1] += 128\n",
        "    return bytes_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLGrN4zh6Igj"
      },
      "source": [
        "تابع نهایی تنها یک عدد را با روش گفته کد می‌کند. با استفاده از این تابع، در تابع میانی، یک لیست از نمایه‌های یک توکن را دریافت، و رشته نهایی که حاصل از فشرده شدن کل این لیست است را خروجی می‌دهیم. در نهایت در تابع اول، کل نمایه‌ها را ورودی گرفته و هر نمایه را (پس از تبدیل به یک آرایه از فواصل) با استفاده از تابع میانی فشرده کرده و در فایل جیسون ذخیره می‌کنیم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2glv7U3haY3"
      },
      "source": [
        "def gamma_code_save_func(path, pos_ind):\n",
        "  result = {}\n",
        "  for key in pos_ind.keys():\n",
        "    result[key] = encode_vb(get_gaps_list(pos_ind[key]))\n",
        "  with open(path, \"w\") as outfile:  \n",
        "        json.dump(result, outfile) \n",
        "        return os.path.getsize(path)\n",
        "\n",
        "def encode_gc(numbers):\n",
        "  return \"\".join([encode_number_gc(number) for number in numbers])\n",
        "\n",
        "def encode_number_gc(number):\n",
        "  bit_of_number = bin(number)[3:]\n",
        "  result = \"\".join([\"1\" for _ in range(len(bit_of_number))]) + \"0\" + bit_of_number\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK5FFI3L6MjH"
      },
      "source": [
        "این تابع با ورودی گرفتن مسیر ذخیره و روش فشرده‌سازی، تابع مربوط به آن روش فشرده‌سازی را صدا می‌زند و خروجی آن تابع را که همان حجم فایل نهایی است، خروجی می‌دهد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqOITrpcU8ef",
        "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
      },
      "source": [
        "def store_index(path, compression_type):\n",
        "    \"\"\"Stores the index in a file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path to store the file\n",
        "\n",
        "    compression_type : str\n",
        "        Could be one of the followings:\n",
        "        - no-compression\n",
        "        - gamma-code\n",
        "        - variable-byte\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        The size of the stored file\n",
        "    \"\"\"\n",
        "    # TODO: compress and store positional_index\n",
        "    size_of_file = eval(compression_type + '_save_func(path, pos_ind)')\n",
        "    return size_of_file        \n",
        "store_index(\"path/to/file.json\", \"no_compression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHTROU2j4pLj"
      },
      "source": [
        "این تابع با گرفتن فواصل بین نمایه‌ها، لیست نمایه‌ها را بازمی‌گرداند."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_vj2xtwLFp1"
      },
      "source": [
        "def get_numbers_list_from_gaps(pos_ind): \n",
        "  return [pos_ind[0]]+[pos_ind[i - 1]+pos_ind[i] for i in range(1,len(pos_ind))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3FuTvRN63xJ"
      },
      "source": [
        "اگر هنگام ذخیره فشرده سازی نکرده باشیم، با کمک این تابع نمایه‌ها را بازیابی می‌کنیم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvUC_w6QMgQw"
      },
      "source": [
        "def no_compression_load_func(path):\n",
        "  with open(path) as infile:  \n",
        "    result = json.load(infile)\n",
        "  return result\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS4QjDcb7EgL"
      },
      "source": [
        "تابع دوم یک رشته می‌گیرد و آن‌ها را مطابق الگوریتم از حالت فشرده خارج کرده و لیست فواصل نمایه‌ها را تولید می‌کند. تابع اول نیز تمامی رشته‌ها را از فایل می‌خواند پس از به دست آوردن لیست فواصل به کمک تابع دوم، نمایه‌ها را به دست می‌آورد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93zJpH3_yXUd"
      },
      "source": [
        "def gamma_code_load_func(path):\n",
        "  result = {}\n",
        "  with open(path) as infile:  \n",
        "    data = json.load(infile)\n",
        "  for key in data.keys():\n",
        "    result[key] = get_numbers_list_from_gaps(decode_gamma(data[key]))\n",
        "  return result\n",
        "\n",
        "def decode_gamma(bitstream):\n",
        "  num, length, offset, first_zero, res = 0, \"\", \"\", 0, []\n",
        "  while bitstream != \"\":\n",
        "    first_zero = bitstream.find('0')\n",
        "    length = bitstream[:first_zero]\n",
        "    if length == \"\":\n",
        "      res.append(1)\n",
        "      bitstream = bitstream[1:]\n",
        "    else:\n",
        "      offset = \"1\" + bitstream[first_zero+1:first_zero+1+len(length)]\n",
        "      res.append(int(offset,2))\n",
        "      bitstream = bitstream[first_zero+1+len(length):]\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nWTOXQ7fSl"
      },
      "source": [
        "تابع دوم یک رشته می‌گیرد و آن‌ها را مطابق الگوریتم از حالت فشرده خارج کرده و لیست فواصل نمایه‌ها را تولید می‌کند. تابع اول نیز تمامی رشته‌ها را از فایل می‌خواند پس از به دست آوردن لیست فواصل به کمک تابع دوم، نمایه‌ها را به دست می‌آورد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33-D8fgZw1KY"
      },
      "source": [
        "def variable_byte_load_func(path):\n",
        "  result = {}\n",
        "  with open(path) as infile:  \n",
        "    data = json.load(infile)\n",
        "  for key in data.keys():\n",
        "    result[key] = get_numbers_list_from_gaps(decode_vb(data[key]))\n",
        "  return result\n",
        "\n",
        "\n",
        "def decode_vb(bytestream):\n",
        "    n = 0\n",
        "    numbers = []\n",
        "    bytestream = bytearray.fromhex(bytestream)\n",
        "    for byte in bytestream:\n",
        "        if byte < 128:\n",
        "            n = 128 * n + byte\n",
        "        else:\n",
        "            n = 128 * n + (byte - 128)\n",
        "            numbers.append(n)\n",
        "            n = 0\n",
        "    return numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYN9I4_BD178",
        "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
      },
      "source": [
        "def load_index(path, compression_type):\n",
        "    \"\"\"Loads the index from a file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path of the file to load from\n",
        "\n",
        "    compression_type : str\n",
        "        Could be one of the followings:\n",
        "        - no-compression\n",
        "        - gamma-code\n",
        "        - variable-byte\n",
        "    \"\"\"\n",
        "    # TODO: load and decompress positional_index\n",
        "    return eval(compression_type + '_load_func(path)')\n",
        "\n",
        "load_index(\"path/to/file.json\", \"no-compression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 55,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdCFLLySQab"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "بخش چهارم ( اصلاح پرسمان )\n",
        "</h2>\n",
        "در صورتی که پرسمان ورودی دارای غلط املایی باشد ( لغت‌ غلط لغتی است که در لغت‌نامه موجود نیست ) لازم است که با جست‌وجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جست‌وجو با پرسمان اصلاح‌شده پرداخته‌شود.\n",
        "برای این‌کار لازم است ابتدا به وسیله‌ی روش bigram \n",
        "و معیار jaccard \n",
        "نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید و سپس بهترین لغت میان آن‌ها را با استفاده از معیار \n",
        "edit distance \n",
        "بیابید.\n",
        "<h3>\n",
        "نکات پیاده‌سازی:\n",
        "</h3>\n",
        "یک تابع پیاده‌سازی شود که ورودی خام را گرفته و متن تصحیح شده‌ی آن‌را نمایش دهد. دقت کنید ورودی و خروجی هر دو رشته‌ی متنی هستند.\n",
        "</br>\n",
        "نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNeWIwicsrOa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu_k1fG_Z06l"
      },
      "source": [
        "def jaccard_distance(lst1, lst2):\n",
        "    s1 = set([lst1[ind:ind+2] for ind in range(len(lst1)-1)])\n",
        "    s2 = set([lst2[ind:ind+2] for ind in range(len(lst2)-1)])\n",
        "    \n",
        "    return len(s1 & s2)/len(s1 | s2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suf9PFEU2ytL"
      },
      "source": [
        "پیاده سازی معیار jaccard که دو لیست میگرد و آن معیار را خروجی میدهد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iQ1eeNYw7Vt"
      },
      "source": [
        "stop_words = get_stop_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XhCUUYTxhlb"
      },
      "source": [
        "stop_words = [x[0] for x in stop_words[0: 15]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRYyXrf8CS1"
      },
      "source": [
        "15 تای پرتکرار را به عنوان stop_words در نظر گرفته ایم."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cteHMhSbwoW"
      },
      "source": [
        "def find_best_match(query):\n",
        "    query2 = \"$\" + query + \"$\"\n",
        "    query2 = query\n",
        "    candidates = set()\n",
        "    for ind in range(len(query2)-1):\n",
        "        bichar = query2[ind:ind+2]\n",
        "        for token in bigr_ind[bichar]:\n",
        "            candidates.add(token)\n",
        "        \n",
        "    import math\n",
        "    \n",
        "    jaccard_limit = 0.4\n",
        "    best_candidate, best_distance = query, math.inf\n",
        "    for candidate in candidates:\n",
        "        if jaccard_distance(candidate, query) >= jaccard_limit:\n",
        "            tmp_distance = nltk.edit_distance(candidate, query, transpositions=False)\n",
        "            if tmp_distance < best_distance:\n",
        "                best_candidate, best_distance = candidate, tmp_distance\n",
        "    \n",
        "    \n",
        "    return best_candidate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVQZKkQ23ABW"
      },
      "source": [
        "   این تابع یک کوِِِیری ورودی میگیرد و نزدیک ترین توکن را به این کوِِیری که درست است را بازمیگرداند.\n",
        "   ابتدا بایگرم های کویری را پیدا میکند و با استفاده از دیکشنری bigr_ind کاندیدا های نزدیک ترین توکن را در لیست candidates میریزد. سپس در بخش بعدی با استفاده از معیار های گفته شده بهترین و نزدیکترین توکن را پیدا میکند و خروجی میدهد. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMZTstsbL1G3"
      },
      "source": [
        "def create_bigram_index():\n",
        "    \"\"\"Creates the bigram index for spell correction\"\"\"\n",
        "\n",
        "    # TODO: create bigram index\n",
        "    bigram = {\"he\": [\"hello\", \"hell\", \"he\", \"her\"], \"el\": [\"telephone\", \"else\"]}\n",
        "    return bigram\n",
        "\n",
        "create_bigram_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL3E8C-nL1yG"
      },
      "source": [
        "def get_corrected_text(raw_text):\n",
        "    \"\"\"Corrects the query\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_text : str\n",
        "        Input text that could be a title or a plot\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The corrected text\n",
        "    \"\"\"\n",
        "    tokens = prepare_text(raw_text)\n",
        "    suggestion = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in pos_ind or token in stop_words:\n",
        "            suggestion.append(token)\n",
        "            continue\n",
        "        \n",
        "        suggestion.append(find_best_match(token))\n",
        "    \n",
        "    corrected_text = \" \".join(suggestion)\n",
        "    \n",
        "    return corrected_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouoMt1zv6NKG"
      },
      "source": [
        "این تابع کویری را از کاربر میگیرد و ابتدا آن را توسط تابع prepare_text به توکن های مورد نظر تبدیل میکند و سپس برای هر کدام از این توکن ها که نارست هستند با استفاده از تابع قبلی با نزدیکترین توکن آن را جایگزین کیند و در آخر کویری اصلاح شده را باز میگرداند."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epLKVSVaSPxN"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h2>\n",
        "بخش پنجم ( نکات و جمع بندی پایانی )\n",
        "</h2>\n",
        "<ol>\n",
        "<li>\n",
        "گزارشی از پیاده سازی هر بخش تهیه در پایان هر بخش در همین فایل ژوپیتر قرار دهید.\n",
        "</li>\n",
        "<li>\n",
        "تنها زبان برنامه نویسی مجاز پایتون می باشد\n",
        "</li>\n",
        "<li>\n",
        "کد ها و توابع از نظر شباهت بررسی خواهد شد و با موارد مشابه و تقلب طبق آیین نامه تمارین درسی برخورد خواهد شد.\n",
        "</li>\n",
        "<li>\n",
        "فایل نوت بوک را زیپ کنید و با فرمت StudentNumber_phase1 ارسال نمایید.\n",
        "</li>\n",
        "<li>\n",
        "سیستم را بهینه پیاده سازی کنید تا در زمان کمتری بارگزاری و نمایه سازی شود.\n",
        "</li>\n",
        "<li>\n",
        "به ۵ پیاده سازی برتر نمره امتیازی تعلق میگیرد.\n",
        "</li>\n",
        "</ol>\n",
        "<p>\n",
        "\n",
        "در انجام پروژه اگر سوالی داشتید می توانید با ایمیل\n",
        " sinakazemi1998@gmail.com \n",
        " و\n",
        " parsa.eskandar@gmail.com\n",
        " در ارتباط باشید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lol8GOugeGC1"
      },
      "source": [
        "نحوه تقسیم کار:\n",
        "پرهام چاوشیان : بخش فارسی آماده‌سازی متن در بخش 1 و بخش 3\n",
        "میلاد سعادت : بخش انگلیسی آماده‌سازی متن در بخش 1 و بخش 4\n",
        "فراز قهرمانی : بخش 2 "
      ]
    }
  ]
}