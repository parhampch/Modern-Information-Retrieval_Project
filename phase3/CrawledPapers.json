[
    {
        "id": 317558625,
        "title": "Attention Is All You Need",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "date": "June 2017",
        "authors": [
            "Ashish Vaswani",
            "Noam Shazeer",
            "Niki Parmar",
            "Jakob Uszkoreit"
        ],
        "references": [
            305334466,
            269416998,
            266376373,
            265252627,
            262877889,
            262408350,
            221013135,
            220873863,
            13853244,
            322584145
        ]
    },
    {
        "id": 286512696,
        "title": "Deep Residual Learning for Image Recognition",
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "date": "December 2015",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "references": [
            305196650,
            269935397,
            265908778,
            265787949,
            265295439,
            264979485,
            260126867,
            259441043,
            259440750,
            234119846
        ]
    },
    {
        "id": 333444574,
        "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
        "date": "May 2019",
        "authors": [
            "Mingxing Tan",
            "Quoc V. Le"
        ],
        "references": [
            328110752,
            319622446,
            314282992,
            313641935,
            265295439,
            348350592,
            329740202,
            329740172,
            322950113,
            320968382
        ]
    },
    {
        "id": 352015995,
        "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
        "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.",
        "date": "May 2021",
        "authors": [
            "Enze Xie",
            "Wenhai Wang",
            "Zhiding Yu",
            "Anima Anandkumar"
        ],
        "references": [
            338503872,
            335463644,
            329442528,
            302305068,
            301880609,
            347030093,
            346022004,
            345324756,
            343466461,
            343466187
        ]
    },
    {
        "id": 354891122,
        "title": "PASS: An ImageNet replacement for self-supervised pretraining without humans",
        "abstract": "Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.",
        "date": "September 2021",
        "authors": [
            "Yuki M. Asano",
            "Christian Rupprecht",
            "Andrew Zisserman",
            "Andrea Vedaldi"
        ],
        "references": [
            352224617,
            343463185,
            356869040,
            355882923,
            355882623,
            352398842,
            347459139,
            344753035,
            343465745,
            343461321
        ]
    },
    {
        "id": 328230984,
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.",
        "date": "October 2018",
        "authors": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "references": [
            334116365,
            304506026,
            304018244,
            284576917,
            279068396,
            268079628,
            259239818,
            257882504,
            221361415,
            221346269
        ]
    },
    {
        "id": 344828174,
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
        "date": "October 2020",
        "authors": [
            "Alexey Dosovitskiy",
            "Lucas Beyer",
            "Alexander Kolesnikov",
            "Dirk Weissenborn"
        ],
        "references": [
            343466387,
            343466025,
            343298597,
            339562815,
            339562468
        ]
    },
    {
        "id": 305334466,
        "title": "Recurrent Neural Network Grammars",
        "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.",
        "date": "February 2016",
        "authors": [
            "Chris Dyer",
            "Adhiguna Kuncoro",
            "Miguel Ballesteros",
            "Noah A. Smith"
        ],
        "references": [
            306094026,
            283865501,
            278965988,
            319770403,
            301870716,
            301841091,
            286531327,
            284039049,
            283806803,
            278413581
        ]
    },
    {
        "id": 269416998,
        "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
        "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
        "date": "December 2014",
        "authors": [
            "Junyoung Chung",
            "Caglar Gulcehre",
            "KyungHyun Cho",
            "Y. Bengio"
        ],
        "references": [
            265385879,
            265252627,
            255983764,
            243781690,
            279394722,
            267706055,
            265554383,
            262395872,
            258818168,
            255173850
        ]
    },
    {
        "id": 266376373,
        "title": "Fast and Accurate Shift-Reduce Constituent Parsing",
        "abstract": "Shift-reduce dependency parsers give comparable accuracies to their chart-based counterparts, yet the best shift-reduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.",
        "date": "June 2013",
        "authors": [
            "Muhua Zhu",
            "Yue Zhang",
            "Wenliang Chen",
            "Min Zhang"
        ],
        "references": [
            262408350,
            262402839,
            262323558,
            228916842,
            228362666,
            270877686,
            262361657,
            262349927,
            243787298,
            233823603
        ]
    },
    {
        "id": 265252627,
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
        "date": "September 2014",
        "authors": [
            "Dzmitry Bahdanau",
            "Kyunghyun Cho",
            "Y. Bengio"
        ],
        "references": [
            270877878,
            265386055,
            265385879,
            262877889,
            319770418,
            307174794,
            303256841,
            289758666,
            270878785,
            261309981
        ]
    },
    {
        "id": 262877889,
        "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "abstract": "In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
        "date": "June 2014",
        "authors": [
            "Kyunghyun Cho",
            "Bart van Merrienboer",
            "Caglar Gulcehre",
            "Fethi Bougares"
        ],
        "references": [
            257882504,
            319770387,
            319770183,
            312369762,
            307955489,
            289758666,
            270878785,
            267960550,
            233981807,
            228379274
        ]
    },
    {
        "id": 262408350,
        "title": "Effective self-training for parsing",
        "abstract": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.",
        "date": "June 2006",
        "authors": [
            "David McClosky",
            "Eugene Charniak",
            "Mark Johnson"
        ],
        "references": [
            221617806,
            268237865,
            262349927,
            230876697,
            230876655,
            229078318,
            223311595,
            221275760,
            220875255,
            220875180
        ]
    },
    {
        "id": 221013135,
        "title": "Self-Training PCFG Grammars with Latent Annotations Across Languages.",
        "abstract": "We investigate the effectiveness of self- training PCFG grammars with latent anno- tations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak's lexicalized parser, the PCFG-LA parser was more ef- fectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from self- training. We show for the first time that self-training is able to significantly im- prove the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled train- ing data. Our approach achieves state- of-the-art parsing accuracies for a single parser on both English (91.5%) and Chi- nese (85.2%).",
        "date": "January 2009",
        "authors": [
            "Zhongqiang Huang",
            "Mary Harper"
        ],
        "references": [
            262408350,
            221101491,
            221012635,
            220875081,
            220873863,
            221012840,
            220875180,
            220874256,
            220873964,
            220873958
        ]
    },
    {
        "id": 220873863,
        "title": "Learning Accurate, Compact, and Interpretable Tree Annotation.",
        "abstract": "We present an automatic approach to tree annota- tion in which basic nonterminal symbols are alter- nately split and merged to maximize the likelihood of a training treebank. Starting with a simple X- bar grammar, we learn a new grammar whose non- terminals are subsymbols of the original nontermi- nals. In contrast with previous work, we are able to split various terminals to different degrees, as ap- propriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more ac- curate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
        "date": "January 2006",
        "authors": [
            "Slav Petrov",
            "Leon Barrett",
            "Romain Thibaux",
            "Dan Klein"
        ],
        "references": [
            220875081,
            220874045,
            298836450,
            292453644,
            288942058,
            242430108,
            225740821,
            225517426,
            221112104,
            220875180
        ]
    },
    {
        "id": 13853244,
        "title": "Long Short-term Memory",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "date": "December 1997",
        "authors": [
            "Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            277298117,
            277295865,
            243781690,
            243698906,
            243683010,
            313702304,
            263902178,
            243763246,
            243732587,
            243683189
        ]
    },
    {
        "id": 322584145,
        "title": "Massive Exploration of Neural Machine Translation Architectures",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Denny Britz",
            "Anna Helen Goldie",
            "Minh-Thang Luong",
            "Quoc Le"
        ],
        "references": [
            319770123,
            312216100,
            311223153,
            309766061,
            307747289,
            306093640,
            303657108,
            273640320,
            273388012,
            272194766
        ]
    },
    {
        "id": 305196650,
        "title": "Going deeper with convolutions",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Christian Szegedy",
            "Wei Liu",
            "Yangqing Jia",
            "Pierre Sermanet"
        ],
        "references": [
            319770289,
            289917319,
            266657849,
            266225209,
            259441043,
            319770183,
            319770166,
            310752533,
            286271944,
            267960550
        ]
    },
    {
        "id": 269935397,
        "title": "FitNets: Hints for Thin Deep Nets",
        "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
        "date": "December 2014",
        "authors": [
            "Adriana Romero",
            "Nicolas Ballas",
            "Samira Ebrahimi Kahou",
            "Antoine Chassang"
        ],
        "references": [
            303137904,
            267225654,
            265908778,
            265787949,
            319770367,
            289733708,
            273387909,
            269877085,
            266031774,
            265748773
        ]
    },
    {
        "id": 265908778,
        "title": "Deeply-Supervised Nets",
        "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).",
        "date": "September 2014",
        "authors": [
            "Chen-Yu Lee",
            "Saining Xie",
            "Patrick Gallagher",
            "Zhengyou Zhang"
        ],
        "references": [
            303137904,
            264979485,
            319770626,
            319770183,
            310752533,
            291735245,
            286569315,
            278695382,
            278651717,
            267960550
        ]
    },
    {
        "id": 265787949,
        "title": "Going Deeper with Convolutions",
        "abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
        "date": "September 2014",
        "authors": [
            "Christian Szegedy",
            "Wei Liu",
            "Yangqing Jia",
            "Pierre Sermanet"
        ],
        "references": [
            319770289,
            289917319,
            266657849,
            266225209,
            259441043,
            319770183,
            319770166,
            310752533,
            286271944,
            267960550
        ]
    },
    {
        "id": 265295439,
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
        "date": "September 2014",
        "authors": [
            "Olga Russakovsky",
            "Jia Deng",
            "Hao Su",
            "Jonathan Krause"
        ],
        "references": [
            347495711,
            319770626,
            319770439,
            319770430,
            319770370,
            319770363,
            319770264,
            319770183,
            316240097,
            314450504
        ]
    },
    {
        "id": 264979485,
        "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
        "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",
        "date": "June 2014",
        "authors": [
            "Yangqing Jia",
            "Evan Shelhamer",
            "Jeff Donahue",
            "Sergey Karayev"
        ],
        "references": [
            301463801,
            262270555,
            259441043,
            258520603,
            319770411,
            319770183,
            272149731,
            267960550,
            264890087,
            258839715
        ]
    },
    {
        "id": 260126867,
        "title": "On the Number of Linear Regions of Deep Neural Networks",
        "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.",
        "date": "February 2014",
        "authors": [
            "Guido Montufar",
            "Razvan Pascanu",
            "Kyunghyun Cho",
            "Y. Bengio"
        ],
        "references": [
            259400019,
            258816388,
            319770418,
            319770183,
            316519032,
            310752533,
            284500238,
            267960550,
            267064227,
            265178583
        ]
    },
    {
        "id": 259441043,
        "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learnt simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013), and produced near state of the art results for the detection and classifications tasks. Finally, we release a feature extractor from our best model called OverFeat.",
        "date": "December 2013",
        "authors": [
            "Pierre Sermanet",
            "David Eigen",
            "Xiang Zhang",
            "Michael Mathieu"
        ],
        "references": [
            262270555,
            257672343,
            240308781,
            235360690,
            228102719,
            319770626,
            319770183,
            267960550,
            262323276,
            233815499
        ]
    },
    {
        "id": 259440750,
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed remains finite: for a special class of initial conditions on the weights, very deep networks incur only a finite delay in learning speed relative to shallow networks. We further show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, thereby providing analytical insight into the success of unsupervised pretraining in deep supervised learning tasks.",
        "date": "December 2013",
        "authors": [
            "Andrew M. Saxe",
            "James L Mcclelland",
            "Surya Ganguli"
        ],
        "references": [
            303137904,
            266463521,
            243781690,
            234131129,
            233730646,
            319770626,
            319770183,
            286271944,
            270878508,
            267960550
        ]
    },
    {
        "id": 234119846,
        "title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities",
        "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.",
        "date": "January 2013",
        "authors": [
            "Tommi Vatanen",
            "Tapani Raiko",
            "Harri Valpola",
            "Yann Lecun"
        ],
        "references": [
            231556969,
            228102719,
            215616968,
            289786324,
            267960550,
            235222671,
            225495886,
            221619092,
            221345102,
            46392541
        ]
    },
    {
        "id": 328110752,
        "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
        "abstract": "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy.",
        "date": "September 2018",
        "authors": [
            "Yihui He",
            "Ji Lin",
            "Zhijian Liu",
            "Hanrui Wang"
        ],
        "references": [
            324435700,
            322059721,
            322058668,
            320971183,
            329745708,
            328112952,
            322517761,
            322058064,
            320968382,
            320963610
        ]
    },
    {
        "id": 319622446,
        "title": "The Expressive Power of Neural Networks: A View from the Width",
        "abstract": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that \\emph{depth-bounded} (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for \\emph{width-bounded} ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an \\emph{exponential} bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a \\emph{polynomial} bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.",
        "date": "September 2017",
        "authors": [
            "Zhou Lu",
            "Hongming Pu",
            "Feicheng Wang",
            "Zhiqiang Hu"
        ],
        "references": [
            308896188,
            305196650,
            281895600,
            319770106,
            314361329,
            311609041,
            301857086,
            287250773,
            286512696,
            284500238
        ]
    },
    {
        "id": 314282992,
        "title": "On the Expressive Power of Overlapping Architectures of Deep Learning",
        "abstract": "Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.",
        "date": "May 2018",
        "authors": [
            "Or Sharir",
            "Amnon Shashua"
        ],
        "references": [
            322539221,
            309131743,
            308026508,
            305196650,
            304018355,
            303448829,
            319770395,
            319770007,
            312461699,
            309572399
        ]
    },
    {
        "id": 313641935,
        "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
        "abstract": "In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10x10 board, using TD(\u03bb) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\u03bb) agent with SiLU and dSiLU hidden units.",
        "date": "January 2018",
        "authors": [
            "Stefan Elfwing",
            "Eiji Uchibe",
            "Kenji Doya"
        ],
        "references": [
            307627766,
            292074166,
            282182152,
            280898866,
            280104499,
            280043948,
            319770330,
            312990324,
            306218037,
            304109482
        ]
    },
    {
        "id": 348350592,
        "title": "ResNet with one-neuron hidden layers is a Universal Approximator",
        "abstract": "We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. \u21131(Rd). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.",
        "date": "July 2018",
        "authors": [
            "Hongzhou Lin",
            "Stefanie Sabrina Jegelka"
        ],
        "references": []
    },
    {
        "id": 329740202,
        "title": "Squeeze-and-Excitation Networks",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Jie Hu",
            "Li Shen",
            "Gang Sun"
        ],
        "references": [
            320965021,
            319770123,
            316450913,
            316184205,
            310462326,
            305196650,
            303409658,
            268079628,
            265295439,
            263891809
        ]
    },
    {
        "id": 329740172,
        "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Xiangyu Zhang",
            "Xinyu Zhou",
            "Mengxiao Lin",
            "Jian Sun"
        ],
        "references": [
            319770160,
            318337293,
            316184205,
            313645102,
            305196650,
            303409658,
            301878495,
            301839500,
            287853408,
            282005595
        ]
    },
    {
        "id": 322950113,
        "title": "Regularized Evolution for Image Classifier Architecture Search",
        "abstract": "The effort devoted to hand-crafting image classifiers has motivated the use of architecture search to discover them automatically. Reinforcement learning and evolution have both shown promise for this purpose. This study introduces a regularized version of a popular asynchronous evolutionary algorithm. We rigorously compare it to the non-regularized form and to a highly-successful reinforcement learning baseline. Using the same hardware, compute effort and neural network training code, we conduct repeated experiments side-by-side, exploring different datasets, search spaces and scales. We show regularized evolution consistently produces models with similar or higher accuracy, across a variety of contexts without need for re-tuning parameters. In addition, regularized evolution exhibits considerably better performance than reinforcement learning at early search stages, suggesting it may be the better choice when fewer compute resources are available. This constitutes the first controlled comparison of the two search algorithms in this context. Finally, we present new architectures discovered with regularized evolution that we nickname AmoebaNets. These models set a new state of the art for CIFAR-10 (mean test error = 2.13%) and mobile-size ImageNet (top-5 accuracy = 92.1% with 5.06M parameters), and reach the current state of the art for ImageNet (top-5 accuracy = 96.2%).",
        "date": "February 2018",
        "authors": [
            "Esteban Real",
            "Alok Aggarwal",
            "Yanping Huang",
            "Quoc V Le"
        ],
        "references": [
            325673550,
            321902574,
            318255371,
            316598820,
            316184205,
            313096253,
            309738510,
            308981007,
            306885833,
            304857984
        ]
    },
    {
        "id": 320968382,
        "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Francois Chollet"
        ],
        "references": [
            319769813,
            316184205,
            305196650,
            284579051,
            269722508,
            260642043,
            236736831,
            230867026,
            319770291,
            319770183
        ]
    },
    {
        "id": 338503872,
        "title": "Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Vladimir Nekrasov",
            "Hao Chen",
            "Chunhua Shen",
            "Ian Reid"
        ],
        "references": [
            324584089,
            335144529,
            330595799,
            329748830,
            329745708,
            323570681,
            323302973,
            323118469,
            323027025,
            322517761
        ]
    },
    {
        "id": 335463644,
        "title": "Boundary-Aware Feature Propagation for Scene Segmentation",
        "abstract": "In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes.",
        "date": "October 2019",
        "authors": [
            "Henghui Ding",
            "Xudong Jiang",
            "Ai Qun Liu",
            "Nadia Magnenat Thalmann"
        ],
        "references": [
            335463411,
            339558864,
            339558490,
            339556516,
            339555830,
            339555410,
            339554650,
            338512016,
            338509793,
            338506535
        ]
    },
    {
        "id": 329442528,
        "title": "DenseASPP for Semantic Segmentation in Street Scenes",
        "abstract": "Semantic image segmentation is a basic street scene un- derstanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of seman- tic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly en- coded. To remedy this problem, atrous convolution[14] was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)[2] was proposed to concatenate multiple atrous-convolved fea- tures using different dilation rates into a final feature rep- resentation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spa- tial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, with- out significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapes[4] and achieve state-of-the-art performance.",
        "date": "July 2018",
        "authors": [
            "Maoke Yang",
            "Yu Kun",
            "Chi Zhang",
            "Zhiwei Li"
        ],
        "references": [
            322749812,
            324996175,
            322058210,
            320971443,
            320968233,
            320968206,
            320964900,
            319770420,
            319770168,
            317679203
        ]
    },
    {
        "id": 302305068,
        "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.",
        "date": "May 2016",
        "authors": [
            "Fisher Yu",
            "Vladlen Koltun"
        ],
        "references": [
            319769910,
            301880609,
            282179651,
            277334270,
            275588416,
            319770168,
            308830795,
            302305924,
            283761983,
            276923091
        ]
    },
    {
        "id": 301880609,
        "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
        "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",
        "date": "June 2016",
        "authors": [
            "Marius Cordts",
            "Mohamed Omran",
            "Sebastian Ramos",
            "Timo Rehfeld"
        ],
        "references": [
            319770205,
            319769910,
            319770430,
            319770420,
            319770291,
            319770183,
            319770168,
            311610893,
            311610196,
            308862494
        ]
    },
    {
        "id": 347030093,
        "title": "Improving Semantic Segmentation via Decoupled Body and Edge Supervision",
        "abstract": "Existing semantic segmentation approaches either aim to improve the object\u2019s inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, CamVid, KIITI and BDD show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (https://github.com/lxtGH/DecoupleSegNets).",
        "date": "November 2020",
        "authors": [
            "Xiangtai Li",
            "Xia Li",
            "Li Zhang",
            "Guangliang Cheng"
        ],
        "references": [
            342537621,
            335463644,
            329743810,
            329442528,
            325718841,
            321417929,
            320195152,
            343465903,
            343461519,
            343454917
        ]
    },
    {
        "id": 346022004,
        "title": "End-to-End Object Detection with Transformers",
        "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
        "date": "November 2020",
        "authors": [
            "Nicolas Carion",
            "Francisco Massa",
            "Gabriel Synnaeve",
            "Nicolas Usunier"
        ],
        "references": [
            339561869,
            322057895,
            321347654,
            310769756,
            308278279,
            265252627,
            259212328,
            215616968,
            343466724,
            339562815
        ]
    },
    {
        "id": 345324756,
        "title": "SegFix: Model-Agnostic Boundary Refinement for Segmentation",
        "abstract": "We present a model-agnostic post-processing scheme to improve the boundary quality for the segmentation result that is generated by any existing segmentation model. Motivated by the empirical observation that the label predictions of interior pixels are more reliable, we propose to replace the originally unreliable predictions of boundary pixels by the predictions of interior pixels. Our approach processes only the input image through two steps: (i) localize the boundary pixels and (ii) identify the corresponding interior pixel for each boundary pixel. We build the correspondence by learning a direction away from the boundary pixel to an interior pixel. Our method requires no prior information of the segmentation models and achieves nearly real-time speed. We empirically verify that our SegFix consistently reduces the boundary errors for segmentation results generated from various state-of-the-art models on Cityscapes, ADE20K and GTA5. Code is available at: https://github.com/openseg-group/openseg.pytorch.",
        "date": "October 2020",
        "authors": [
            "Yuhui Yuan",
            "Jingyi Xie",
            "Xilin Chen",
            "Jingdong Wang"
        ],
        "references": [
            338509297,
            335463644,
            335463411,
            329743873,
            329740252,
            320971612,
            320195152,
            317230093,
            314182547,
            346770482
        ]
    },
    {
        "id": 343466461,
        "title": "Learning Dynamic Routing for Semantic Segmentation",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Yanwei Li",
            "Lin Song",
            "Yukang Chen",
            "Zeming Li"
        ],
        "references": [
            338503872,
            321242021,
            301880609,
            283471087,
            276923248,
            221361415,
            220659463,
            346160386,
            339559337,
            339554584
        ]
    },
    {
        "id": 343466187,
        "title": "Benchmarking the Robustness of Semantic Segmentation Models",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Christoph Kamann",
            "Carsten Rother"
        ],
        "references": [
            323655313,
            321324947,
            319770123,
            319312265,
            316911609,
            316270100,
            316184205,
            313713721,
            311222763,
            305196650
        ]
    },
    {
        "id": 352224617,
        "title": "Excavating AI: the politics of images in machine learning training sets",
        "abstract": "By looking at the politics of classification within machine learning systems, this article demonstrates why the automated interpretation of images is an inherently social and political project. We begin by asking what work images do in computer vision systems, and what is meant by the claim that computers can \u201crecognize\u201d an image? Next, we look at the method for introducing images into computer systems and look at how taxonomies order the foundational concepts that will determine how a system interprets the world. Then we turn to the question of labeling: how humans tell computers which words will relate to a given image. What is at stake in the way AI systems use these labels to classify humans, including by race, gender, emotions, ability, sexuality, and personality? Finally, we turn to the purposes that computer vision is meant to serve in our society\u2014the judgments, choices, and consequences of providing computers with these capacities. Methodologically, we call this an archeology of datasets: studying the material layers of training images and labels, cataloguing the principles and values by which taxonomies are constructed, and analyzing how these taxonomies create the parameters of intelligibility for an AI system. By doing this, we can critically engage with the underlying politics and values of a system, and analyze which normative patterns of life are assumed, supported, and reproduced.",
        "date": "June 2021",
        "authors": [
            "Kate Crawford",
            "Trevor Paglen"
        ],
        "references": [
            334512882,
            330879115,
            322877702,
            329927208,
            329748610,
            319770183,
            319394978,
            249983357,
            248819167,
            247122070
        ]
    },
    {
        "id": 343463185,
        "title": "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Sachit Menon",
            "Alexandru Damian",
            "Shijia Hu",
            "Nikhil Ravi"
        ],
        "references": [
            345324463,
            329748624,
            329745710,
            326723251,
            321374755,
            339555522,
            339483013,
            338962150,
            338514531,
            320968363
        ]
    },
    {
        "id": 356869040,
        "title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
        "abstract": "2019 Neural information processing systems foundation. All rights reserved. We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet - objects are largely centered and unoccluded - and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",
        "date": "January 2019",
        "authors": [
            "A Barbu",
            "D Mayo",
            "J Alverio",
            "W Luo"
        ],
        "references": []
    },
    {
        "id": 355882923,
        "title": "Exploring Simple Siamese Representation Learning",
        "abstract": "",
        "date": "June 2021",
        "authors": [
            "Xinlei Chen",
            "Kaiming He"
        ],
        "references": [
            309640954,
            306187421,
            221620245,
            221361415,
            4246277,
            343456678,
            339559093,
            338503604,
            329743791,
            315454672
        ]
    },
    {
        "id": 355882623,
        "title": "Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning",
        "abstract": "",
        "date": "June 2021",
        "authors": [
            "Zhenda Xie",
            "Yutong Lin",
            "Zheng Zhang",
            "Yue Cao"
        ],
        "references": [
            339561869,
            323931846,
            301880609,
            301837491,
            277023095,
            263471626,
            221361415,
            339555805,
            338503604,
            329743791
        ]
    },
    {
        "id": 352398842,
        "title": "Large image datasets: A pyrrhic win for computer vision?",
        "abstract": "",
        "date": "January 2021",
        "authors": [
            "Abeba Birhane",
            "Vinay Uday Prabhu"
        ],
        "references": [
            343463185,
            334634583,
            333332997,
            322674945,
            318337409,
            314285557,
            306023833,
            302515980,
            284576917,
            271196763
        ]
    },
    {
        "id": 347459139,
        "title": "Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy",
        "abstract": "",
        "date": "January 2020",
        "authors": [
            "Kaiyu Yang",
            "Klint Qinami",
            "Li Fei-Fei",
            "Jia Deng"
        ],
        "references": [
            322587393,
            315818258,
            307747289,
            307473328,
            303545933,
            301844872,
            295853816,
            284219429,
            280530043,
            271196763
        ]
    },
    {
        "id": 344753035,
        "title": "Between Subjectivity and Imposition. Power Dynamics in Data Annotation for Computer Vision",
        "abstract": "The interpretation of data is fundamental to machine learning. This paper investigates practices of image data annotation as performed in industrial contexts. We define data annotation as a sense-making practice, where annotators assign meaning to data through the use of labels. Previous human-centered investigations have largely focused on annotators? subjectivity as a major cause of biased labels. We propose a wider view on this issue: guided by constructivist grounded theory, we conducted several weeks of fieldwork at two annotation companies. We analyzed which structures, power relations, and naturalized impositions shape the interpretation of data. Our results show that the work of annotators is profoundly informed by the interests, values, and priorities of other actors above their station. Arbitrary classifications are vertically imposed on annotators, and through them, on data. This imposition is largely naturalized. Assigning meaning to data is often presented as a technical matter. This paper shows it is, in fact, an exercise of power with multiple implications for individuals and society.",
        "date": "October 2020",
        "authors": [
            "Milagros Miceli",
            "Martin Schuessler",
            "Tianling Yang"
        ],
        "references": [
            341693469,
            338403092,
            332748249,
            329990585,
            325594485,
            324670607,
            317888465,
            297664844,
            288669223,
            271647061
        ]
    },
    {
        "id": 343465745,
        "title": "Self-Supervised Learning of Pretext-Invariant Representations",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Ishan Misra",
            "Laurens van der Maaten"
        ],
        "references": [
            339768441,
            329750748,
            323931846,
            318337409,
            316098571,
            315747848,
            312492399,
            308295512,
            306187421,
            343456678
        ]
    },
    {
        "id": 343461321,
        "title": "Learning Representations by Predicting Bags of Visual Words",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Spyros Gidaris",
            "Andrei Bursuc",
            "Nikos Komodakis",
            "Patrick Perez"
        ],
        "references": [
            339555791,
            323931846,
            308295512,
            305881526,
            301837491,
            279839496,
            279068396,
            277023095,
            265295439,
            263471626
        ]
    },
    {
        "id": 334116365,
        "title": "Universal Language Model Fine-tuning for Text Classification",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Jeremy Howard",
            "Sebastian Ruder"
        ],
        "references": [
            320920333,
            328159324,
            325448855,
            322590138,
            322582488,
            319770381,
            319770369,
            319770357,
            319770231,
            318849650
        ]
    },
    {
        "id": 304506026,
        "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units",
        "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic process which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.",
        "date": "June 2016",
        "authors": [
            "Dan Hendrycks",
            "Kevin Gimpel"
        ],
        "references": [
            332717934,
            319770320,
            319769813,
            305119419,
            303821573,
            303409435,
            301879329,
            284579051,
            319770257,
            286794765
        ]
    },
    {
        "id": 304018244,
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "abstract": "We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.",
        "date": "June 2016",
        "authors": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "references": [
            301404923,
            319770348,
            318497709,
            312607081,
            309438804,
            306094228,
            301445887,
            301405087,
            301404907,
            301404729
        ]
    },
    {
        "id": 284576917,
        "title": "Glove: Global Vectors for Word Representation",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "references": [
            319770439,
            314818211,
            301408902,
            289176734,
            285896121,
            285895924,
            270878536,
            270878508,
            270877599,
            267709055
        ]
    },
    {
        "id": 279068396,
        "title": "Skip-Thought Vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",
        "date": "June 2015",
        "authors": [
            "Ryan Kiros",
            "Yukun Zhu",
            "Ruslan R. Salakhutdinov",
            "Richard Zemel"
        ],
        "references": [
            329975395,
            319770465,
            319770439,
            319770257,
            319770249,
            308837339,
            301409028,
            301408986,
            289758666,
            284039049
        ]
    },
    {
        "id": 268079628,
        "title": "How transferable are features in deep neural networks?",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
        "date": "November 2014",
        "authors": [
            "Jason Yosinski",
            "Jeff Clune",
            "Y. Bengio",
            "Hod Lipson"
        ],
        "references": [
            264979485,
            259441043,
            319770357,
            319770183,
            310752533,
            283601334,
            268265707,
            267960550,
            258424423,
            258374356
        ]
    },
    {
        "id": 259239818,
        "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
        "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 74.4. A combination of techniques leads to 37% reduction in perplexity, or 11% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.",
        "date": "December 2013",
        "authors": [
            "Ciprian Chelba",
            "Tomas Mikolov",
            "Mike Schuster",
            "Qi Ge"
        ],
        "references": [
            241637478,
            228348202,
            224246503,
            307174896,
            288345724,
            270878020,
            266458936,
            261167314,
            242821088,
            222800591
        ]
    },
    {
        "id": 257882504,
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
        "date": "October 2013",
        "authors": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "G.s. Corrado"
        ],
        "references": [
            258082321,
            241637478,
            234131319,
            228348202,
            285895924,
            266458936,
            262367926,
            229091480,
            228095628,
            225818196
        ]
    },
    {
        "id": 221361415,
        "title": "ImageNet: a Large-Scale Hierarchical Image Database",
        "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
        "date": "June 2009",
        "authors": [
            "Jia Deng",
            "Wei Dong",
            "Richard Socher",
            "Li-Jia Li"
        ],
        "references": [
            277292831,
            238347229,
            313527089,
            312457795,
            269953350,
            242442798,
            239443282,
            233710354,
            232630154,
            230854795
        ]
    },
    {
        "id": 221346269,
        "title": "Extracting and composing robust features with denoising autoencoders",
        "abstract": "Previous work has shown that the dicul- ties in learning deep generative or discrim- inative models can be overcome by an ini- tial unsupervised learning step that maps in- puts to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a rep- resentation based on the idea of making the learned representations robust to partial cor- ruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to ini- tialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising ad- vantage of corrupting the input of autoen- coders on a pattern classification benchmark suite.",
        "date": "January 2008",
        "authors": [
            "Pascal Vincent",
            "Hugo Larochelle",
            "Y. Bengio",
            "Pierre-Antoine Manzagol"
        ],
        "references": [
            303137904,
            221620372,
            221617957,
            221364240,
            312451443,
            292215395,
            239566324,
            237044580,
            230876626,
            222438607
        ]
    },
    {
        "id": 343466387,
        "title": "Self-Training With Noisy Student Improves ImageNet Classification",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Qizhe Xie",
            "Minh-Thang Luong",
            "Eduard Hovy",
            "Quoc V. Le"
        ],
        "references": [
            334843952,
            332069196,
            319770123,
            318392350,
            316098571,
            313481233,
            310462326,
            308278012,
            305881127,
            305196650
        ]
    },
    {
        "id": 343466025,
        "title": "Exploring Self-Attention for Image Recognition",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Hengshuang Zhao",
            "Jiaya Jia",
            "Vladlen Koltun"
        ],
        "references": [
            319770123,
            316450913,
            308278279,
            306187421,
            305196650,
            303698513,
            302305068,
            283659096,
            272194766,
            265295439
        ]
    },
    {
        "id": 343298597,
        "title": "Quantifying Attention Flow in Transformers",
        "abstract": "",
        "date": "January 2020",
        "authors": [
            "Samira Abnar",
            "Willem Zuidema"
        ],
        "references": [
            311990858,
            343299346,
            336999161,
            335780510,
            335778955,
            334116956,
            322583071,
            317558625,
            309729965
        ]
    },
    {
        "id": 339562815,
        "title": "Attention Augmented Convolutional Networks",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Irwan Bello",
            "Barret Zoph",
            "Quoc Le",
            "Ashish Vaswani"
        ],
        "references": [
            311223153,
            306187421,
            305196650,
            265252627,
            221361415,
            221110516,
            13853244,
            2985446,
            338511824,
            338510249
        ]
    },
    {
        "id": 339562468,
        "title": "S4L: Self-Supervised Semi-Supervised Learning",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Lucas Beyer",
            "Xiaohua Zhai",
            "Avital Oliver",
            "Alexander Kolesnikov"
        ],
        "references": [
            334843952,
            323931846,
            323411512,
            316098571,
            308716468,
            305881127,
            305196650,
            301837491,
            280581078,
            279968088
        ]
    },
    {
        "id": 306094026,
        "title": "A Fast Unified Model for Parsing and Sentence Understanding",
        "abstract": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.",
        "date": "March 2016",
        "authors": [
            "Samuel R. Bowman",
            "Jon Gauthier",
            "Abhinav Rastogi",
            "Raghav Gupta"
        ],
        "references": [
            305334466,
            319770465,
            319770272,
            319770245,
            312416396,
            305334471,
            305334400,
            301880883,
            301841091,
            299487682
        ]
    },
    {
        "id": 283865501,
        "title": "Feature Optimization for Constituent Parsing via Neural Networks",
        "abstract": "The performance of discriminative constituent parsing relies crucially on feature engineering, and effective features usually have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neural network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, induces the feature representation in the hidden layer and makes parsing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm. By pre-Training the model on a large amount of automatically parsed data, and then fine-Tuning on the manually annotated Treebank data, our parser achieves the highest F1 score at 86.6% on Chinese Treebank 5.1, and a competitive F1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we significantly outperform Berkeley arser by 3.4 points on average for Chinese and 2.5 points for English.",
        "date": "January 2015",
        "authors": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Nianwen Xue"
        ],
        "references": [
            270878092,
            270818464,
            266376373,
            312607081,
            304533937,
            299487682,
            271231477,
            270878508,
            270877743,
            269997813
        ]
    },
    {
        "id": 278965988,
        "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Chris Dyer",
            "Miguel Ballesteros",
            "Wang Ling",
            "Austin Matthews"
        ],
        "references": [
            289176638,
            281812760,
            279068962,
            329975395,
            319770465,
            319770387,
            307955489,
            299487682,
            284039049,
            283619991
        ]
    },
    {
        "id": 319770403,
        "title": "A Tutorial on Particle Filtering and Smoothing : Fifteen years later",
        "abstract": "Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.",
        "date": "January 2008",
        "authors": [
            "Arnaud Doucet",
            "Adam Michael Johansen"
        ],
        "references": []
    },
    {
        "id": 301870716,
        "title": "Classes for Fast Maximum Entropy Training",
        "abstract": "Maximum entropy models are considered by many to be one of the most promising avenues of language modeling research. Unfortunately, long training times make maximum entropy research difficult. We present a novel speedup technique: we change the form of the model to use classes. Our speedup works by creating two maximum entropy models, the first of which predicts the class of each word, and the second of which predicts the word itself. This factoring of the model leads to fewer non-zero indicator functions, and faster normalization, achieving speedups of up to a factor of 35 over one of the best previous techniques. It also results in typically slightly lower perplexities. The same trick can be used to speed training of other machine learning techniques, e.g. neural networks, applied to any problem with a large number of outputs, such as language modeling.",
        "date": "August 2001",
        "authors": [
            "Joshua Goodman"
        ],
        "references": [
            220355244,
            3192689,
            2596889,
            2418338,
            230876357,
            38363908,
            2626194,
            2362847,
            2328946
        ]
    },
    {
        "id": 301841091,
        "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs",
        "abstract": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage.",
        "date": "March 2016",
        "authors": [
            "Eliyahu Kiperwasser",
            "Yoav Goldberg"
        ],
        "references": [
            289176638,
            283556656,
            281812760,
            279068962,
            278965988,
            277022875,
            273067823,
            264003485,
            234814829,
            228569700
        ]
    },
    {
        "id": 286531327,
        "title": "Efficient higher-order CRFs for morphological tagging",
        "abstract": "Training higher-order conditional random fields is prohibitive for huge tag sets. We present an approximated conditional random field using coarse-to-fine decoding and early updating. We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1st-order models.",
        "date": "January 2013",
        "authors": [
            "T. M\u00fcller",
            "H. Schmid",
            "H. Sch\u00fctze"
        ],
        "references": [
            262274512,
            255682142,
            221102585,
            220875017,
            220874670
        ]
    },
    {
        "id": 284039049,
        "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
        "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
        "date": "January 2013",
        "authors": [
            "Richard Socher",
            "A. Perelygin",
            "J.Y. Wu",
            "J. Chuang"
        ],
        "references": [
            254675183,
            303475895,
            284025120,
            225955045
        ]
    },
    {
        "id": 283806803,
        "title": "Generative Incremental Dependency Parsing with Neural Networks",
        "abstract": "We propose a neural network model for scalable generative transition-based dependency parsing. A probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation.",
        "date": "January 2015",
        "authors": [
            "Jan Buys",
            "Phil Blunsom"
        ],
        "references": [
            286902441,
            279068962,
            277560449,
            270878122,
            270877878,
            262402839,
            259239818,
            257870129,
            251574306,
            228707544
        ]
    },
    {
        "id": 278413581,
        "title": "A Bayesian Model for Generative Transition-based Dependency Parsing",
        "abstract": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.",
        "date": "June 2015",
        "authors": [
            "Jan Buys",
            "Phil Blunsom"
        ],
        "references": [
            262402839,
            257870129,
            228707544,
            221618802,
            221013207,
            221013182,
            221013171,
            221012805,
            220874833,
            220874243
        ]
    },
    {
        "id": 265385879,
        "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
        "date": "September 2014",
        "authors": [
            "Kyunghyun Cho",
            "Bart van Merrienboer",
            "Dzmitry Bahdanau",
            "Y. Bengio"
        ],
        "references": [
            266797103,
            262877889,
            221012675,
            319770465,
            289758666,
            265554383,
            255173850,
            233981807,
            233409624,
            221013286
        ]
    },
    {
        "id": 255983764,
        "title": "Pylearn2: A machine learning research library",
        "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.",
        "date": "August 2013",
        "authors": [
            "Ian Goodfellow",
            "David Warde-Farley",
            "Pascal Lamblin",
            "Vincent Dumoulin"
        ],
        "references": [
            266225209,
            319770411,
            319770395,
            312538118,
            306218037,
            304109482,
            303256841,
            271513141,
            267960550,
            266031774
        ]
    },
    {
        "id": 243781690,
        "title": "Untersuchungen zu dynamischen neuronalen Netzen",
        "abstract": "",
        "date": "April 1991",
        "authors": [
            "Sepp Hochreiter"
        ],
        "references": [
            311100814,
            341530631,
            313601183,
            312986214,
            291743705,
            286375944,
            285483611,
            273130022,
            266850402,
            265461524
        ]
    },
    {
        "id": 279394722,
        "title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks",
        "abstract": "In this paper we propose and investigate a novel nonlinear unit, called L p unit, for deep neural networks. The proposed L p unit receives signals from several projections of a subset of units in the layer below and computes a normalized L p norm. We notice two interesting interpretations of the L p unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the L p unit is, to a certain degree, similar to the recently proposed maxout unit [13] which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the L p unit is more efficient at representing complex, nonlinear separating boundaries. Each L p unit defines a superelliptic boundary, with its exact shape defined by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few L p units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed L p units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the L p units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed L p unit on the recently proposed deep recurrent neural networks (RNN).",
        "date": "September 2014",
        "authors": [
            "Caglar Gulcehre",
            "Kyunghyun Cho",
            "Razvan Pascanu",
            "Y. Bengio"
        ],
        "references": [
            258816388,
            258247515,
            255983764,
            234140324,
            233866039,
            233753224,
            233730646,
            228467601,
            228102719,
            228095594
        ]
    },
    {
        "id": 265554383,
        "title": "Sequence to Sequence Learning with Neural Networks",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.7 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a strong phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which beats the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "date": "September 2014",
        "authors": [
            "Ilya Sutskever",
            "Oriol Vinyals",
            "Quoc V. Le"
        ],
        "references": [
            270877878,
            265386055,
            263086337,
            243781690,
            221620298,
            221618573,
            221346365,
            51968606,
            13853244,
            5583935
        ]
    },
    {
        "id": 262395872,
        "title": "Random Search for Hyper-Parameter Optimization",
        "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.",
        "date": "March 2012",
        "authors": [
            "James Bergstra",
            "Y. Bengio"
        ],
        "references": [
            227447179,
            221346269,
            221345414,
            220319875,
            215991023,
            45130828,
            10710734,
            6026283,
            2985446,
            321133827
        ]
    },
    {
        "id": 258818168,
        "title": "Speech Recognition with Deep Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
        "date": "March 2013",
        "authors": [
            "Alex Graves",
            "Abdel-rahman Mohamed",
            "Geoffrey Hinton"
        ],
        "references": [
            261119155,
            255564594,
            230875873,
            224216007,
            224149891,
            221620610,
            221619960,
            221617663,
            221484545,
            221346365
        ]
    },
    {
        "id": 255173850,
        "title": "Generating Sequences With Recurrent Neural Networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
        "date": "August 2013",
        "authors": [
            "Alex Graves"
        ],
        "references": [
            266485700,
            228095594,
            227458453,
            224626482,
            221620610,
            220320057,
            220017637,
            13853244,
            5583935,
            3302208
        ]
    },
    {
        "id": 262402839,
        "title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing",
        "abstract": "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transition-based system for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages.",
        "date": "July 2012",
        "authors": [
            "Bernd Bohnet",
            "Joakim Nivre"
        ],
        "references": [
            266887545,
            234809510,
            234796040,
            230876740,
            230876736,
            268720443,
            266034441,
            262355038,
            234828464,
            228981212
        ]
    },
    {
        "id": 262323558,
        "title": "Capturing paradigmatic and syntagmatic lexical relations: towards accurate Chinese part-of-speech tagging",
        "abstract": "From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a state-of-the-art baseline.",
        "date": "July 2012",
        "authors": [
            "Weiwei Sun",
            "Hans Uszkoreit"
        ],
        "references": [
            228734693,
            221481293,
            221013125,
            288905957,
            285905347,
            269031283,
            234812408,
            226270700,
            221101487,
            221013109
        ]
    },
    {
        "id": 228916842,
        "title": "Statistical dependency analysis with support vector machines",
        "abstract": "In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90% accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures.",
        "date": "January 2003",
        "authors": [
            "Hiroyasu Yamada",
            "Yuji Matsumoto"
        ],
        "references": [
            220874244,
            220482445,
            220017637,
            301864384,
            260730920,
            221101482,
            220874091,
            220343911,
            200033835,
            2538570
        ]
    },
    {
        "id": 228362666,
        "title": "A classifier-based parser with linear run-time complexity",
        "abstract": "We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an exten-sion of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classifi-cation, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.",
        "date": "November 2005",
        "authors": [
            "Kenji Sagae",
            "Alon Lavie"
        ],
        "references": [
            228916842,
            228707544,
            221013189,
            220817026,
            220017637,
            221013334,
            220873245,
            2867292,
            2547815,
            2540346
        ]
    },
    {
        "id": 270877686,
        "title": "Exploiting Lexical Dependencies from Large-Scale Data for Better Shift-Reduce Constituency Parsing",
        "abstract": "This paper proposes a method to improve shift-reduce constituency parsing by using lexical dependencies. The lexical dependency information is obtained from a large amount of auto-parsed data that is generated by a baseline shift-reduce parser on unlabeled data. We then incorporate a set of novel features defined on this information into the shift-reduce parsing model. The features can help to disambiguate action conflicts during decoding. Experimental results show that the new features achieve absolute improvements over a strong baseline by 0.9% and 1.1% on English and Chinese respectively. Moreover, the improved parser outperforms all previously reported shift-reduce constituency parsers.",
        "date": "December 2012",
        "authors": [
            "Muhua Zhu",
            "Jingbo Zhu",
            "Huizhen Wang"
        ],
        "references": [
            221013081,
            221012688,
            220873576
        ]
    },
    {
        "id": 262361657,
        "title": "Utilizing dependency language models for graph-based dependency parsing models",
        "abstract": "Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency languagemodel and beam search. The dependency language model is built on a large-amount of additional auto-parsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.",
        "date": "July 2012",
        "authors": [
            "Wenliang Chen",
            "Min Zhang",
            "Haizhou Li"
        ],
        "references": [
            237533325,
            228916842,
            221013081,
            221013054,
            221012782,
            220874856,
            220874801,
            220873136,
            220873101,
            220017637
        ]
    },
    {
        "id": 262349927,
        "title": "Discriminative Reranking for Natural Language Parsing",
        "abstract": "",
        "date": "June 2000",
        "authors": [
            "Michael Collins"
        ],
        "references": []
    },
    {
        "id": 243787298,
        "title": "On the parameter space of generative lexicalized statistical parsing models",
        "abstract": "In this thesis, we apply as well as develop techniques and methodologies for the examination of the complex systems that are lexicalized statistical parsing models. The primary idea is that of treating the \u201cmodel as data\u201d, which is not a particular method, but a paradigm and a research methodology. Our argument is that lexicalized statistical parsing models have become increasingly complex, and therefore require thorough scrutiny, both to achieve the scientific aim of understanding what has been built thus far, and to achieve both the scientific and engineering goal of using that understanding for progress. In this thesis, we take a particular, dominant type of parsing model and perform a macro analysis, to reveal its core (and design a software engine that modularizes the periphery), and we also crucially perform a detailed analysis, which provides for the first time a window onto the efficacy of specific parameters. These analyses have not only yielded insight into the core model, but they have also enabled the identification of \u201cinefficiencies\u201d in our baseline model, such that those inefficiencies can be reduced to form a more compact model, or exploited for finding a better-estimated model with higher accuracy, or both.",
        "date": "January 2004",
        "authors": [
            "Dan Bikel"
        ],
        "references": [
            221101811,
            220875062,
            220874982,
            220873033,
            220816862,
            220708270,
            220685657,
            220017637,
            220017613,
            216300475
        ]
    },
    {
        "id": 233823603,
        "title": "Practical Structured Learning Techniques for Natural Language Processing",
        "abstract": "",
        "date": "August 2006",
        "authors": [
            "Daume H C III"
        ],
        "references": []
    },
    {
        "id": 270877878,
        "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation",
        "abstract": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condition, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, featurerich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang's (2007) original Hiero implementation. Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.",
        "date": "June 2014",
        "authors": [
            "Jacob Devlin",
            "Rabih Zbib",
            "Zhongqiang Huang",
            "Thomas Lamar"
        ],
        "references": [
            290650943,
            319770436,
            312369762,
            289690359,
            287008400,
            285895924,
            285432248,
            283112108,
            277186684,
            270878785
        ]
    },
    {
        "id": 265386055,
        "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation",
        "abstract": "The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.",
        "date": "September 2014",
        "authors": [
            "Jean Pouget-Abadie",
            "Dzmitry Bahdanau",
            "Bart van Merrienboer",
            "Kyunghyun Cho"
        ],
        "references": [
            220816913,
            258818168
        ]
    },
    {
        "id": 319770418,
        "title": "Maxout Networks",
        "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
        "date": "February 2013",
        "authors": [
            "Ian Goodfellow",
            "David Warde-Farley",
            "Mehdi Mirza",
            "Aaron Courville"
        ],
        "references": []
    },
    {
        "id": 307174794,
        "title": "Statistical Machine Translation",
        "abstract": "This introductory text to statistical machine translation (SMT) provides all of the theories and methods needed to build a statistical machine translator, such as Google Language Tools and Babelfish. In general, statistical techniques allow automatic translation systems to be built quickly for any language-pair using only translated texts and generic software. With increasing globalization, statistical machine translation will be central to communication and commerce. Based on courses and tutorials, and classroom-tested globally, it is ideal for instruction or self-study, for advanced undergraduates and graduate students in computer science and/or computational linguistics, and researchers in natural language processing. The companion website provides open-source corpora and tool-kits.",
        "date": "January 2009",
        "authors": [
            "Philipp Koehn"
        ],
        "references": [
            265657436,
            237327924,
            237247141,
            237247130,
            234815897,
            234815723,
            234811483,
            234802687,
            230668373,
            228945619
        ]
    },
    {
        "id": 303256841,
        "title": "Theano: a CPU and GPU math expression compiler",
        "abstract": "",
        "date": "January 2010",
        "authors": [
            "James Bergstra",
            "O. Breuleux",
            "Frederic Bastien",
            "Pascal Lamblin"
        ],
        "references": []
    },
    {
        "id": 289758666,
        "title": "Recurrent continuous translation models",
        "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
        "date": "January 2013",
        "authors": [
            "Nal Kalchbrenner",
            "P. Blunsom"
        ],
        "references": [
            302879092,
            270878336,
            255563511
        ]
    },
    {
        "id": 270878785,
        "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
        "abstract": "",
        "date": "December 2012",
        "authors": [
            "Holger Schwenk"
        ],
        "references": []
    },
    {
        "id": 261309981,
        "title": "Hybrid speech recognition with Deep Bidirectional LSTM",
        "abstract": "Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.",
        "date": "December 2013",
        "authors": [
            "Alex Graves",
            "Navdeep Jaitly",
            "Abdel-rahman Mohamed"
        ],
        "references": [
            255564594,
            230875873,
            228828379,
            224216007,
            221346365,
            220320057,
            13853244,
            3316656,
            3302208,
            267706055
        ]
    },
    {
        "id": 319770387,
        "title": "Deep Sparse Rectifier Neural Networks",
        "abstract": "While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training",
        "date": "January 2011",
        "authors": [
            "Xavier Glorot",
            "Antoine Bordes",
            "Y. Bengio"
        ],
        "references": []
    },
    {
        "id": 319770183,
        "title": "Imagenet classification with deep convolutional neural networks",
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry",
        "date": "January 2012",
        "authors": [
            "Alex Krizhevsky",
            "I Sutskever",
            "G Hinton"
        ],
        "references": []
    },
    {
        "id": 312369762,
        "title": "Continuous space translation models with neural networks",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "Le Hai Son",
            "A. Allauzen",
            "F. Yvon"
        ],
        "references": [
            302963741
        ]
    },
    {
        "id": 307955489,
        "title": "Distributed representations of words and phrases and their compositionality",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "T. Mikolov"
        ],
        "references": []
    },
    {
        "id": 267960550,
        "title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "date": "January 2012",
        "authors": [
            "Alex Krizhevsky",
            "Ilya Sutskever",
            "Geoffrey E. Hinton"
        ],
        "references": [
            258627767,
            244947191,
            228102719,
            221376179,
            221361415,
            221344904,
            220860992,
            220016377,
            48199502,
            40443832
        ]
    },
    {
        "id": 233981807,
        "title": "ADADELTA: An adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",
        "date": "December 2012",
        "authors": [
            "Matthew D. Zeiler"
        ],
        "references": [
            266225209,
            236735729,
            225273758,
            216792889,
            265350973,
            236736791,
            229091480,
            221497515
        ]
    },
    {
        "id": 228379274,
        "title": "EuroParl: A parallel corpus for statistical machine translation",
        "abstract": "We collected a corpus of parallel text in 11 lan-guages from the proceedings of the European Par-liament, which are published on the web 1 . This cor-pus has found widespread use in the NLP commu-nity. Here, we focus on its acquisition and its appli-cation as training data for statistical machine trans-lation (SMT). We trained SMT systems for 110 lan-guage pairs, which reveal interesting clues into the challenges ahead.",
        "date": "November 2004",
        "authors": [
            "Philipp Koehn"
        ],
        "references": [
            234795504,
            230875890,
            220873989,
            220355307,
            220873246,
            220839665,
            2662390
        ]
    },
    {
        "id": 221617806,
        "title": "PAC Generalization Bounds for Co-training.",
        "abstract": "The rule-based bootstrapping introduced by Yarowsky, and its co- training variant by Blum and Mitchell, have met with considerable em- pirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences \u2014 partial rules and partial labeling of the unlabeled data \u2014 and the use of an agreement-based objective function as sug- gested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . One well-known form of bootstrapping is the EM algorithm (Dempster, Laird and Rubin, 1977). This algorithm iteratively updates model parameters by using the current model to infer (a probability distribution on) labels for the unlabeled data and then adjusting the model parameters to fit the (distribution on) filled-in labels. When the model defines a joint probability distribution over observable data and unobservable labels, each iteration of the EM algorithm can be shown to increase the probability of the observable data given the model parameters. However, EM is often subject to local minima \u2014 situations in which the filled-in data and the model parameters fit each other well but the model parameters are far from their maximum-likelihood values. Furthermore, even if EM does find the globally optimal maximum likelihood parameters, a model with a large number of parameters will over-fit the data. No PAC-style guarantee has yet been given for the generalization accuracy of the maximum likelihood model.",
        "date": "January 2001",
        "authors": [
            "Sanjoy Dasgupta",
            "Michael L. Littman",
            "David Allen Mcallester"
        ],
        "references": [
            221995817,
            2457211,
            2298037
        ]
    },
    {
        "id": 268237865,
        "title": "Better k -best parsing",
        "abstract": "We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.",
        "date": "November 2005",
        "authors": [
            "Liang Huang",
            "David Chiang"
        ],
        "references": [
            225179572,
            220873101,
            220355462,
            220017637,
            2850230,
            2356246,
            312607081,
            303147113,
            262349927,
            247442902
        ]
    },
    {
        "id": 230876697,
        "title": "North American News Text Corpus",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "David Graff"
        ],
        "references": []
    },
    {
        "id": 230876655,
        "title": "Applying Cotraining Methods to Statistical Parsing",
        "abstract": "",
        "date": "January 2001",
        "authors": [
            "S Anoop"
        ],
        "references": []
    },
    {
        "id": 229078318,
        "title": "Combining Labeled and Unlabeld Data with Co-Training",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "Avrim Blum",
            "Tom Mitchell"
        ],
        "references": []
    },
    {
        "id": 223311595,
        "title": "MAP adaptation of stochastic grammars",
        "abstract": "This paper investigates supervised and unsupervised adaptation of stochastic grammars, including n-gram language models and probabilistic context-free grammars (PCFGs), to a new domain. It is shown that the commonly used approaches of count merging and model interpolation are special cases of a more general maximum a posteriori (MAP) framework, which additionally allows for alternate adaptation approaches. This paper investigates the effectiveness of different adaptation strategies, and, in particular, focuses on the need for supervision in the adaptation process. We show that n-gram models as well as PCFGs benefit from either supervised or unsupervised MAP adaptation in various tasks. For n-gram models, we compare the benefit from supervised adaptation with that of unsupervised adaptation on a speech recognition task with an adaptation sample of limited size (about 17 h), and show that unsupervised adaptation can obtain 51% of the 7.7% adaptation gain obtained by supervised adaptation. We also investigate the benefit of using multiple word hypotheses (in the form of a word lattice) for unsupervised adaptation on a speech recognition task for which there was a much larger adaptation sample available. The use of word lattices for adaptation required the derivation of a generalization of the well-known Good-Turing estimate.",
        "date": "January 2006",
        "authors": [
            "Michiel Bacchiani",
            "Michael Riley",
            "Brian Roark",
            "Richard Sproat"
        ],
        "references": [
            263890871,
            220685657,
            220475988,
            220355374,
            4084664,
            3333325,
            2955633,
            2927500,
            2910947,
            2876571
        ]
    },
    {
        "id": 221275760,
        "title": "Computation of the N Best Parse Trees for Weighted and Stochastic Context-Free Grammars",
        "abstract": "Context-Free Grammars are the object of increasing interest in the pattern recognition research community in an attempt to overcome the limited modeling capabilities of the simpler regular grammars, and have application in a variety of fields such as language modeling, speech recognition, optical character recognition, computational biology, etc. This paper proposes an efficient algorithm to solve one of the problems associated to the use of weighted and stochastic Context-Free Grammars: the problem of computing the N best parse trees of a given string. After the best parse tree has been computed using the CYK algorithm, a large number of alternative parse trees are obtained, in order by weight (or probability), in a small fraction of the time required by the CYK algorithm to find the best parse tree. This is confirmed by experimental results using grammars from two different domains: a chromosome grammar, and a grammar modeling natural language sentences from the Wall Street Journal corpus.",
        "date": "August 2000",
        "authors": [
            "V\u00edctor M. Jim\u00e9nez",
            "Andr\u00e9s Marzal"
        ],
        "references": [
            242394905,
            221482600,
            221478582,
            31447209,
            312607081,
            267149006,
            262493823,
            242361757,
            236411695,
            222994624
        ]
    },
    {
        "id": 220875255,
        "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
        "abstract": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser\u2013Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.",
        "date": "January 1996",
        "authors": [
            "Stanley F. Chen",
            "Joshua Goodman"
        ],
        "references": [
            301144179,
            248593046,
            230875889,
            224377724,
            221102042,
            220685657,
            220355244,
            1782255,
            313201001,
            303517837
        ]
    },
    {
        "id": 220875180,
        "title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking",
        "abstract": "Discriminative reranking is one method for constructing high-performance statis- tical parsers (Collins, 2000). A discrim- inative reranker requires a source of can- didate parses for each sentence. This pa- per describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method gener- ates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sen- tence, obtaining an f-score of 91.0% on sentences of length 100 or less.",
        "date": "January 2005",
        "authors": [
            "Eugene Charniak",
            "Mark Johnson"
        ],
        "references": [
            220874539,
            51991698,
            3491928,
            2921800,
            2839498,
            2561857,
            268237865,
            262349927,
            259810695,
            228609663
        ]
    },
    {
        "id": 221101491,
        "title": "When is Self-Training Effective for Parsing?",
        "abstract": "Self-training has been shown capable of improving on state-of-the-art parser per- formance (McClosky et al., 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak, 1997; Steedman et al., 2003). However, it has remained unclear when and why self- training is helpful. In this paper, we test four hypotheses (namely, presence of a phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better un- derstanding of why self-training works for parsing. Since improvements from self- training are correlated with unknown bi- grams and biheads but not unknown words, the benefit of self-training appears most in- fluenced by seeing known words in new combinations.",
        "date": "January 2008",
        "authors": [
            "David McClosky",
            "Eugene Charniak",
            "Mark Johnson"
        ],
        "references": [
            262408350,
            239807299,
            220874688,
            220873863,
            220817154,
            312607081,
            238198632,
            230876697,
            220875180,
            220873964
        ]
    },
    {
        "id": 221012635,
        "title": "Mandarin Part-of-Speech Tagging and Discriminative Reranking.",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Zhongqiang Huang",
            "Mary Harper",
            "Wen Wang"
        ],
        "references": [
            228734693,
            224640972,
            247932100,
            228649189,
            228057950,
            225070191,
            221346436,
            221102892,
            220875180,
            220355542
        ]
    },
    {
        "id": 220875081,
        "title": "Probabilistic CFG with Latent Annotations.",
        "abstract": "This paper defines a generative probabilis- tic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine- grained CFG rules are automatically in- duced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are de- scribed and empirically compared. In ex- periments using the Penn WSJ corpus, our automatically trained model gave a per- formance of 86.6% (F , sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.",
        "date": "January 2005",
        "authors": [
            "Takuya Matsuzaki",
            "Yusuke Miyao",
            "Jun'ichi Tsujii"
        ],
        "references": [
            220816667,
            292453644,
            246297098,
            228523594,
            225740821,
            220873944,
            220488267,
            220355517,
            2929293,
            2922698
        ]
    },
    {
        "id": 221012840,
        "title": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing.",
        "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF gram- mar over trees, learned by iteratively splitting grammar productions (not categories). Dif- ferent regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond lo- cal tree configurations. We present a multi- scale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
        "date": "January 2008",
        "authors": [
            "Slav Petrov",
            "Dan Klein"
        ],
        "references": [
            221013249,
            221012623,
            220875081,
            220874765,
            220874045,
            220873863,
            220873596,
            220017637,
            2921800,
            2476043
        ]
    },
    {
        "id": 220874256,
        "title": "Semi-Supervised Convex Training for Dependency Parsing.",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "Qin Iris Wang",
            "Dale Schuurmans",
            "Dekang Lin"
        ],
        "references": [
            262408350,
            228916842,
            228058014,
            221101491,
            220874801,
            220874243,
            220874121,
            220873101,
            220320285,
            220017637
        ]
    },
    {
        "id": 220873964,
        "title": "Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets",
        "abstract": "Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains. In this paper we use self- training in order to improve the quality of a parser and to adapt it to a different do- main, using only small amounts of manually annotated seed data. We report significant improvement both when the seed and test data are in the same domain and in the out- of-domain adaptation scenario. In particu- lar, we achieve 50% reduction in annotation cost for the in-domain case, yielding an im- provement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied suc- cessfully to these tasks. We were also able to formulate a characterization of when self- training is valuable.",
        "date": "January 2007",
        "authors": [
            "Roi Reichart",
            "Ari Rappoport"
        ],
        "references": [
            262408350,
            221101491,
            220874801,
            220814356,
            2927500,
            2477043,
            230876696,
            228796527,
            223311595,
            220875180
        ]
    },
    {
        "id": 220873958,
        "title": "Forest Reranking: Discriminative Parsing with Non-Local Features.",
        "abstract": "Conventional n-best reranking techniques of- ten suffer from the limited scope of the n- best list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed for- est of exponentially many parses. Since ex- act inference is intractable with non-local fea- tures, we present an approximate algorithm in- spired by forest rescoring that makes discrim- inative training practical over the whole Tree- bank. Our final result, an F-score of 91.7, out- performs both 50-best and 100-best reranking baselines, and is better than any previously re- ported systems trained on the Treebank.",
        "date": "January 2008",
        "authors": [
            "Liang Huang"
        ],
        "references": [
            262408350,
            221012623,
            220874045,
            220873101,
            220017637,
            2850230,
            2561857,
            2481913,
            268237865,
            262349927
        ]
    },
    {
        "id": 220874045,
        "title": "Discriminative Training of a Neural Network Statistical Parser.",
        "abstract": "Discriminative methods have shown signican t improvements over traditional generative meth- ods in many machine learning applications, but there has been dicult y in extending them to natural language parsing. One problem is that much of the work on discriminative methods conates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a dis- criminative learning method while still param- eterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one gen- erative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model out- performs the previous two, achieving state-of- the-art levels of performance (90.1% F-measure on constituents).",
        "date": "January 2004",
        "authors": [
            "James Henderson"
        ],
        "references": [
            220816667,
            220017637,
            262349927,
            246304479,
            220875022,
            220355517,
            220343911,
            215721451,
            200033835,
            4355809
        ]
    },
    {
        "id": 298836450,
        "title": "Automatic word sense discrimination",
        "abstract": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering, Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous worn are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from re corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.",
        "date": "March 1998",
        "authors": [
            "H Schutze"
        ],
        "references": [
            221299854,
            2633937,
            238984455,
            232878134,
            230854746,
            229593960,
            228057706,
            223483101,
            222393384,
            222281706
        ]
    },
    {
        "id": 292453644,
        "title": "Computational complexity of probabilistic disambiguation: NP-completeness results for parsing problems that arise in speech and language processing applications",
        "abstract": "Recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. In this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e.g., derivations, parse-trees and sentences. The extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e.g., maximum probability parse-tree given some input sentence). The implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. In this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. We provide proofs that some, frequently occurring problems of ambiguity resolution are NP-complete. These problems are encountered in various applications, e.g., language understanding for text- and speech-based applications. Assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems.",
        "date": "January 2002",
        "authors": [
            "K. Sima'an"
        ],
        "references": []
    },
    {
        "id": 288942058,
        "title": "A clustering technique for summarizing multivariate data",
        "abstract": "",
        "date": "January 1976",
        "authors": [
            "G. Ball",
            "D. Hall"
        ],
        "references": []
    },
    {
        "id": 242430108,
        "title": "Aspects of The Theory of Syntax",
        "abstract": "",
        "date": "February 1970",
        "authors": [
            "Noam Chomsky"
        ],
        "references": [
            235039252,
            9517409,
            324640748,
            324304653,
            320181926,
            313154879,
            303837236,
            290803936,
            285058764,
            284399676
        ]
    },
    {
        "id": 225740821,
        "title": "Computational Complexity of Probabilistic Disambiguation",
        "abstract": "Recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. In this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e.g., derivations, parse-trees and sentences. The extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e.g., maximum probability parse-tree given some input sentence). The implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. In this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. We provide proofs that some, frequently occurring problems of ambiguity resolution are NP-complete. These problems are encountered in various applications, e.g., language understanding for text- and speech-based applications. Assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems.",
        "date": "August 2002",
        "authors": [
            "Khalil Sima'an"
        ],
        "references": [
            319393405,
            242394905,
            221101811,
            220874758,
            220873033,
            220816736,
            220017637,
            38358970,
            2607825,
            2475419
        ]
    },
    {
        "id": 225517426,
        "title": "Inducing probabilistic grammars by Bayesian model merging",
        "abstract": "We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (Occam's Razor). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.",
        "date": "January 1970",
        "authors": [
            "Andreas Stolcke",
            "Stephen Omohundro"
        ],
        "references": [
            274364636,
            220312496,
            220049445,
            38364607,
            35846898
        ]
    },
    {
        "id": 221112104,
        "title": "Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing",
        "abstract": "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization? In this paper, we show how to induce a probabilistic parser with latent head infor- mation from simple linguistic principles. Our parser has a performance of 85.1% (LP/LR F1), which is as good as that of early lexicalized ones. This is remarkable since the induction of probabilistic grammars is in general a hard task.",
        "date": "October 2005",
        "authors": [
            "Detlef Prescher"
        ],
        "references": [
            236157548,
            230876175,
            220873033,
            220817598,
            220017637,
            2921800,
            2912527,
            2522353,
            2445064,
            312607081
        ]
    },
    {
        "id": 277298117,
        "title": "Bridging Long Time Lags by Weight Guessing and \"Long Short Term Memory\"",
        "abstract": "Numerous recent papers (including many NIPS papers) focus on standard recurrent nets' inability to deal with long time lags between relevant input signals and teacher signals. Rather sophisticated, alternative methods were proposed. We first show: problems used to promote certain algorithms in numerous previous papers can be solved more quickly by random weight guessing than by the proposed algorithms. This does not mean that guessing is a good algorithm. It just casts doubt on whether the other algorithms are, or whether the chosen problems are meaningful. We then use long short term memory (LSTM), our own recent algorithm, to solve hard problems that can neither be quickly solved by random weight guessing nor by any other recurrent net algorithm we are aware of. 1 Introduction / Outline Many recent papers focus on standard recurrent nets' inability to deal with long time lags between relevant signals. See, e.g., Bengio et al., El Hihi and Bengio, and others [3, 1, 6, 15]. Rather s...",
        "date": "January 1996",
        "authors": [
            "Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            277295865,
            243781690,
            243698906,
            243683010,
            226171158,
            313702304,
            243763246,
            240434405,
            239594484,
            239062408
        ]
    },
    {
        "id": 277295865,
        "title": "Guessing Can Outperform Many Long Time Lag Algorithms",
        "abstract": "Numerous recent papers focus on standard recurrent nets' problems with long time lags between relevant signals. Some propose rather sophisticated, alternative methods. We show: many problems used to test previous methods can be solved more quickly by random weight guessing.",
        "date": "January 1996",
        "authors": [
            "J\u00fcrgen Schmidhuber",
            "Sepp Hochreiter"
        ],
        "references": [
            243781690,
            238833309,
            226171158,
            220500076,
            243683189,
            239594484,
            223416710,
            220500234,
            220500087,
            220359636
        ]
    },
    {
        "id": 243698906,
        "title": "Finite State Automata and Simple Recurrent Networks",
        "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t\u22121, together with element t, to predict element t + 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.",
        "date": "September 1989",
        "authors": [
            "Axel Cleeremans",
            "David Servan-Schreiber",
            "James L Mcclelland"
        ],
        "references": [
            243743707,
            242353012,
            239059398,
            230876307,
            229091480,
            222438973
        ]
    },
    {
        "id": 243683010,
        "title": "The utility driven dynamic error propagation network",
        "abstract": "Error propagation networks are able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns. Three possible architectures are explored which deal with learning sequences of known finite length and sequences of unknown and possibly infinite length. Several examples are given and an application to speech coding is discussed. A further development of dynamic nets is made which allows them to be trained by a signal which expresses the correctness of the output of the net, the utility signal. One possible architecture for such a utility driven dynandc net is given and a simple example is presented. Utility driven dynamic nets are potentially able to calculate and maximise any function of the input and output data streams, within the comidered conext. This is a very powerful property, and an appendix presents a comparison of the information processing in utility driven dynamic nets and that in the human brain.",
        "date": "January 1987",
        "authors": [
            "Tony Robinson",
            "F. Fallside"
        ],
        "references": []
    },
    {
        "id": 313702304,
        "title": "Learning sequential structures with the real-time recurrent learning algorithm",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "A.W. Smith",
            "D. Zipser"
        ],
        "references": []
    },
    {
        "id": 263902178,
        "title": "LEARNING SEQUENTIAL STRUCTURE WITH THE REAL-TIME RECURRENT LEARNING ALGORITHM",
        "abstract": "Recurrent connections in neural networks potentially allow information about events occurring in the past to be preserved and used in current computations. How effectively this potential is realized depends on the power of the learning algorithm used. As an example of a task requiring recurrency, Servan-Schreiber, Cleeremans, and McClelland1 have applied a simple recurrent learning algorithm to the task of recognizing finite-state grammars of increasing difficulty. These nets showed considerable power and were able to learn fairly complex grammars by emulating the state machines that produced them. However, there was a limit to the difficulty of the grammars that could be learned. We have applied a more powerful recurrent learning procedure, called real-time recurrent learning2,6 (RTRL), to some of the same problems studied by Servan-Schreiber, Cleeremans, and McClelland. The RTRL algorithm solved more difficult forms of the task than the simple recurrent networks. The internal representations developed by RTRL networks revealed that they learn a rich set of internal states that represent more about the past than is required by the underlying grammar. The dynamics of the networks are determined by the state structure and are not chaotic.",
        "date": "November 2011",
        "authors": [
            "Anthony W.smith",
            "Davidzipser"
        ],
        "references": []
    },
    {
        "id": 243763246,
        "title": "A recurrent cascade-correlation learning architecture",
        "abstract": "",
        "date": "January 1991",
        "authors": [
            "Scott E. Fahlman",
            "Christian Lebiere"
        ],
        "references": []
    },
    {
        "id": 243732587,
        "title": "Generalization of Back-Propagation to Recurrent Neural Networks",
        "abstract": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \u03b4 rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.",
        "date": "November 1987",
        "authors": [
            "Fernando J. Pineda"
        ],
        "references": [
            16609657,
            345402887,
            243243435,
            3115638
        ]
    },
    {
        "id": 319770123,
        "title": "Densely Connected Convolutional Networks",
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).",
        "date": "August 2016",
        "authors": [
            "Gao Huang",
            "Zhuang Liu",
            "Kilian Weinberger"
        ],
        "references": [
            308278012,
            319770414,
            319770387,
            319770216,
            319770191,
            319770183,
            311609041,
            309076254,
            306281834,
            306218037
        ]
    },
    {
        "id": 312216100,
        "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",
        "date": "January 2017",
        "authors": [
            "Guillaume Klein",
            "Yoon Kim",
            "Yuntian Deng",
            "Jean Senellart"
        ],
        "references": [
            308262743,
            305334401,
            272194766,
            266225209,
            262877889,
            220874004,
            319770465,
            308646556,
            306093856,
            284788001
        ]
    },
    {
        "id": 311223153,
        "title": "Speed/accuracy trade-offs for modern convolutional object detectors",
        "abstract": "In this paper, we study the trade-off between accuracy and speed when building an object detection system based on convolutional neural networks. We consider three main families of detectors --- Faster R-CNN, R-FCN and SSD --- which we view as \"meta-architectures\". Each of these can be combined with different kinds of feature extractors, such as VGG, Inception or ResNet. In addition, we can vary other parameters, such as the image resolution, and the number of box proposals. We develop a unified framework (in Tensorflow) that enables us to perform a fair comparison between all of these variants. We analyze the performance of many different previously published model combinations, as well as some novel ones, and thus identify a set of models which achieve different points on the speed-accuracy tradeoff curve, ranging from fast models, suitable for use on a mobile phone, to a much slower model that achieves a new state of the art on the COCO detection challenge.",
        "date": "November 2016",
        "authors": [
            "Jonathan Huang",
            "Vivek Rathod",
            "Chen Sun",
            "Menglong Zhu"
        ],
        "references": [
            319770289,
            316184205,
            312759848,
            319770430,
            319770416,
            319770411,
            319770183,
            319769911,
            313760952,
            312430297
        ]
    },
    {
        "id": 309766061,
        "title": "A Convolutional Encoder Model for Neural Machine Translation",
        "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and conceptually simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. We achieve a new state-of-the-art on WMT'16 English-Romanian translation and outperform several recently published results on the WMT'15 English-German task. We also achieve almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.",
        "date": "November 2016",
        "authors": [
            "Jonas Gehring",
            "Michael Auli",
            "David Grangier",
            "Yann N. Dauphin"
        ],
        "references": [
            308884050,
            308807302,
            319770465,
            319770411,
            319769995,
            319769905,
            310626854,
            306094124,
            306093962,
            306093909
        ]
    },
    {
        "id": 307747289,
        "title": "Show and tell: A neural image caption generator",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Oriol Vinyals",
            "Alexander Toshev",
            "Samy Bengio",
            "Dumitru Erhan"
        ],
        "references": [
            329977566,
            329975395,
            319770465,
            319770439,
            319770357,
            312714920,
            308804607,
            303721259,
            290345348,
            272194743
        ]
    },
    {
        "id": 306093640,
        "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
        "abstract": "In this work, we cast abstractive text summarization as a sequence-to-sequence problem and employ the framework of Attentional Encoder-Decoder Recurrent Neural Networks to this problem, outperforming state-of-the art model of Rush et. al. (2015) on two different corpora. We also move beyond the basic architecture, and propose several novel models to address important problems in summarization including modeling key-words, capturing the hierarchy of sentence-to-word structure and addressing the problem of words that are key to a document, but rare elsewhere. Our work shows that many of our proposed solutions contribute to further improvement in performance. In addition, we propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",
        "date": "February 2016",
        "authors": [
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Cicero Nogueira Dos Santos",
            "Caglar Gulcehre"
        ],
        "references": [
            306093072,
            301404465,
            279068977,
            306093832,
            305334286,
            301880883,
            288644117,
            281487270,
            278048272,
            277722709
        ]
    },
    {
        "id": 303657108,
        "title": "TensorFlow: A system for large-scale machine learning",
        "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
        "date": "May 2016",
        "authors": [
            "Mart\u00edn Abadi",
            "Paul Barham",
            "Jianmin Chen",
            "Zhifeng Chen"
        ],
        "references": [
            305196650,
            319770465,
            319770439,
            319770183,
            319769811,
            311610318,
            311474948,
            308575069,
            307799813,
            306209959
        ]
    },
    {
        "id": 273640320,
        "title": "LSTM: A search space odyssey",
        "abstract": "Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.",
        "date": "March 2015",
        "authors": [
            "Klaus Greff",
            "Rupesh Kumar Srivastava",
            "Jan Koutn\u00edk",
            "Bas R. Steunebrink"
        ],
        "references": [
            308034527,
            287850221,
            317646849,
            287741874,
            287059155,
            286271944,
            285008105,
            284867766,
            279815520,
            279714069
        ]
    },
    {
        "id": 273388012,
        "title": "Neural Responding Machine for Short-Text Conversation",
        "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.",
        "date": "March 2015",
        "authors": [
            "Lifeng Shang",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "references": [
            287781150,
            268382939,
            265252627,
            265209669,
            262877889,
            319770465,
            311469848,
            289758666,
            283112108,
            265554383
        ]
    },
    {
        "id": 272194766,
        "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
        "date": "February 2015",
        "authors": [
            "Kelvin Xu",
            "Jimmy Ba",
            "Ryan Kiros",
            "Kyunghyun Cho"
        ],
        "references": [
            307747289,
            270878844,
            329977566,
            319770465,
            319770249,
            319770183,
            308871635,
            303721259,
            286794765,
            269935079
        ]
    },
    {
        "id": 319770289,
        "title": "Deep Neural Networks for Object Detection",
        "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a lowcost by a fewnetwork applications. State-of-the-art performance of the approach is shown on Pascal VOC.",
        "date": "January 2013",
        "authors": [
            "Christian Szegedy",
            "Alexander Toshev",
            "Dumitru Erhan"
        ],
        "references": [
            240308781,
            230690781,
            221363369,
            221361415,
            319770820,
            319770183,
            267709055,
            237417270,
            221497515,
            221364242
        ]
    },
    {
        "id": 289917319,
        "title": "Deep Neural Networks for object detection",
        "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.",
        "date": "January 2013",
        "authors": [
            "C. Szegedy",
            "Alexander Toshev",
            "Dumitru Erhan"
        ],
        "references": [
            240308781,
            230690781,
            319770820,
            319770183,
            281327886,
            267960550,
            267709055,
            237417270,
            221497515,
            221364242
        ]
    },
    {
        "id": 266657849,
        "title": "Scaling up matrix computations on shared-memory manycore systems with 1000 CPU cores",
        "abstract": "While the growing number of cores per chip allows researchers to solve larger scientific and engineering problems, the parallel efficiency of the deployed parallel software starts to decrease. This unscalability problem happens to both vendor-provided and open-source software and wastes CPU cycles and energy. By expecting CPUs with hundreds of cores to be imminent, we have designed a new framework to perform matrix computations for massively many cores. Our performance analysis on manycore systems shows that the unscalability bottleneck is related to Non-Uniform Memory Access (NUMA): memory bus contention and remote memory access latency. To overcome the bottleneck, we have designed NUMA-aware tile algorithms with the help of a dynamic scheduling runtime system to minimize NUMA memory accesses. The main idea is to identify the data that is, either read a number of times or written once by a thread resident on a remote NUMA node, then utilize the runtime system to conduct data caching and movement between different NUMA nodes. Based on the experiments with QR factorizations, we demonstrate that our framework is able to achieve great scalability on a 48-core AMD Opteron system (e.g., parallel efficiency drops only 3% from one core to 48 cores). We also deploy our framework to an extreme-scale shared-memory SGI machine which has 1024 CPU cores and runs a single Linux operating system image. Our framework continues to scale well, and can outperform the vendor-optimized Intel MKL library by up to 750%.",
        "date": "June 2014",
        "authors": [
            "Fengguang Song",
            "Jack Dongarra"
        ],
        "references": [
            262311378,
            262274001,
            262159992,
            258521061,
            224132799,
            286643249,
            268461720,
            265478227,
            261020725,
            256327878
        ]
    },
    {
        "id": 266225209,
        "title": "Large Scale Distributed Deep Networks",
        "abstract": "Recent work in unsupervised feature learning and deep learning has shown that be-ing able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network train-ing. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k cate-gories. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition ser-vice. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
        "date": "October 2012",
        "authors": [
            "Jeffrey Dean",
            "G.s. Corrado",
            "Rajat Monga",
            "Kai Chen"
        ],
        "references": [
            255564380,
            319770626,
            319770111,
            303256841,
            267429210,
            265748773,
            265178583,
            261051511,
            236736851,
            225818196
        ]
    },
    {
        "id": 319770166,
        "title": "Provable Bounds for Learning Some Deep Representations",
        "abstract": "We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^backslashgamma$ for some $backslashgamma textless1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns $backslash$em almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.",
        "date": "October 2014",
        "authors": [
            "Sanjeev Arora",
            "Aditya Bhaskara",
            "Rong Ge",
            "Tengyu Ma"
        ],
        "references": []
    },
    {
        "id": 310752533,
        "title": "Visualizing and understanding convolutional networks",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "M.D. Zeiler",
            "R. Fergus"
        ],
        "references": []
    },
    {
        "id": 286271944,
        "title": "On the importance of initialization and momentum in deep learning",
        "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
        "date": "January 2013",
        "authors": [
            "I. Sutskever",
            "J. Martens",
            "G. Dahl",
            "G. Hinton"
        ],
        "references": [
            243648538,
            215616968,
            2749736,
            265002936
        ]
    },
    {
        "id": 303137904,
        "title": "Greedy layer-wise training of deep networks",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Y. Bengio",
            "Pascal Lamblin",
            "Dan Popovici",
            "Hugo Larochelle"
        ],
        "references": [
            221620465,
            221618913,
            200744506,
            11294735,
            304109482,
            274549920,
            226568893,
            222000227,
            201841044,
            15437023
        ]
    },
    {
        "id": 267225654,
        "title": "A Two-Stage Pretraining Algorithm for Deep Boltzmann Machines",
        "abstract": "A deep Boltzmann machine (DBM) is a recently introduced Markov random field model that has multiple layers of hidden units. It has been shown empirically that it is difficult to train a DBM with approximate maximum- likelihood learning using the stochastic gradient unlike its simpler special case, restricted Boltzmann machine (RBM). In this paper, we propose a novel pretraining algorithm that consists of two stages; obtaining approximate posterior distributions over hidden units from a simpler model and maximizing the variational lower-bound given the fixed hidden posterior distributions. We show empirically that the proposed method overcomes the difficulty in training DBMs from randomly initialized parameters and results in a better, or comparable, generative model when compared to the conventional pretraining algorithm.",
        "date": "September 2013",
        "authors": [
            "Kyunghyun Cho",
            "Tapani Raiko",
            "Alexander Ilin",
            "Juha Karhunen"
        ],
        "references": [
            267972168,
            261122138,
            240308775,
            231556969,
            230555076,
            228102719,
            304109482,
            289098162,
            280940018,
            267783299
        ]
    },
    {
        "id": 319770367,
        "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
        "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.",
        "date": "April 2014",
        "authors": [
            "Emily Denton",
            "Wojciech Zaremba",
            "Joan Bruna",
            "Yann Lecun"
        ],
        "references": []
    },
    {
        "id": 289733708,
        "title": "Do deep nets really need to be deep?",
        "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.",
        "date": "January 2014",
        "authors": [
            "L.J. Ba",
            "R. Caruana"
        ],
        "references": [
            221653840,
            261230561,
            221487297
        ]
    },
    {
        "id": 273387909,
        "title": "Distilling the Knowledge in a Neural Network",
        "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
        "date": "March 2015",
        "authors": [
            "Geoffrey Hinton",
            "Oriol Vinyals",
            "Jeff Dean"
        ],
        "references": [
            266225209,
            233806999,
            228102719,
            221653840,
            354167136,
            319770183,
            298606994,
            286794765,
            267960550,
            260637318
        ]
    },
    {
        "id": 269877085,
        "title": "Compressing Deep Convolutional Networks using Vector Quantization",
        "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.",
        "date": "December 2014",
        "authors": [
            "Yunchao Gong",
            "Liu Liu",
            "Ming Yang",
            "Lubomir Bourdev"
        ],
        "references": [
            317083797,
            264979485,
            51873001,
            47815472,
            319770430,
            319770367,
            319770363,
            319770276,
            319770111,
            267960550
        ]
    },
    {
        "id": 266031774,
        "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
        "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the prob-lem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we intro-duce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed fea-tures. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.",
        "date": "January 2011",
        "authors": [
            "Yuval Netzer",
            "Tao Wang",
            "Adam Coates",
            "Alessandro Bissacco"
        ],
        "references": [
            242737800,
            224136091,
            221620343,
            221619818,
            221618416,
            221364080,
            221253776,
            221110077,
            220933133,
            220478096
        ]
    },
    {
        "id": 265748773,
        "title": "Learning Multiple Layers of Features from Tiny Images",
        "abstract": "April 8, 2009Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it di cult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signi cantly",
        "date": "May 2012",
        "authors": [
            "Alex Krizhevsky"
        ],
        "references": [
            303137904,
            239571798,
            304109482,
            263499383,
            221346268,
            221346193,
            200044309,
            23253323,
            6912170,
            2821917
        ]
    },
    {
        "id": 319770626,
        "title": "Multi-column Deep Neural Networks for Image Classification",
        "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traf\ufb01c signs. Our biologically plausible, wide and deep arti\ufb01cial neural network architectures can. Small (often minimal) receptive \ufb01elds of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the \ufb01rst to achieve near-human performance. On a traf\ufb01c sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classi\ufb01cation benchmarks.",
        "date": "June 2012",
        "authors": [
            "Dan Ciresan",
            "Ueli Meier",
            "Jurgen Schmidhuber"
        ],
        "references": []
    },
    {
        "id": 291735245,
        "title": "Regularization of neural networks using dropconnect",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "L. Wan",
            "M. Zeiler",
            "Sixn Zhang",
            "Y.L. Cun"
        ],
        "references": []
    },
    {
        "id": 286569315,
        "title": "Discriminative transfer learning with tree-based priors",
        "abstract": "High capacity classifiers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classification performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classifier's parameters. We show that the performance of deep neural networks can be improved by applying these priors to the weights in the last layer. Our method combines the strength of discriminatively trained deep neural networks, which typically require large amounts of training data, with tree-based priors, making deep neural networks work well on infrequent classes as well. We also propose an algorithm for learning the underlying tree structure. Starting from an initial pre-specified tree, this algorithm modifies the tree to make it more pertinent to the task being solved, for example, removing semantic relationships in favour of visual ones for an image classification task. Our method achieves state-of-the-art classification results on the CIFAR-100 image data set and the MIR Flickr image-text data set.",
        "date": "January 2013",
        "authors": [
            "N. Srivastava",
            "R. Salakhutdinov"
        ],
        "references": [
            45489469,
            221345667,
            2123184
        ]
    },
    {
        "id": 278695382,
        "title": "The Nature of Statistical Learning Theory",
        "abstract": "In this chapter we consider bounds on the rate of uniform convergence. We consider upper bounds (there exist lower bounds as well (Vapnik and Chervonenkis, 1974); however, they are not as important for controlling the learning processes as the upper bounds).",
        "date": "January 2000",
        "authors": [
            "Vladimir N. Vapnik"
        ],
        "references": []
    },
    {
        "id": 278651717,
        "title": "The Nature of Statistical Learning Theory",
        "abstract": "In the history of research of the learning problem one can extract four periods that can be characterized by four bright events: (i) Constructing the first learning machines, (ii) constructing the fundamentals of the theory, (iii) constructing neural networks, (iv) constructing the alternatives to neural networks.",
        "date": "January 2000",
        "authors": [
            "Vladimir N. Vapnik"
        ],
        "references": []
    },
    {
        "id": 347495711,
        "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "Li Fei-Fei",
            "R. Fergus",
            "P. Perona"
        ],
        "references": []
    },
    {
        "id": 319770439,
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
        "date": "January 2013",
        "authors": [
            "Tomas Mikolov",
            "G.s. Corrado",
            "Kai Chen",
            "Jeffrey Dean"
        ],
        "references": []
    },
    {
        "id": 319770430,
        "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "abstract": "Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40% (achieving a final mAP of 48% on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.",
        "date": "November 2014",
        "authors": [
            "Ross Girshick",
            "Jeff Donahue",
            "Trevor Darrell",
            "Jitendra Malik"
        ],
        "references": []
    },
    {
        "id": 319770370,
        "title": "Three things everyone should know to improve object retrieval",
        "abstract": "The objective of this work is object retrieval in large scale image datasets, where the object is speci\ufb01ed by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through ef\ufb01cient use of the inverted index; (iii) an improvement of the image augmentation method proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of standard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate retrieval speeds. Combining these complementary methods achieves a new state-of-the-art performance on these datasets.",
        "date": "January 2012",
        "authors": [
            "Relja Arandjelovic",
            "Andrew Zisserman"
        ],
        "references": []
    },
    {
        "id": 319770363,
        "title": "Deep Fisher Networks for Large-Scale Image Classification",
        "abstract": "As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore the extent of this analogy, proposing a version of the stateof- the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture significantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a smaller computational learning cost. Our hybrid architecture allows us to assess how the performance of a conventional hand-crafted image classification pipeline changes with increased depth. We also show that convolutional networks and Fisher vector encodings are complementary in the sense that their combination further improves the accuracy.",
        "date": "January 2013",
        "authors": [
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "references": []
    },
    {
        "id": 319770264,
        "title": "Regularization of Neural Networks using DropConnect",
        "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.",
        "date": "January 2013",
        "authors": [
            "Li Wan",
            "Matthew D Zeiler",
            "Sixn Zhang",
            "Yann Lecun"
        ],
        "references": []
    },
    {
        "id": 316240097,
        "title": "Distinctive Image Features from Scale-Invariant Keypoints",
        "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
        "date": "November 2004",
        "authors": [
            "David G. Lowe"
        ],
        "references": [
            244441057,
            225207866,
            225133940,
            224377792,
            224072186,
            222501480,
            200038914,
            3854190,
            2841703,
            2816739
        ]
    },
    {
        "id": 314450504,
        "title": "Object detection using a max-margin Hough transform",
        "abstract": "",
        "date": "June 2009",
        "authors": [
            "S. Maji",
            "J. Malik"
        ],
        "references": [
            227191648,
            224323280,
            220660094,
            6731822,
            2464475,
            319770820,
            313562430,
            312915478,
            312538118,
            308161149
        ]
    },
    {
        "id": 301463801,
        "title": "Open-vocabulary Object Retrieval",
        "abstract": "",
        "date": "July 2014",
        "authors": [
            "Sergio Guadarrama",
            "Erik Rodner",
            "Ryan Farrell",
            "Ning Zhang"
        ],
        "references": [
            312995014,
            329974353,
            319770183,
            304417828,
            291286882,
            279843694,
            270878885,
            267960550,
            267687028,
            262410602
        ]
    },
    {
        "id": 262270555,
        "title": "Selective Search for Object Recognition",
        "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html).",
        "date": "September 2013",
        "authors": [
            "Jasper R. R. Uijlings",
            "K. E. A. Sande",
            "T. Gevers",
            "A.W.M. Smeulders"
        ],
        "references": [
            228602850,
            319770820,
            319770432,
            314450504,
            284040965,
            281327886,
            246418967,
            240224936,
            232626819,
            232617564
        ]
    },
    {
        "id": 258520603,
        "title": "Recognizing Image Style",
        "abstract": "The style of an image plays a significant role in how it is viewed, but has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best -- even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 55K Flickr photographs annotated with curated style labels as well as free-form tags, and 85K paintings annotated with style and genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.",
        "date": "November 2013",
        "authors": [
            "Sergey Karayev",
            "Aaron Hertzmann",
            "Holger Winnemoeller",
            "Aseem Agarwala"
        ],
        "references": [
            302747723,
            319770372,
            319770357,
            269128228,
            262390837,
            262314937,
            262286466,
            261336804,
            261168672,
            257409887
        ]
    },
    {
        "id": 319770411,
        "title": "Torch7: A Matlab-like Environment for Machine Learning",
        "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be in- terfaced to third-party software thanks to Lua's light interface.",
        "date": "January 2011",
        "authors": [
            "Ronan Collobert",
            "Koray Kavukcuoglu",
            "Clement Farabet"
        ],
        "references": []
    },
    {
        "id": 272149731,
        "title": "Pylearn2: a machine learning research library",
        "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.",
        "date": "January 2013",
        "authors": [
            "IJ Goodfellow",
            "D Warde-Farley"
        ],
        "references": []
    },
    {
        "id": 264890087,
        "title": "Torch7: A Matlab-like Environment for Machine Learning",
        "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be in-terfaced to third-party software thanks to Lua's light interface.",
        "date": "January 2011",
        "authors": [
            "Ronan Collobert",
            "Koray Kavukcuoglu",
            "Clement Farabet"
        ],
        "references": [
            216792890,
            216792770
        ]
    },
    {
        "id": 258839715,
        "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling",
        "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets and DPM have been shown to perform well for this problem but they are limited by flat low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.",
        "date": "November 2013",
        "authors": [
            "Ning Zhang",
            "Manohar Paluri",
            "Marc'Aurelio Ranzato",
            "Trevor Darrell"
        ],
        "references": [
            261121695,
            257052073,
            238592859,
            227943302,
            224716259,
            224135964,
            221111309,
            221111179,
            51968606,
            2985446
        ]
    },
    {
        "id": 259400019,
        "title": "On the number of response regions of deep feed forward networks with piece-wise linear activations",
        "abstract": "This paper explores the complexity of deep feedforward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has $kn$ hidden units and $n_0$ inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$ layer model with $n$ hidden units on each layer it is $\\Omega(\\left\\lfloor {n}/ {n_0}\\right\\rfloor^{k-1}n^{n_0})$. The number $\\left\\lfloor{n}/{n_0}\\right\\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$ tends to infinity or when $k$ tends to infinity and $n \\geq 2n_0$. Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis.",
        "date": "December 2013",
        "authors": [
            "Razvan Pascanu",
            "Guido Montufar",
            "Y. Bengio"
        ],
        "references": [
            263091596,
            228102719,
            225098395,
            319770386,
            316519032,
            310752533,
            267064227,
            265178583,
            258424423,
            221664789
        ]
    },
    {
        "id": 258816388,
        "title": "Revisiting Natural Gradient for Deep Networks",
        "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.",
        "date": "January 2013",
        "authors": [
            "Razvan Pascanu",
            "Y. Bengio"
        ],
        "references": [
            266463521,
            235557274,
            234131091,
            233753224,
            222348233,
            285906939,
            235409844,
            234131187,
            224926910,
            224513847
        ]
    },
    {
        "id": 316519032,
        "title": "An introduction to hyperplane arrangements",
        "abstract": "",
        "date": "October 2007",
        "authors": [
            "Richard Stanley"
        ],
        "references": []
    },
    {
        "id": 284500238,
        "title": "Approximation by superposition of sigmoidale function",
        "abstract": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
        "date": "January 1989",
        "authors": [
            "George Cybenko"
        ],
        "references": [
            266002435,
            226439292
        ]
    },
    {
        "id": 267064227,
        "title": "Facing up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes",
        "abstract": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Mathematics, 1974. Includes bibliographical references (leaves 135-139). Vita.",
        "date": "January 1975",
        "authors": [
            "T. Zaslavsky"
        ],
        "references": []
    },
    {
        "id": 265178583,
        "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
        "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.",
        "date": "November 2012",
        "authors": [
            "Geoffrey Hinton",
            "li Deng",
            "Dong Yu",
            "George Dahl"
        ],
        "references": [
            261119155,
            261090788,
            260693356,
            239765773,
            228446017,
            224737607,
            224327435,
            224216007,
            221620570,
            221619818
        ]
    },
    {
        "id": 257672343,
        "title": "Object Recognition by Sequential Figure-Ground Ranking",
        "abstract": "We present an approach to visual object-class segmentation and recognition based on a pipeline that combines multiple figure-ground hypotheses with large object spatial support, generated by bottom-up computational processes that do not exploit knowledge of specific categories, and sequential categorization based on continuous estimates of the spatial overlap between the image segment hypotheses and each putative class. We differ from existing approaches not only in our seemingly unreasonable assumption that good object-level segments can be obtained in a feed-forward fashion, but also in formulating recognition as a regression problem. Instead of focusing on a one-vs.-all winning margin that may not preserve the ordering of segment qualities inside the non-maximum (non-winning) set, our learning method produces a globally consistent ranking with close ties to segment quality, hence to the extent entire object or part hypotheses are likely to spatially overlap the ground truth. We demonstrate results beyond the current state of the art for image classification, object detection and semantic segmentation, in a number of challenging datasets including Caltech-101, ETHZ-Shape as well as PASCAL VOC 2009 and 2010.",
        "date": "July 2012",
        "authors": [
            "J. Carreira",
            "Fuxin Li",
            "Cristian Sminchisescu"
        ],
        "references": [
            319770432,
            319770425,
            319770260,
            319770213,
            316240097,
            314405838,
            313530549,
            309532479,
            285058550,
            279178907
        ]
    },
    {
        "id": 240308781,
        "title": "Learning Hierarchical Features for Scene Labeling",
        "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a $(320\\times 240)$ image labeling in less than a second, including feature extraction.",
        "date": "August 2013",
        "authors": [
            "Clement Farabet",
            "Camille Couprie",
            "Laurent Najman",
            "Yann Lecun"
        ],
        "references": [
            241191907,
            230690782,
            225496431,
            224716259,
            305083260,
            266439873,
            252482398,
            243773059,
            239344658,
            232649995
        ]
    },
    {
        "id": 235360690,
        "title": "Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks",
        "abstract": "Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present.",
        "date": "February 2013",
        "authors": [
            "Alessandro Giusti",
            "Dan C. Cire\u015fan",
            "Jonathan Masci",
            "Luca Maria Gambardella"
        ],
        "references": [
            266225209,
            224260296,
            224203140,
            319770626,
            319770183,
            267960550,
            267709055,
            265748773,
            243659305,
            221663833
        ]
    },
    {
        "id": 228102719,
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
        "date": "July 2012",
        "authors": [
            "Geoffrey E. Hinton",
            "Nitish Srivastava",
            "Alex Krizhevsky",
            "Ilya Sutskever"
        ],
        "references": [
            306218037,
            266248467,
            265350973,
            233822447,
            226270700,
            11207765
        ]
    },
    {
        "id": 262323276,
        "title": "Prime Object Proposals with Randomized Prim's Algorithm",
        "abstract": "Generic object detection is the challenging task of proposing windows that localize all the objects in an image, regardless of their classes. Such detectors have recently been shown to benefit many applications such as speeding-up class-specific object detection, weakly supervised learning of object detectors and object discovery. In this paper, we introduce a novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm. Using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object, the algorithm generates random partial spanning trees with large expected sum of edge weights. Object localizations are proposed as bounding-boxes of those partial trees. Our method has several benefits compared to the state-of-the-art. Thanks to the efficiency of Prim's algorithm, it samples proposals very quickly: 1000 proposals are obtained in about 0.7s. With proposals bound to super pixel boundaries yet diversified by randomization, it yields very high detection rates and windows that tightly fit objects. In extensive experiments on the challenging PASCAL VOC 2007 and 2012 and SUN2012 benchmark datasets, we show that our method improves over state-of-the-art competitors for a wide range of evaluation scenarios.",
        "date": "December 2013",
        "authors": [
            "Santiago Manen",
            "Matthieu Guillaumin",
            "Luc Van Gool"
        ],
        "references": [
            262203992,
            221366761,
            221305138,
            221110937,
            221110458,
            221110086,
            220320833,
            51855547,
            51539284,
            50999764
        ]
    },
    {
        "id": 233815499,
        "title": "Pedestrian Detection with Unsupervised Multi-Stage Feature Learning",
        "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",
        "date": "December 2012",
        "authors": [
            "Pierre Sermanet",
            "Koray Kavukcuoglu",
            "Soumith Chintala",
            "Yann Lecun"
        ],
        "references": [
            303137904,
            261306330,
            246596930,
            243116386,
            224579211,
            224260345,
            224164350,
            221620168,
            221618859,
            221363014
        ]
    },
    {
        "id": 234131129,
        "title": "Big Neural Networks Waste Capacity",
        "abstract": "This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact there are highly diminishing returns for capacity in terms of training error, leading to underfitting. This suggests that the optimization method - first order gradient descent - fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.",
        "date": "January 2013",
        "authors": [
            "Yann N. Dauphin",
            "Y. Bengio"
        ],
        "references": [
            239765773,
            228102719,
            221361415,
            221345198,
            319770395,
            267960550,
            221619092,
            221487297,
            221480736,
            221345102
        ]
    },
    {
        "id": 233730646,
        "title": "On the difficulty of training Recurrent Neural Networks",
        "abstract": "There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.",
        "date": "November 2012",
        "authors": [
            "Razvan Pascanu",
            "Tomas Mikolov",
            "Y. Bengio"
        ],
        "references": [
            266485700,
            233836017,
            233753224,
            228095594,
            224663746,
            223074905,
            266458936,
            243773399,
            229091480,
            222526235
        ]
    },
    {
        "id": 270878508,
        "title": "Parsing with Compositional Vector Grammars",
        "abstract": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.",
        "date": "August 2013",
        "authors": [
            "Richard Socher",
            "John Bauer",
            "Christopher D. Manning",
            "Ng Andrew Y."
        ],
        "references": [
            220875081,
            28763418,
            222681214
        ]
    },
    {
        "id": 231556969,
        "title": "Deep Learning Made Easier by Linear Transformations in",
        "abstract": "We transform the outputs of each hidden neuron in a multi-layer perceptron network to be zero mean and zero slope, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of handwritten digits with a 3-layer network and learning a low-dimensional representation for images by using a 6-layer auto-encoder network. The transformations were beneficial in all cases, with and without regularization. 1",
        "date": "May 2012",
        "authors": [
            "Tapani Raiko",
            "Harri Valpola",
            "Yann Lecun"
        ],
        "references": [
            221346269,
            215616968,
            51888056,
            319770436,
            306221946,
            304533936,
            289786324,
            265748773,
            221619092,
            221345102
        ]
    },
    {
        "id": 215616968,
        "title": "Understanding the difficulty of training deep feedforward neural networks",
        "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.",
        "date": "January 2010",
        "authors": [
            "Xavier Glorot",
            "Y. Bengio"
        ],
        "references": [
            303137904,
            239033363,
            221346269,
            306218037,
            245993331,
            235130690,
            229091480,
            224309464,
            221618232,
            221346162
        ]
    },
    {
        "id": 289786324,
        "title": "Centering Neural Network Gradient Factors",
        "abstract": "It has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero; recently we have extended this approach to also encompass the centering of error signals [15]. Here we generalize this notion to all factors involved in the network's gradient, leading us to propose centering the slope of hidden unit activation functions as well. Slope centering removes the linear component of backpropagated error; this improves credit assignment in networks with shortcut connections. Benchmark results show that this can speed up learning significantly without adversely affecting the trained network's generalization ability.",
        "date": "January 1998",
        "authors": [
            "Nicol N. Schraudolph"
        ],
        "references": [
            237127809,
            221112207,
            216792866,
            34749594,
            22228479,
            2690080,
            2498372,
            324110517,
            282260583,
            271236629
        ]
    },
    {
        "id": 235222671,
        "title": "Science and Spectacle: the Work of Jodrell Bank in Post-War British Culture",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "Jon Agar"
        ],
        "references": []
    },
    {
        "id": 225495886,
        "title": "Efficient BackProp",
        "abstract": "The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most \u201cclassical\u201d second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
        "date": "January 1998",
        "authors": [
            "Yann Lecun",
            "Leon Bottou",
            "Genevieve B. Orr",
            "Klaus-Robert M\u00fcller"
        ],
        "references": [
            222482794,
            221618539,
            220499843,
            216792889,
            216792843,
            15149034,
            13383386,
            13230969,
            3175480,
            2822332
        ]
    },
    {
        "id": 221619092,
        "title": "Topmoumoute Online Natural Gradient Algorithm",
        "abstract": "Guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.",
        "date": "January 2007",
        "authors": [
            "Nicolas Le Roux",
            "Pierre-Antoine Manzagol",
            "Y. Bengio"
        ],
        "references": [
            221345414,
            11294744,
            2577260,
            37433283,
            13481213,
            12384402,
            2453999,
            2433873
        ]
    },
    {
        "id": 221345102,
        "title": "Deep learning via Hessian-free optimization",
        "abstract": "We develop a 2 nd -order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto- encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep- learning and how 2 nd -order optimization, and our method in particular, effectively deals with it.",
        "date": "August 2010",
        "authors": [
            "James Martens"
        ],
        "references": [
            220319875,
            200744514,
            11294744,
            2822332,
            12384402,
            6912170,
            5580442
        ]
    },
    {
        "id": 46392541,
        "title": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",
        "abstract": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.",
        "date": "December 2010",
        "authors": [
            "Dan Claudiu Cire\u015fan",
            "Ueli Meier",
            "Luca Maria Gambardella",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            303137904,
            279233597,
            243781690,
            228871422,
            228344387,
            224716259,
            221080133,
            220860992,
            216792739,
            200744514
        ]
    },
    {
        "id": 324435700,
        "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
        "abstract": "This work proposes an automated algorithm, called NetAdapt, that adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or the number of parameters, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget (e.g., latency) is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.66$\\times$ speedup in measured inference latency with higher accuracy.",
        "date": "April 2018",
        "authors": [
            "Tien-Ju Yang",
            "Andrew Howard",
            "Bo Chen",
            "Xiao Zhang"
        ],
        "references": [
            315667264,
            349947240,
            345080016,
            329743813,
            323141651,
            322518045,
            321901991,
            320968331,
            319770346,
            311609041
        ]
    },
    {
        "id": 322059721,
        "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Yihui He",
            "Xiangyu Zhang",
            "Jian Sun"
        ],
        "references": [
            322649540,
            320968382,
            320968331,
            320964885,
            319770367,
            319770349,
            319770342,
            319770334,
            319770323,
            319770291
        ]
    },
    {
        "id": 322058668,
        "title": "Domain-Adaptive Deep Network Compression",
        "abstract": "Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called fine-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer. We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method significantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone \u2013 with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.",
        "date": "October 2017",
        "authors": [
            "Marc Masana",
            "Joost van de Weijer",
            "Luis Herranz",
            "Andrew D. Bagdanov"
        ],
        "references": [
            310610589,
            319770342,
            319770334,
            319770230,
            319770191,
            319769911,
            319769909,
            313879156,
            313601183,
            312222399
        ]
    },
    {
        "id": 320971183,
        "title": "More is Less: A More Complicated Network with Less Inference Complexity",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Xuanyi Dong",
            "Junshi Huang",
            "Yi Yang",
            "Shuicheng Yan"
        ],
        "references": [
            319770414,
            319770334,
            319770291,
            319770230,
            319770191,
            319770183,
            319769909,
            319769906,
            311610246,
            311609205
        ]
    },
    {
        "id": 329745708,
        "title": "Learning Transferable Architectures for Scalable Image Recognition",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Barret Zoph",
            "Vijay Vasudevan",
            "Jonathon Shlens",
            "Quoc V. Le"
        ],
        "references": [
            319770123,
            316598820,
            316184205,
            313857509,
            311769895,
            311223153,
            310462326,
            309738510,
            306885833,
            306187421
        ]
    },
    {
        "id": 328112952,
        "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part X",
        "abstract": "This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7\\(\\times \\) speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2).",
        "date": "September 2018",
        "authors": [
            "Tien-Ju Yang",
            "Andrew Howard",
            "Bo Chen",
            "Xiao Zhang"
        ],
        "references": [
            319858391,
            315667264,
            310440966,
            280329902,
            269932912,
            221361415,
            2441749,
            329743813,
            329740172,
            329740169
        ]
    },
    {
        "id": 322517761,
        "title": "Inverted Residuals and Linear Bottlenecks: Mobile Networks forClassification, Detection and Segmentation",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters",
        "date": "January 2018",
        "authors": [
            "Mark Sandler",
            "Andrew Howard",
            "Menglong Zhu",
            "Andrey Zhmoginov"
        ],
        "references": [
            322539221,
            320075558,
            317300069,
            316184205,
            308278279,
            307536925,
            305196650,
            303409435,
            301839500,
            278413297
        ]
    },
    {
        "id": 322058064,
        "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Jian-Hao Luo",
            "Jianxin Wu",
            "Weiyao Lin"
        ],
        "references": [
            307536925,
            305196650,
            287853408,
            281895813,
            275279789,
            265295439,
            261368736,
            240308775,
            228102719,
            224342191
        ]
    },
    {
        "id": 320963610,
        "title": "LCNN: Lookup-Based Convolutional Neural Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Hessam Bagherinezhad",
            "Mohammad Rastegari",
            "Ali Farhadi"
        ],
        "references": [
            308457764,
            305881526,
            305196650,
            301878495,
            301848151,
            287853408,
            282844341,
            280329902,
            275279789,
            263352539
        ]
    },
    {
        "id": 308896188,
        "title": "Error bounds for approximations with deep ReLU networks",
        "abstract": "We study how approximation errors of neural networks with ReLU activation functions depend on the depth of the network. We establish rigorous error bounds showing that deep ReLU networks are significantly more expressive than shallow ones as long as approximations of smooth functions are concerned. At the same time, we show that on a set of functions constrained only by their degree of smoothness, a ReLU network architecture cannot in general achieve approximation accuracy with better than a power law dependence on the network size, regardless of its depth.",
        "date": "October 2016",
        "authors": [
            "Dmitry Yarotsky"
        ],
        "references": [
            322539221,
            301841654,
            281895600,
            277411157,
            319770106,
            316349627,
            306186507,
            304018151,
            282403870,
            263894641
        ]
    },
    {
        "id": 281895600,
        "title": "On the Expressive Power of Deep Learning: A Tensor Analysis",
        "abstract": "It has long been conjectured that hypothesis spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical architectures than with shallow ones. Despite the vast empirical evidence, formal arguments to date are limited and do not capture the kind of networks used in practice. Using tensor factorization, we derive a universal hypothesis space implemented by an arithmetic circuit over functions applied to local data structures (e.g. image patches). The resulting networks first pass the input through a representation layer, and then proceed with a sequence of layers comprising sum followed by product-pooling, where sum corresponds to the widely used convolution operator. The hierarchical structure of networks is born from factorizations of tensors based on the linear weights of the arithmetic circuits. We show that a shallow network corresponds to a rank-1 decomposition, whereas a deep network corresponds to a Hierarchical Tucker (HT) decomposition. Log-space computation for numerical stability transforms the networks into SimNets. In its basic form, our main theoretical result shows that the set of polynomially sized rank-1 decomposable tensors has measure zero in the parameter space of polynomially sized HT decomposable tensors. In deep learning terminology, this amounts to saying that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require an exponential size if one wishes to implement (or approximate) them with a shallow network. Our construction and theory shed new light on various practices and ideas employed by the deep learning community, and in that sense bear a paradigmatic contribution as well.",
        "date": "June 2016",
        "authors": [
            "Nadav Cohen",
            "Or Sharir",
            "Amnon Shashua"
        ],
        "references": [
            322539221,
            305196650,
            332138022,
            329651461,
            319770386,
            319770263,
            310752533,
            291295837,
            289810433,
            288359152
        ]
    },
    {
        "id": 319770106,
        "title": "On the Expressive Power of Deep Learning: A Tensor Analysis",
        "abstract": "It has long been conjectured that hypothesis spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical architectures than with shallow ones. Despite the vast empirical evidence, formal arguments to date are limited and do not capture the kind of networks used in practice. Using tensor factorization, we derive a universal hypothesis space implemented by an arithmetic circuit over functions applied to local data structures (e.g. image patches). The resulting networks first pass the input through a representation layer, and then proceed with a sequence of layers comprising sum followed by product-pooling, where sum corresponds to the widely used convolution operator. The hierarchical structure of networks is born from factorizations of tensors based on the linear weights of the arithmetic circuits. We show that a shallow network corresponds to a rank-1 decomposition, whereas a deep network corresponds to a Hierarchical Tucker (HT) decomposition. Log-space computation for numerical stability transforms the networks into SimNets. In its basic form, our main theoretical result shows that the set of polynomially sized rank-1 decomposable tensors has measure zero in the parameter space of polynomially sized HT decomposable tensors. In deep learning terminology, this amounts to saying that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require an exponential size if one wishes to implement (or approximate) them with a shallow network. Our construction and theory shed new light on various practices and ideas employed by the deep learning community, and in that sense bear a paradigmatic contribution as well.",
        "date": "January 2015",
        "authors": [
            "Nadav Cohen",
            "Or Sharir",
            "Amnon Shashua"
        ],
        "references": []
    },
    {
        "id": 314361329,
        "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks",
        "abstract": "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$ and $\\Omega( W L \\log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\\Theta(W U)$ on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.",
        "date": "March 2017",
        "authors": [
            "Nick Harvey",
            "Chris Liaw",
            "Abbas Mehrabian"
        ],
        "references": [
            309131837,
            308896188,
            281895600,
            277411157,
            2809085,
            319770106,
            309572399,
            301857086,
            287250773,
            246732421
        ]
    },
    {
        "id": 311609041,
        "title": "Deep Residual Learning for Image Recognition",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "references": [
            305196650,
            269935397,
            265908778,
            265787949,
            265295439,
            264979485,
            264929445,
            260126867,
            259441043,
            259440750
        ]
    },
    {
        "id": 301857086,
        "title": "Benefits of depth in neural networks",
        "abstract": "For any positive integer $k$, there exist neural networks with $\\Theta(k^3)$ layers, $\\Theta(1)$ nodes per layer, and $\\Theta(1)$ distinct parameters which can not be approximated by networks with $\\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed \"semi-algebraic gates\" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, and boosted decision trees (in this last case with a stronger separation: $\\Omega(2^{k^3})$ total tree nodes are required).",
        "date": "February 2016",
        "authors": [
            "Matus Telgarsky"
        ],
        "references": [
            260126867,
            236736771,
            215990477,
            2985446,
            319770183,
            303901717,
            290161073,
            287250773,
            284788019,
            275055381
        ]
    },
    {
        "id": 287250773,
        "title": "The Power of Depth for Feedforward Neural Networks",
        "abstract": "We show that there are simple functions on $\\mathbb{R}^d$, expressible by small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for most continuous activation functions, including rectified linear units and sigmoids, and is a formal demonstration that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.",
        "date": "December 2015",
        "authors": [
            "Ronen Eldan",
            "Ohad Shamir"
        ],
        "references": [
            281895600,
            260126867,
            259400019,
            236736771,
            225866987,
            221497561,
            220573544,
            220365641,
            51967354,
            2716732
        ]
    },
    {
        "id": 322539221,
        "title": "Notes on the number of linear regions of deep neural networks",
        "abstract": "We follow up on previous work addressing the number of response regions of the functions representable by feedforward neural networks with piecewise linear activation functions. We discuss upper bounds on the maximum number of linear regions for deep networks with rectified linear units. We elaborate on the identification of input regions as an analysis tool, and how it implies exponential gaps between shallow and deep networks, not only in terms of the maximum number of linear regions but also other additive properties over the input domain and more general types of activation functions.",
        "date": "March 2017",
        "authors": [
            "Guido Montufar"
        ],
        "references": [
            260126867,
            220695511
        ]
    },
    {
        "id": 309131743,
        "title": "Tensorial Mixture Models",
        "abstract": "Casting neural networks in generative frameworks is a highly sought-after endeavor these days. Contemporary methods, such as Generative Adversarial Networks, capture some of the generative capabilities, but not all. In particular, they lack the ability of tractable marginalization, and thus are not suitable for many tasks. Other methods, based on arithmetic circuits and sum-product networks, do allow tractable marginalization, but their performance is challenged by the need to learn the structure of a circuit. Building on the tractability of arithmetic circuits, we leverage concepts from tensor analysis, and derive a family of generative models we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolutional network structure, and in addition, lend themselves to theoretical analyses that allow comprehensive understanding of the relation between their structure and their expressive properties. We thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective representation of rich distributions in an easily controlled manner. These two capabilities are brought together in the task of classification under missing data, where TMMs deliver state of the art accuracies with seamless implementation and design.",
        "date": "October 2016",
        "authors": [
            "Or Sharir",
            "Ronen Tamari",
            "Nadav Cohen",
            "Amnon Shashua"
        ],
        "references": [
            315457753,
            305881127,
            319770395,
            319770386,
            319770355,
            319770229,
            319770144,
            309438886,
            303993361,
            303921589
        ]
    },
    {
        "id": 308026508,
        "title": "WaveNet: A Generative Model for Raw Audio",
        "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",
        "date": "September 2016",
        "authors": [
            "Aaron van den oord",
            "Sander Dieleman",
            "Heiga Zen",
            "Karen Simonyan"
        ],
        "references": [
            304671740,
            304372544,
            304335754,
            308843134,
            308805598,
            304486081,
            304372081,
            304018350,
            301874314,
            301847993
        ]
    },
    {
        "id": 304018355,
        "title": "Exponential expressivity in deep neural networks through transient chaos",
        "abstract": "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.",
        "date": "December 2016",
        "authors": [
            "Ben Poole",
            "Subhaneil Lahiri",
            "Maithra Raghu",
            "Jascha Sohl-Dickstein"
        ],
        "references": [
            301841654,
            278969484,
            260126867,
            287469471,
            287250773,
            282403870,
            269722411,
            267960550,
            263894641,
            259367763
        ]
    },
    {
        "id": 303448829,
        "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
        "abstract": "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth - they are able to efficiently model strong correlation under favored partitions of the input.",
        "date": "May 2016",
        "authors": [
            "Nadav Cohen",
            "Amnon Shashua"
        ],
        "references": [
            322539221,
            305196650,
            301841654,
            281895600,
            281285245,
            319769990,
            311609041,
            287250773,
            286512696,
            282403870
        ]
    },
    {
        "id": 319770395,
        "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
        "abstract": "A great deal of research has focused on algorithms for learning features from un- labeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning al- gorithms and deep models. In this paper, however, we show that several very sim- ple factors, such as the number of hidden nodes in the model, may be as important to achieving high performance as the choice of learning algorithm or the depth of the model. Specifically, we will apply several off-the-shelf feature learning al- gorithms (sparse auto-encoders, sparse RBMs and K-means clustering, Gaussian mixtures) to NORB and CIFAR datasets using only single-layer networks. We then present a detailed analysis of the effect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (stride) be- tween extracted features, and the effect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are as critical to achieving high performance as the choice of algorithm itselfso critical, in fact, that when these parameters are pushed to their limits, we are able to achieve state-of-the- art performance on both CIFAR and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyper-parameters to tune beyond the model structure it- self, and is very easy implement. Despite the simplicity of our system, we achieve performance beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.0% accuracy respectively).",
        "date": "January 2011",
        "authors": [
            "Adam Coates",
            "Honglak Lee",
            "Andrew Y Ng"
        ],
        "references": []
    },
    {
        "id": 319770007,
        "title": "Benefits of depth in neural networks",
        "abstract": "For any positive integer $k$, there exist neural networks with $backslashTheta(k^3)$ layers, $backslashTheta(1)$ nodes per layer, and $backslashTheta(1)$ distinct parameters which can not be approximated by networks with $backslashmathcalO(k)$ layers unless they are exponentially large --- they must possess $backslashOmega(2^k)$ nodes. This result is proved here for a class of nodes termed \"semi-algebraic gates\" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $backslashOmega(2^{k^}3)$ total tree nodes are required).",
        "date": "February 2016",
        "authors": [
            "Matus Telgarsky"
        ],
        "references": []
    },
    {
        "id": 312461699,
        "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks",
        "abstract": "We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.",
        "date": "January 2017",
        "authors": [
            "Wenjie Luo",
            "Yujia Li",
            "Raquel Urtasun",
            "Richard Zemel"
        ],
        "references": []
    },
    {
        "id": 309572399,
        "title": "Depth Separation in ReLU Networks for Approximating Smooth Non-Linear Functions",
        "abstract": "We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on $[0,1]^d$, which can be approximated to accuracy $\\epsilon$ by ReLU networks of depth and width $\\mathcal{O}(\\text{poly}(\\log(1/\\epsilon)))$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least $\\Omega(1/\\epsilon)$.",
        "date": "October 2016",
        "authors": [
            "Itay Safran",
            "Ohad Shamir"
        ],
        "references": [
            281895600,
            319770007,
            287250773,
            286512696,
            226439292
        ]
    },
    {
        "id": 307627766,
        "title": "From free energy to expected energy: Improving energy-based value function approximation in reinforcement learning",
        "abstract": "Free-energy based reinforcement learning (FERL) was proposed for learning in high-dimensional state- and action spaces. However, the FERL method does only really work well with binary, or close to binary, state input, where the number of active states are fewer than the number of non-active states. In the FERL method the value function is approximated by the negative free energy of a restricted Boltzmann machine (RBM). In our earlier study, we demonstrated that the performance and the robustness of the FERL method can be improved by scaling the free energy by a constant that is related to the size of network. In this study, we propose that RBM function approximation can be further improved by approximating the value function by the negative expected energy (EERL), instead of the negative free energy, as well as being able to handle continuous state input. We validate our proposed method by demonstrating that EERL: 1) outperforms FERL, as well as standard neural network and linear function approximation, for three versions of a gridworld task with high-dimensional image state input; 2) achieves new state-of-the-art results in stochastic SZ-Tetris in both model-free and model-based learning settings; and 3) significantly outperforms FERL and standard neural network function approximation for a robot navigation task with raw and noisy RGB images as state input and a large number of actions.",
        "date": "August 2016",
        "authors": [
            "Stefan Elfwing",
            "Eiji Uchibe",
            "Kenji Doya"
        ],
        "references": [
            266205382,
            265588028,
            239571798,
            235756951,
            304109482,
            284106719,
            272161307,
            265748773,
            247757128,
            247709546
        ]
    },
    {
        "id": 292074166,
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",
        "date": "January 2016",
        "authors": [
            "David Silver",
            "Aja Huang",
            "Christopher Maddison",
            "Arthur Guez"
        ],
        "references": [
            277411157,
            269935589,
            319770183,
            312990324,
            312986214,
            300449670,
            286329574,
            279964320,
            272837232,
            269417695
        ]
    },
    {
        "id": 282182152,
        "title": "Deep Reinforcement Learning with Double Q-learning",
        "abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.",
        "date": "September 2015",
        "authors": [
            "Hado Van Hasselt",
            "Arthur Guez",
            "David Silver"
        ],
        "references": [
            280104499,
            273704409,
            229328831,
            221619239,
            322808208,
            272837232,
            268352406,
            263757661,
            235709806,
            222440273
        ]
    },
    {
        "id": 280898866,
        "title": "Approximate Modified Policy Iteration and its Application to the Game of Tetris",
        "abstract": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.1 \u00a9 2015 Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, Matthieu Geist.",
        "date": "August 2015",
        "authors": [
            "Bruno Scherrer",
            "Mohammad Ghavamzadeh",
            "Victor Gabillon",
            "Boris Lesner"
        ],
        "references": [
            280700426,
            313991982,
            313505421,
            311672961,
            307881957,
            290113970,
            286474367,
            272623654,
            266952772,
            263242310
        ]
    },
    {
        "id": 280104499,
        "title": "Massively Parallel Methods for Deep Reinforcement Learning",
        "abstract": "We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.",
        "date": "July 2015",
        "authors": [
            "Arun Nair",
            "Praveen Srinivasan",
            "Sam Blackwell",
            "Cagdas Alcicek"
        ],
        "references": [
            265787949,
            229328831,
            319770291,
            319770183,
            272837232,
            267960550,
            265385906,
            262355828,
            259367763,
            258818168
        ]
    },
    {
        "id": 280043948,
        "title": "High-Dimensional Function Approximation for Knowledge-Free Reinforcement Learning: a Case Study in SZ-Tetris",
        "abstract": "SZ-Tetris, a restricted version of Tetris, is a dicult reinforcement learning task. Previous research showed that, similarly to the original Tetris, value function-based methods such as temporal dierence learning, do not work well for SZ-Tetris. The best performance in this game was achieved by employing direct policy search techniques, in particular the cross-entropy method in combination with handcrafted features. Nonetheless, a simple heuristic hand-coded player scores even higher. Here we show that it is possible to equal its performance with CMA-ES (Covariance Matrix Adaptation Evolution Strategy). We demonstrate that further improvement is possible by employing systematic n-tuple network, a knowledge-free function approximator, and VD-CMA-ES, a linear variant of CMA-ES for high dimension optimization. Last but not least, we show that a large systematic n-tuple network (involving more than 4 million parameters) allows the classical temporal dierence learning algorithm to obtain similar average performance to VD-CMA-ES, but at 20 times lower computational expense, leading to the best policy for SZ-Tetris known to date. These results enrich the current understanding of diculty of SZ-Tetris, and shed new light on the capabilities of particular search paradigms when applied to representations of various characteristics and dimensionality.",
        "date": "July 2015",
        "authors": [
            "Wojciech Ja\u015bkowski",
            "Marcin Szubert",
            "Pawe\u0142 Liskowski",
            "Krzysztof Krawiec"
        ],
        "references": [
            269333585,
            266656106,
            265612548,
            265588028,
            263198856,
            322808208,
            313505421,
            290106209,
            263757661,
            247709546
        ]
    },
    {
        "id": 319770330,
        "title": "Prioritized Experience Replay",
        "abstract": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",
        "date": "November 2016",
        "authors": [
            "Tom Schaul",
            "John Quan",
            "Ioannis Antonoglou",
            "David Silver"
        ],
        "references": []
    },
    {
        "id": 312990324,
        "title": "Td-gammon, a self-Teaching backgammon program, achieves master-level play",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "G. Tesauro"
        ],
        "references": []
    },
    {
        "id": 306218037,
        "title": "Learning multiple layers of features from tiny images",
        "abstract": "",
        "date": "January 2009",
        "authors": [
            "A. Krizhevsky",
            "G. Hinton"
        ],
        "references": []
    },
    {
        "id": 304109482,
        "title": "Training products of experts by minimizing contrastive divergence",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "G. Hinton"
        ],
        "references": []
    },
    {
        "id": 320965021,
        "title": "Lip Reading Sentences in the Wild",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Joon Son Chung",
            "Andrew Senior",
            "Oriol Vinyals",
            "Andrew Zisserman"
        ],
        "references": [
            315311266,
            314520875,
            312320208,
            305196650,
            319770183,
            313195364,
            311610155,
            308856024,
            304415024,
            304372660
        ]
    },
    {
        "id": 316450913,
        "title": "Residual Attention Network for Image Classification",
        "abstract": "In this work, we propose \"Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.",
        "date": "April 2017",
        "authors": [
            "Fei Wang",
            "Mengqing Jiang",
            "Chen Qian",
            "Shuo Yang"
        ],
        "references": [
            308845461,
            319770414,
            319770168,
            312727767,
            311610237,
            311609041,
            310441005,
            308859280,
            308277376,
            308277253
        ]
    },
    {
        "id": 316184205,
        "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
        "date": "April 2017",
        "authors": [
            "Andrew G. Howard",
            "Menglong Zhu",
            "Bo Chen",
            "Dmitry Kalenichenko"
        ],
        "references": [
            311223153,
            308457764,
            320968382,
            319770346,
            319770334,
            319770183,
            319769911,
            319769906,
            308277794,
            308277088
        ]
    },
    {
        "id": 310462326,
        "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
        "abstract": "A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks are met with diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. A benchmark on the ILSVRC 2012 validation set demonstrates substantial improvements over the state-of-the-art. Compared to Inception-ResNet-v2, it reduces the top-5 error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",
        "date": "November 2016",
        "authors": [
            "Xingcheng Zhang",
            "Zhizhong Li",
            "Chen Change Loy",
            "Dahua Lin"
        ],
        "references": [
            316970253,
            308320871,
            308278012,
            319770414,
            319770387,
            319770291,
            319770264,
            319770183,
            311609041,
            308821532
        ]
    },
    {
        "id": 303409658,
        "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups",
        "abstract": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).",
        "date": "July 2017",
        "authors": [
            "Yani Ioannou",
            "Duncan P. Robertson",
            "Roberto Cipolla",
            "Antonio Criminisi"
        ],
        "references": [
            308833537,
            319770418,
            319770414,
            319770367,
            319770343,
            319770291,
            319770272,
            319770183,
            319770137,
            306218037
        ]
    },
    {
        "id": 263891809,
        "title": "Deep Networks with Internal Selective Attention through Feedback Connections",
        "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.",
        "date": "July 2014",
        "authors": [
            "Marijn Stollenga",
            "Jonathan Masci",
            "Faustino Gomez",
            "Juergen Schmidhuber"
        ],
        "references": [
            286966779,
            319770626,
            319770264,
            319770183,
            313706874,
            310752533,
            306218037,
            291777069,
            291735245,
            287124401
        ]
    },
    {
        "id": 319770160,
        "title": "Show and Tell: A Neural Image Caption Generator",
        "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU score improvements on Flickr30k, from 55 to 66, and on SBU, from 19 to 27.",
        "date": "November 2014",
        "authors": [
            "Oriol Vinyals",
            "Alexander Toshev",
            "Samy Bengio",
            "Dumitru Erhan"
        ],
        "references": [
            305196650,
            329977566,
            329975395,
            319770465,
            319770439,
            319770357,
            312714920,
            303721259,
            290345348,
            268155634
        ]
    },
    {
        "id": 318337293,
        "title": "Primal-Dual Group Convolutions for Deep Neural Networks",
        "abstract": "In this paper, we present a simple and modularized neural network architecture, named primal-dual group convolutional neural networks (PDGCNets). The main point lies in a novel building block, a pair of two successive group convolutions: primal group convolution and dual group convolution. The two group convolutions are complementary: (i) the convolution on each primal partition in primal group convolution is a spatial convolution, while on each dual partition in dual group convolution, the convolution is a point-wise convolution; (ii) the channels in the same dual partition come from different primal partitions. We discuss one representative advantage: Wider than a regular convolution with the number of parameters and the computation complexity preserved. We also show that regular convolutions, group convolution with summation fusion (as used in ResNeXt), and the Xception block are special cases of primal-dual group convolutions. Empirical results over standard benchmarks, CIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are more efficient in using parameters and computation complexity with similar or higher accuracy.",
        "date": "July 2017",
        "authors": [
            "Ting Zhang",
            "Guo-Jun Qi",
            "Bin Xiao",
            "Jingdong Wang"
        ],
        "references": [
            308320871,
            308278012,
            320968382,
            319770414,
            319770272,
            319770183,
            313879627,
            312727767,
            310769931,
            310441005
        ]
    },
    {
        "id": 313645102,
        "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights",
        "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code will be made publicly available.",
        "date": "February 2017",
        "authors": [
            "Aojun Zhou",
            "Anbang Yao",
            "Guo Yiwen",
            "Lin Xu"
        ],
        "references": [
            306227124,
            305196650,
            304163898,
            303270485,
            319770168,
            319770132,
            319770111,
            308277088,
            301921832,
            301874967
        ]
    },
    {
        "id": 301878495,
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet",
        "date": "February 2016",
        "authors": [
            "Forrest N. Iandola",
            "Song Han",
            "Matthew W. Moskewicz",
            "Khalid Ashraf"
        ],
        "references": [
            316970253,
            316867991,
            319770411,
            319770367,
            319770357,
            319770334,
            319770323,
            319770291,
            319770219,
            319770183
        ]
    },
    {
        "id": 301839500,
        "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",
        "date": "March 2016",
        "authors": [
            "Mart\u00edn Abadi",
            "Ashish Agarwal",
            "Paul Barham",
            "Eugene Brevdo"
        ],
        "references": [
            305196650,
            319770465,
            319770439,
            306209959,
            304533937,
            303256841,
            291286882,
            286594438,
            283780337,
            281952292
        ]
    },
    {
        "id": 287853408,
        "title": "Quantized Convolutional Neural Networks for Mobile Devices",
        "abstract": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate $4 \\sim 6 \\times$ speed-up and $15 \\sim 20 \\times$ compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.",
        "date": "December 2015",
        "authors": [
            "Jiaxiang Wu",
            "Cong Leng",
            "Yuhang Wang",
            "Qingzhu Lin"
        ],
        "references": [
            305196650,
            319770430,
            319770367,
            319770183,
            319770109,
            319769911,
            309076254,
            308867995,
            308854323,
            308299971
        ]
    },
    {
        "id": 282005595,
        "title": "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights",
        "abstract": "Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a \" mean-field \" factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights-if we average the MNN output using the inferred posterior.",
        "date": "December 2014",
        "authors": [
            "Daniel Soudry",
            "Itay Hubara",
            "Ron Meir"
        ],
        "references": [
            319770183,
            286794765,
            280940018,
            272825857,
            268853624,
            267960550,
            267706055,
            260637318,
            254062444,
            248512106
        ]
    },
    {
        "id": 325673550,
        "title": "Efficient Architecture Search by Network Transformation",
        "abstract": "Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.",
        "date": "April 2018",
        "authors": [
            "Han Cai",
            "Tianyao Chen",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "references": [
            318813751,
            309738510,
            319770422,
            319770291,
            319770183,
            319769991,
            314237511,
            309738632,
            308277201,
            306218037
        ]
    },
    {
        "id": 321902574,
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning",
        "abstract": "Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.",
        "date": "December 2017",
        "authors": [
            "Felipe Petroski Such",
            "Vashisht Madhavan",
            "Edoardo Conti",
            "Joel Lehman"
        ],
        "references": [
            321902735,
            321902224,
            317425537,
            320798539,
            320280165,
            319769991,
            319164082,
            318652786,
            318584439,
            314943017
        ]
    },
    {
        "id": 318255371,
        "title": "Dual Path Networks",
        "abstract": "In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with more than 3 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.",
        "date": "July 2017",
        "authors": [
            "Yunpeng Chen",
            "Jianan Li",
            "Huaxin Xiao",
            "Xiaojie Jin"
        ],
        "references": [
            318652675,
            310462326,
            308964584,
            306885833,
            314283062,
            311610731,
            311609041,
            310441005,
            308277376,
            308277201
        ]
    },
    {
        "id": 316598820,
        "title": "DeepArchitect: Automatically Designing and Training Deep Architectures",
        "abstract": "In deep learning, performance is strongly affected by the choice of architecture and hyperparameters. While there has been extensive work on automatic hyperparameter optimization for simple spaces, complex spaces such as the space of deep architectures remain largely unexplored. As a result, the choice of architecture is done manually by the human expert through a slow trial and error process guided mainly by intuition. In this paper we describe a framework for automatically designing and training deep models. We propose an extensible and modular language that allows the human expert to compactly represent complex search spaces over architectures and their hyperparameters. The resulting search spaces are tree-structured and therefore easy to traverse. Models can be automatically compiled to computational graphs once values for all hyperparameters have been chosen. We can leverage the structure of the search space to introduce different model search algorithms, such as random search, Monte Carlo tree search (MCTS), and sequential model-based optimization (SMBO). We present experiments comparing the different algorithms on CIFAR-10 and show that MCTS and SMBO outperform random search. In addition, these experiments show that our framework can be used effectively for model discovery, as it is possible to describe expressive search spaces and discover competitive models without much effort from the human expert. Code for our framework and experiments has been made publicly available.",
        "date": "April 2017",
        "authors": [
            "Renato Negrinho",
            "Geoff Gordon"
        ],
        "references": [
            309738510,
            305196650,
            301839500,
            319770411,
            319770272,
            314237511,
            311609041,
            308813785,
            306218037,
            304781977
        ]
    },
    {
        "id": 313096253,
        "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks",
        "abstract": "For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).",
        "date": "January 2017",
        "authors": [
            "Chrisantha Fernando",
            "Dylan Banarse",
            "Charles Blundell",
            "Yori Zwols"
        ],
        "references": [
            310440792,
            303859203,
            301847678,
            284219622,
            312619873,
            303734483,
            303409493,
            301846299,
            286794765,
            273387909
        ]
    },
    {
        "id": 309738510,
        "title": "Designing Neural Network Architectures using Reinforcement Learning",
        "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modelling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing network design meta-modelling approaches on image classification.",
        "date": "November 2016",
        "authors": [
            "Bowen Baker",
            "Otkrist Gupta",
            "Nikhil Naik",
            "Ramesh Raskar"
        ],
        "references": [
            319769813,
            292074166,
            277411157,
            308846031,
            304781977,
            287175820,
            286512696,
            277290316,
            274572264,
            272837232
        ]
    },
    {
        "id": 308981007,
        "title": "Deep Pyramidal Residual Networks",
        "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolution layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. At the same time, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the capability of high-level attributes. Moreover, this also applies to residual networks and is very closely related to their performance. In this research, instead of using downsampling to achieve a sharp increase at each residual unit, we gradually increase the feature map dimension at all the units to involve as many locations as possible. This is discussed in depth together with our new insights as it has proven to be an effective design to improve the generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR datasets have shown that our network architecture has a superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet",
        "date": "October 2016",
        "authors": [
            "Dongyoon Han",
            "Jiwhan Kim",
            "Junmo Kim"
        ],
        "references": [
            319770430,
            319770414,
            319770411,
            319770357,
            319770291,
            319770272,
            319770191,
            319770183,
            312256274,
            311609041
        ]
    },
    {
        "id": 306885833,
        "title": "Densely Connected Convolutional Networks",
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet.",
        "date": "July 2017",
        "authors": [
            "Gao Huang",
            "Zhuang Liu",
            "Laurens van der Maaten",
            "Kilian Weinberger"
        ],
        "references": [
            319770414,
            319770411,
            319770387,
            319770272,
            319770216,
            319770191,
            319770183,
            319770168,
            312727767,
            311609041
        ]
    },
    {
        "id": 304857984,
        "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks",
        "abstract": "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accom- panied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.",
        "date": "July 2016",
        "authors": [
            "Corinna Cortes",
            "Xavi Gonzalvo",
            "Vitaly Kuznetsov",
            "Mehryar Mohri"
        ],
        "references": [
            284219370,
            282906751,
            281895600,
            319770183,
            319770166,
            313501588,
            304781977,
            287250773,
            281487015,
            279458887
        ]
    },
    {
        "id": 319769813,
        "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
        "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
        "date": "January 2016",
        "authors": [
            "Djork-Arn\u00e9 Clevert",
            "Thomas Unterthiner",
            "Sepp Hochreiter"
        ],
        "references": [
            273157730,
            272752025,
            319770272,
            319770183,
            306221946,
            280329704,
            276461438,
            275974753,
            272194743,
            269935358
        ]
    },
    {
        "id": 284579051,
        "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
        "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers (>= 5). Using ELUs, we obtained the best published single-crop result on CIFAR-100 and CIFAR-10. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with similar classification performance, obtaining less than 10% classification error for a single crop, single model network.",
        "date": "November 2015",
        "authors": [
            "Djork-Arn\u00e9 Clevert",
            "Thomas Unterthiner",
            "Sepp Hochreiter"
        ],
        "references": [
            281118645,
            277282943,
            273157730,
            319770272,
            319770183,
            306221946,
            289786324,
            280329704,
            276461438,
            275974753
        ]
    },
    {
        "id": 269722508,
        "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration",
        "abstract": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "date": "December 2014",
        "authors": [
            "Jonghoon Jin",
            "Aysegul Dundar",
            "Eugenio Culurciello"
        ],
        "references": [
            269935399,
            319770411,
            319770367,
            319770342,
            319770291,
            319770183,
            319770111,
            306218037,
            286569315,
            268624415
        ]
    },
    {
        "id": 260642043,
        "title": "Rigid-Motion Scattering for Texture Classification",
        "abstract": "A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network. Convolutions are calculated on the rigid-motion group, with wavelets defined on the translation and rotation variables. It preserves joint rotation and translation information, while providing global invariants at any desired scale. Texture classification is studied, through the characterization of stationary processes from a single realization. State-of-the-art results are obtained on multiple texture data bases, with important rotation and scaling variabilities.",
        "date": "March 2014",
        "authors": [
            "Laurent SIfre",
            "St\u00e9phane Georges Mallat"
        ],
        "references": [
            279843675,
            268406254,
            266225209,
            259399834,
            319770430,
            319770183,
            316588757,
            313527089,
            267960550,
            261226002
        ]
    },
    {
        "id": 236736831,
        "title": "Acceleration of Stochastic Approximation by Averaging",
        "abstract": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
        "date": "July 1992",
        "authors": [
            "Boris T. Polyak",
            "Anatoli Juditsky"
        ],
        "references": [
            284151356,
            267439735,
            279958061,
            268996515,
            268682619,
            268645196,
            267129803,
            266978677,
            266523004,
            266066589
        ]
    },
    {
        "id": 230867026,
        "title": "Simplifying ConvNets for Fast Learning",
        "abstract": "In this paper, we propose different strategies for simplifying filters, used as feature extractors, to be learnt in convolutional neural networks (ConvNets) in order to modify the hypothesis space, and to speed-up learning and processing times. We study two kinds of filters that are known to be computationally efficient in feed-forward processing: fused convolution/sub-sampling filters, and separable filters. We compare the complexity of the back-propagation algorithm on ConvNets based on these different kinds of filters. We show that using these filters allows to reach the same level of recognition performance as with classical ConvNets for handwritten digit recognition, up to 3.3 times faster.",
        "date": "September 2012",
        "authors": [
            "Franck Mamalet",
            "Christophe Garcia"
        ],
        "references": [
            251423608,
            231556969,
            228344387,
            224340092,
            221415287,
            220320307,
            344849828,
            313139691,
            224164036,
            216792756
        ]
    },
    {
        "id": 319770291,
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.",
        "date": "September 2014",
        "authors": [
            "Karen Simonyan",
            "Andrew Zisserman"
        ],
        "references": []
    },
    {
        "id": 324584089,
        "title": "Simple Baselines for Human Pose Estimation and Tracking",
        "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.",
        "date": "April 2018",
        "authors": [
            "Bin Xiao",
            "Haiping Wu",
            "Yichen Wei"
        ],
        "references": [
            329747444,
            322059684,
            329747604,
            329740421,
            329488196,
            322076361,
            322060558,
            322060396,
            322059217,
            322057861
        ]
    },
    {
        "id": 335144529,
        "title": "Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations",
        "abstract": "",
        "date": "May 2019",
        "authors": [
            "Vladimir Nekrasov",
            "Thanuja Dharmasiri",
            "Andrew Spek",
            "Tom Drummond"
        ],
        "references": [
            319035939,
            317887394,
            317823343,
            317040443,
            315796330,
            315096611,
            311756352,
            311222763,
            303840621,
            303750237
        ]
    },
    {
        "id": 330595799,
        "title": "CReaM: Condensed Real-time Models for Depth Prediction using Convolutional Neural Networks",
        "abstract": "",
        "date": "October 2018",
        "authors": [
            "Andrew Spek",
            "Thanuja Dharmasiri",
            "Tom Drummond"
        ],
        "references": [
            327807807,
            323904676,
            322221144,
            320293291,
            319770123,
            317887394,
            317823343,
            316184205,
            316075716,
            303840621
        ]
    },
    {
        "id": 329748830,
        "title": "Taskonomy: Disentangling Task Transfer Learning",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Amir R. Zamir",
            "Alexander Sax",
            "William Shen",
            "Leonidas J. Guibas"
        ],
        "references": [
            314092103,
            313366031,
            311585806,
            310769756,
            303970238,
            303822170,
            303750237,
            301879687,
            301837491,
            301648368
        ]
    },
    {
        "id": 323570681,
        "title": "On the Power of Over-parametrization in Neural Networks with Quadratic Activation",
        "abstract": "We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training data points, we show as long as $ k \\ge \\sqrt{2n}$, over-parametrization enables local search algorithms to find a \\emph{globally} optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when $k\\ge \\sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.",
        "date": "March 2018",
        "authors": [
            "Simon Du",
            "Jason Lee"
        ],
        "references": [
            320975973,
            320707672,
            318107236,
            316663833,
            314093619,
            311900760,
            310329204,
            309738231,
            304017872,
            292074166
        ]
    },
    {
        "id": 323302973,
        "title": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport",
        "abstract": "Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function $f$ which is only accessible via point evaluations. It is typically used in settings where $f$ is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network \\emph{architectures}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.",
        "date": "February 2018",
        "authors": [
            "Kirthevasan Kandasamy",
            "Willie Neiswanger",
            "Jeff Schneider",
            "Barnabas Poczos"
        ],
        "references": [
            309738510
        ]
    },
    {
        "id": 323118469,
        "title": "Efficient Neural Architecture Search via Parameters Sharing",
        "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.",
        "date": "February 2018",
        "authors": [
            "Hieu Pham",
            "Melody Y. Guan",
            "Barret Zoph",
            "Quoc V. Le"
        ],
        "references": [
            317300069,
            316598820,
            311222630,
            309738510,
            306885833,
            306187421,
            305229133,
            261100864,
            221489926,
            13853244
        ]
    },
    {
        "id": 323027025,
        "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on the PASCAL VOC 2012 semantic image segmentation dataset and achieve a performance of 89% on the test set without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow.",
        "date": "February 2018",
        "authors": [
            "Liang-Chieh Chen",
            "Yukun Zhu",
            "George Papandreou",
            "Florian Schroff"
        ],
        "references": [
            320974965,
            314115448,
            312759848,
            311769895,
            311222763,
            310610877,
            305196650,
            301880609,
            301839500,
            281670742
        ]
    },
    {
        "id": 335463411,
        "title": "Semantic Correlation Promoted Shape-Variant Context for Segmentation",
        "abstract": "Context is essential for semantic segmentation. Due to the diverse shapes of objects and their complex layout in various scene images, the spatial scales and shapes of contexts for different objects have very large variation. It is thus ineffective or inefficient to aggregate various context information from a predefined fixed region. In this work, we propose to generate a scale-and shape-variant semantic mask for each pixel to confine its contextual region. To this end, we first propose a novel paired convolution to infer the semantic correlation of the pair and based on that to generate a shape mask. Using the inferred spatial scope of the contextual region, we propose a shape-variant convolution, of which the receptive field is controlled by the shape mask that varies with the appearance of input. In this way, the proposed network aggregates the context information of a pixel from its semantic-correlated region instead of a predefined fixed region. Furthermore, this work also proposes a labeling denoising model to reduce wrong predictions caused by the noisy low-level features. Without bells and whistles, the proposed segmentation network achieves new state-of-the-arts consistently on the six public segmentation datasets.",
        "date": "June 2019",
        "authors": [
            "Henghui Ding",
            "Xudong Jiang",
            "Bing Shuai",
            "Ai Qun Liu"
        ],
        "references": [
            330411441,
            329740318,
            329740252,
            338512713,
            338509793,
            338508333,
            329745999,
            329743799,
            329743787,
            329743448
        ]
    },
    {
        "id": 339558864,
        "title": "Dynamic Multi-Scale Filters for Semantic Segmentation",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Junjun He",
            "Zhongying Deng",
            "Yu Qiao"
        ],
        "references": [
            335463644,
            335463411,
            329743873,
            329740252,
            329442528,
            315666513,
            314115448,
            311222763,
            308278279,
            305196650
        ]
    },
    {
        "id": 339558490,
        "title": "Unpaired Image Captioning via Scene Graph Alignments",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Jiuxiang Gu",
            "Shafiq Joty",
            "Jianfei Cai",
            "Handong Zhao"
        ],
        "references": [
            335463644,
            335463411,
            329740252,
            322060135,
            321160935,
            321160918,
            320727406,
            319643556,
            311411280,
            308320812
        ]
    },
    {
        "id": 339556516,
        "title": "Joint Learning of Saliency Detection and Weakly Supervised Semantic Segmentation",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Zeng Yu",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Lihe Zhang"
        ],
        "references": [
            335463644,
            335463411,
            334987659,
            329740252,
            325718841,
            319770123,
            319186937,
            318981857,
            318981480,
            311842448
        ]
    },
    {
        "id": 339555830,
        "title": "Deep Learning for Light Field Saliency Detection",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Tiantian Wang",
            "Yongri Piao",
            "Huchuan Lu",
            "Xiao Li"
        ],
        "references": [
            335463644,
            335463411,
            334844293,
            329740252,
            326200125,
            322058016,
            318981857,
            318981480,
            314634750,
            311459324
        ]
    },
    {
        "id": 339555410,
        "title": "Adaptive Context Network for Scene Parsing",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Jun Fu",
            "Jing Liu",
            "Yuhang Wang",
            "Yong Li"
        ],
        "references": [
            335463644,
            335463411,
            329743873,
            329740252,
            329442528,
            314115448,
            311222763,
            302305068,
            301880609,
            278413297
        ]
    },
    {
        "id": 339554650,
        "title": "Fast Video Object Segmentation via Dynamic Targeting Network",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Lu Zhang",
            "Zhe Lin",
            "Jianming Zhang",
            "Huchuan Lu"
        ],
        "references": [
            335463644,
            335463411,
            329743795,
            329740252,
            325809224,
            324104851,
            319164056,
            318959852,
            311715217,
            311459265
        ]
    },
    {
        "id": 338512016,
        "title": "Dual Attention Network for Scene Segmentation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Jun Fu",
            "Jing Liu",
            "Haijie Tian",
            "Yong Li"
        ],
        "references": [
            329743873,
            329740252,
            329442528,
            319736499,
            314115448,
            311222763,
            301880609,
            276923248,
            264975444,
            234768533
        ]
    },
    {
        "id": 338509793,
        "title": "Scene Graph Generation With External Knowledge and Image Reconstruction",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Jiuxiang Gu",
            "Handong Zhao",
            "Zhe Lin",
            "Sheng Li"
        ],
        "references": [
            335463411,
            329740252,
            324492738,
            322060440,
            321498323,
            321160935,
            321160918,
            321124901,
            319643556,
            318814060
        ]
    },
    {
        "id": 338506535,
        "title": "Adaptive Pyramid Context Network for Semantic Segmentation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Junjun He",
            "Zhongying Deng",
            "Lei Zhou",
            "Yali Wang"
        ],
        "references": [
            329743873,
            311222763,
            303448993,
            302305068,
            283471087,
            281670742,
            278413297,
            268689640,
            265295439,
            264975444
        ]
    },
    {
        "id": 322749812,
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
        "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",
        "date": "December 2017",
        "authors": [
            "Vijay Badrinarayanan",
            "Alex Kendall",
            "Roberto Cipolla"
        ],
        "references": [
            332814678,
            319770420,
            319770305,
            319770291,
            319770284,
            319770272,
            319770198,
            319770168,
            314100361,
            311610893
        ]
    },
    {
        "id": 324996175,
        "title": "Understanding Convolution for Semantic Segmentation",
        "abstract": "",
        "date": "March 2018",
        "authors": [
            "Panqu Wang",
            "Pengfei Chen",
            "Ye Yuan",
            "Ding Liu"
        ],
        "references": [
            308262491,
            306357649,
            302305068,
            301899012,
            301880609,
            301877264,
            286134669,
            281670742,
            275588416,
            265295439
        ]
    },
    {
        "id": 322058210,
        "title": "Video Scene Parsing with Predictive Feature Learning",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Xiaojie Jin",
            "Xin Li",
            "Huaxin Xiao",
            "Xiaohui Shen"
        ],
        "references": [
            319770165,
            308190263,
            307984819,
            307307331,
            305655121,
            303544998,
            302305068,
            301899012,
            301880609,
            301877264
        ]
    },
    {
        "id": 320971443,
        "title": "Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Tobias Pohlen",
            "Alexander Hermans",
            "Markus Mathias",
            "Bastian Leibe"
        ],
        "references": [
            322749812,
            311610734,
            306376589,
            305196650,
            303840621,
            303448993,
            302569301,
            302305068,
            301880609,
            283471087
        ]
    },
    {
        "id": 320968233,
        "title": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Guosheng Lin",
            "Anton Milan",
            "Chunhua Shen",
            "Ian Reid"
        ],
        "references": [
            322749812,
            308278291,
            302305068,
            301880609,
            283658606,
            283471087,
            281670742,
            272845187,
            272194536,
            268747978
        ]
    },
    {
        "id": 320968206,
        "title": "Pyramid Scene Parsing Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Hengshuang Zhao",
            "Jianping Shi",
            "Xiaojuan Qi",
            "Xiaogang Wang"
        ],
        "references": [
            322749812,
            306357649,
            305196650,
            303448993,
            302305068,
            301880609,
            283471087,
            281670742,
            279968654,
            278413297
        ]
    },
    {
        "id": 320964900,
        "title": "Large Kernel Matters \u2014 Improve Semantic Segmentation by Global Convolutional Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Chao Peng",
            "Xiangyu Zhang",
            "Gang Yu",
            "Guiming Luo"
        ],
        "references": [
            308278291,
            305196650,
            302305068,
            301880609,
            301877264,
            281670742,
            278413297,
            277334270,
            276923248,
            272194536
        ]
    },
    {
        "id": 319770420,
        "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",
        "date": "December 2015",
        "authors": [
            "Liang-Chieh Chen",
            "George Papandreou",
            "Iasonas Kokkinos",
            "Kevin Murphy"
        ],
        "references": []
    },
    {
        "id": 319770168,
        "title": "Fully Convolutional Networks for Semantic Segmentation",
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
        "date": "November 2014",
        "authors": [
            "Jonathan Long",
            "Evan Shelhamer",
            "Trevor Darrell"
        ],
        "references": [
            305196650,
            272845187,
            265787949,
            264160686,
            263048918,
            262568634,
            259441043,
            240308781,
            227211589,
            221618184
        ]
    },
    {
        "id": 317679203,
        "title": "Rethinking Atrous Convolution for Semantic Image Segmentation",
        "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",
        "date": "June 2017",
        "authors": [
            "Liang-Chieh Chen",
            "George Papandreou",
            "Florian Schroff",
            "Hartwig Adam"
        ],
        "references": [
            320974965,
            317062328,
            315796330,
            314115448,
            311769895,
            311223153,
            311222763,
            310953445,
            310610877,
            309288645
        ]
    },
    {
        "id": 319769910,
        "title": "Efficient piecewise training of deep structure models for semantic segmentation",
        "abstract": "Recent advances on semantic image segmentation are achieved by training deep convolutional neural networks (DCNNs) on a large amount of labelled images. On the other hand, structured models such as conditional random fields (CRFs) are an effective tool for labelling tasks, which play an important role in that CRFs can capture the global context of the image as well as local neighbourhood information. Here we explore contextual information to improve semantic image segmentation by taking advantage of the strength of both DCNNs and CRFs. Specifically, we train CRFs whose potential functions are modelled by fully convolutional neural networks (FCNNs). The resulted deep conditional random fields (DCRFs) are thus able to learn complex feature representations; and during the course of learning, dependencies between the output variables are taken into account. As in conventional DCNNs, the training of our model is performed in an end-to-end fashion using back-propagation. Different from DCNNs, however, inference may be needed at each gradient descent iteration, which can be computationally very expensive since typically millions of iterations are required. To enable efficient training, we propose to use approximate training, namely, piecewise training of CRFs, avoiding repeated inference. We achieve very competitive results on the PASCAL VOC 2012 dataset for semantic segmentation. Our model is trained on the VOC dataset alone with no extra data; and we achieve an intersection-over-union score of 70.7 on its test set, which outperforms state-of-the-art results. Our superior results particularly highlight the usefulness of the learnt pairwise neighbourhood relations modelled by complex FCNNs.",
        "date": "April 2015",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Ian Reid",
            "Anton van den Hengel"
        ],
        "references": [
            273388101,
            319770430,
            319770268,
            319770198,
            319770183,
            319770168,
            308849673,
            308820111,
            301921832,
            273157570
        ]
    },
    {
        "id": 282179651,
        "title": "Vision-Based Offline-Online Perception Paradigm for Autonomous Driving",
        "abstract": "Autonomous driving is a key factor for future mobility. Properly perceiving the environment of the vehicles is essential for a safe driving, which requires computing accurate geometric and semantic information in real-time. In this paper, we challenge state-of-the-art computer vision algorithms for building a perception system for autonomous driving. An inherent drawback in the computation of visual semantics is the trade-off between accuracy and computational cost. We propose to circumvent this problem by following an offline-online strategy. During the offline stage dense 3D semantic maps are created. In the online stage the current driving area is recognized in the maps via a re-localization process, which allows to retrieve the pre-computed accurate semantics and 3D geometry in real-time. Then, detecting the dynamic obstacles we obtain a rich understanding of the current scene. We evaluate quantitatively our proposal in the KITTI dataset and discuss the related open challenges for the computer vision community.",
        "date": "February 2015",
        "authors": [
            "German Ros",
            "Sebastian Ramos",
            "Manuel Granados",
            "Amir Bakhtiary"
        ],
        "references": [
            310775931,
            284040965,
            269250272,
            268733104,
            262398911,
            262284532,
            261416457,
            261415992,
            261415967,
            261263624
        ]
    },
    {
        "id": 277334270,
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling",
        "abstract": "We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models.",
        "date": "May 2015",
        "authors": [
            "Vijay Badrinarayanan",
            "Ankur Handa",
            "Roberto Cipolla"
        ],
        "references": [
            305196650,
            319770305,
            319770198,
            319770183,
            314100361,
            310752533,
            309076254,
            303432735,
            302962450,
            285990902
        ]
    },
    {
        "id": 275588416,
        "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
        "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
        "date": "April 2015",
        "authors": [
            "Philipp Fischer",
            "Alexey Dosovitskiy",
            "Eddy Ilg",
            "Philip H\u00e4usser"
        ],
        "references": [
            319770430,
            319770355,
            319770269,
            319770183,
            319770168,
            310752533,
            308854455,
            302579452,
            301921832,
            289788943
        ]
    },
    {
        "id": 308830795,
        "title": "Multiclass semantic video segmentation with object-level active inference",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Buyu Liu",
            "Xuming He"
        ],
        "references": [
            294282434,
            261391832,
            261116324,
            224262409,
            221304854,
            221304820,
            221304279,
            221111462,
            300358366,
            286690680
        ]
    },
    {
        "id": 302305924,
        "title": "Feature Space Optimization for Semantic Video Segmentation",
        "abstract": "We present an approach to long-range spatio-temporal regularization in semantic video segmentation. Temporal regularization in video is challenging because both the camera and the scene may be in motion. Thus Euclidean distance in the space-time volume is not a good proxy for correspondence. We optimize the mapping of pixels to a Euclidean feature space so as to minimize distances between corresponding points. Structured prediction is performed by a dense CRF that operates on the optimized features. Experimental results demonstrate that the presented approach increases the accuracy and temporal consistency of semantic video segmentation.",
        "date": "June 2016",
        "authors": [
            "Abhijit Kundu",
            "Vibhav Vineet",
            "Vladlen Koltun"
        ],
        "references": [
            319769910,
            302305068,
            301880609,
            281670982,
            277334270,
            261391832,
            256663526,
            256663158,
            221303731,
            221259554
        ]
    },
    {
        "id": 283761983,
        "title": "Attention to Scale: Scale-aware Semantic Image Segmentation",
        "abstract": "Incorporating multi-scale features to deep convolutional neural networks (DCNNs) has been a key element to achieve state-of-art performance on semantic image segmentation benchmarks. One way to extract multi-scale features is by feeding several resized input images to a shared deep network and then merge the resulting multi-scale features for pixel-wise classification. In this work, we adapt a state-of-art semantic image segmentation model with multi-scale input images. We jointly train the network and an attention model which learns to softly weight the multi-scale features, and show that it outperforms average- or max-pooling over scales. The proposed attention model allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output of DCNN for each scale is essential to achieve excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with exhaustive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.",
        "date": "November 2015",
        "authors": [
            "Liang-Chieh Chen",
            "Yi Yang",
            "Jiang Wang",
            "Wei Xu"
        ],
        "references": [
            319769910,
            304409176,
            303137904,
            281670742,
            278413297,
            273005574,
            272194536,
            269332682,
            269116202,
            268689640
        ]
    },
    {
        "id": 276923091,
        "title": "Learning Deconvolution Network for Semantic Segmentation",
        "abstract": "We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.",
        "date": "May 2015",
        "authors": [
            "Hyeonwoo Noh",
            "Seunghoon Hong",
            "Bohyung Han"
        ],
        "references": [
            277334270,
            272845640,
            269116202,
            268689640,
            265787949,
            264979485,
            256663526,
            240308781,
            221361415,
            220659463
        ]
    },
    {
        "id": 319770205,
        "title": "Monocular pedestrian detection: survey and experiments.",
        "abstract": "Pedestrian detection is a rapidly evolving area in computer vision with key applications in intelligent vehicles, surveillance, and advanced robotics. The objective of this paper is to provide an overview of the current state of the art from both methodological and experimental perspectives. The first part of the paper consists of a survey. We cover the main components of a pedestrian detection system and the underlying models. The second (and larger) part of the paper contains a corresponding experimental study. We consider a diverse set of state-of-the-art systems: wavelet-based AdaBoost cascade [74], HOG/linSVM [11], NN/LRF [75], and combined shape-texture detection [23]. Experiments are performed on an extensive data set captured onboard a vehicle driving through urban environment. The data set includes many thousands of training samples as well as a 27-minute test sequence involving more than 20,000 images with annotated pedestrian locations. We consider a generic evaluation setting and one specific to pedestrian detection onboard a vehicle. Results indicate a clear advantage of HOG/linSVM at higher image resolutions and lower processing speeds, and a superiority of the wavelet-based AdaBoost cascade approach at lower image resolutions and (near) real-time processing speeds. The data set (8.5 GB) is made public for benchmarking purposes.",
        "date": "January 2009",
        "authors": [
            "Markus Enzweiler",
            "Dariu M. Gavrila"
        ],
        "references": [
            319770820,
            319770538,
            313566497,
            313425098,
            306353539,
            288905811,
            261227389,
            247487210,
            239059434,
            232472619
        ]
    },
    {
        "id": 311610893,
        "title": "Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Anton van den Hengel",
            "Ian Reid"
        ],
        "references": [
            281670742,
            273388101,
            272845187,
            272194536,
            270454670,
            269116202,
            268747978,
            268689640,
            264975444,
            264160686
        ]
    },
    {
        "id": 311610196,
        "title": "Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Jun Xie",
            "Martin Kiefel",
            "Ming-Ting Sun",
            "Andreas Geiger"
        ],
        "references": [
            301880609,
            294282434,
            282187243,
            279501215,
            272422948,
            271551255,
            269935290,
            269332657,
            265878275,
            264975660
        ]
    },
    {
        "id": 308862494,
        "title": "Learning to segment under various forms of weak supervision",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Jia Xu",
            "Alexander G. Schwing",
            "Raquel Urtasun"
        ],
        "references": [
            273388101,
            264975660,
            261336412,
            261116324,
            235661797,
            228095638,
            221663066,
            221363954,
            221361415,
            221346088
        ]
    },
    {
        "id": 342537621,
        "title": "Gated Fully Fusion for Semantic Segmentation",
        "abstract": "Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of high-level features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features. Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion(GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lower-level features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.",
        "date": "April 2020",
        "authors": [
            "Xiangtai Li",
            "Houlong Zhao",
            "Lei Han",
            "Yunhai Tong"
        ],
        "references": [
            335463411,
            329743873,
            329740252,
            329442528,
            339559337,
            338512016,
            329745999,
            329744331,
            329743799,
            329743787
        ]
    },
    {
        "id": 329743810,
        "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Ting-Chun Wang",
            "Ming-Yu Liu",
            "Jun-Yan Zhu",
            "Andrew Tao"
        ],
        "references": [
            322060458,
            322060135,
            322057512,
            329741422,
            329740448,
            322059903,
            322058797,
            322058318,
            322058208,
            321747972
        ]
    },
    {
        "id": 325718841,
        "title": "PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing",
        "abstract": "Depth estimation and scene parsing are two particularly important tasks in visual scene understanding. In this paper we tackle the problem of simultaneous depth estimation and scene parsing in a joint CNN. The task can be typically treated as a deep multi-task learning problem [42]. Different from previous methods directly optimizing multiple tasks given the input training data, this paper proposes a novel multi-task guided prediction-and-distillation network (PAD-Net), which first predicts a set of intermediate auxiliary tasks ranging from low level to high level, and then the predictions from these intermediate auxiliary tasks are utilized as multi-modal input via our proposed multi-modal distillation modules for the final tasks. During the joint learning, the intermediate tasks not only act as supervision for learning more robust deep representations but also provide rich multi-modal information for improving the final tasks. Extensive experiments are conducted on two challenging datasets (i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing tasks, demonstrating the effectiveness of the proposed approach.",
        "date": "June 2018",
        "authors": [
            "Dan Xu",
            "Wanli Ouyang",
            "Xiaogang Wang",
            "Nicu Sebe"
        ],
        "references": [
            324104638,
            322221396,
            317062328,
            322647914,
            320968206,
            319770420,
            319770183,
            319770168,
            319002575,
            317356667
        ]
    },
    {
        "id": 321417929,
        "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
        "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",
        "date": "November 2017",
        "authors": [
            "Ting-Chun Wang",
            "Ming-Yu Liu",
            "Jun-Yan Zhu",
            "Andrew Tao"
        ],
        "references": [
            322060458,
            322060135,
            322057512,
            321417717,
            322418794,
            322059903,
            322058797,
            322058318,
            322058208,
            320971323
        ]
    },
    {
        "id": 320195152,
        "title": "Learning Affinity via Spatial Propagation Networks",
        "abstract": "In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier. We validate the framework on the task of refinement for image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",
        "date": "October 2017",
        "authors": [
            "Sifei Liu",
            "Shalini De Mello",
            "Jinwei Gu",
            "Guangyu Zhong"
        ],
        "references": [
            308278291,
            304409176,
            311610936,
            311609141,
            308858604,
            308813001,
            308278825,
            303812083,
            303521097,
            301921832
        ]
    },
    {
        "id": 343465903,
        "title": "Strip Pooling: Rethinking Spatial Pooling for Scene Parsing",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Qibin Hou",
            "Li Zhang",
            "Ming-Ming Cheng",
            "Jiashi Feng"
        ],
        "references": [
            335523540,
            335463644,
            335463411,
            335329298,
            335329208,
            329743873,
            329740252,
            329442528,
            323867606,
            314115448
        ]
    },
    {
        "id": 343461519,
        "title": "Dynamic Graph Message Passing Networks",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Li Zhang",
            "Dan Xu",
            "Anurag Arnab",
            "Philip Hilaire Torr"
        ],
        "references": [
            329442528,
            322412623,
            322221396,
            317399572,
            314115448,
            308278291,
            303698513,
            302305068,
            301880609,
            277959210
        ]
    },
    {
        "id": 343454917,
        "title": "Spatial Pyramid Based Graph Reasoning for Semantic Segmentation",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Xia Li",
            "Yibo Yang",
            "Qijie Zhao",
            "Tiancheng Shen"
        ],
        "references": [
            329743873,
            329740252,
            329442528,
            322383418,
            314115448,
            302305068,
            301880609,
            283471087,
            281670742,
            278413297
        ]
    },
    {
        "id": 339561869,
        "title": "FCOS: Fully Convolutional One-Stage Object Detection",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Zhi Tian",
            "Chunhua Shen",
            "Hao Chen",
            "He Tong"
        ],
        "references": [
            339561958,
            335056781,
            338509384,
            338506619,
            338506505,
            338506305,
            324387691,
            322059369,
            322059217,
            320968449
        ]
    },
    {
        "id": 322057895,
        "title": "DeepSetNet: Predicting Sets with Deep Neural Networks",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Hamid Rezatofighi",
            "Vijay Kumar B G",
            "Anton Milan",
            "Ehsan Abbasnejad"
        ],
        "references": [
            320968452,
            320968233,
            319770820,
            319770465,
            319770420,
            319770349,
            319770291,
            319770184,
            319770183,
            319769911
        ]
    },
    {
        "id": 321347654,
        "title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis",
        "abstract": "The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.",
        "date": "November 2017",
        "authors": [
            "Aaron van den Oord",
            "Yazhe Li",
            "Igor Babuschkin",
            "Karen Simonyan"
        ],
        "references": [
            309192180,
            308026508,
            305196650,
            320920335,
            319770355,
            319769909,
            312550770,
            311469848,
            308278061,
            304018350
        ]
    },
    {
        "id": 310769756,
        "title": "Fully Convolutional Instance-aware Semantic Segmentation",
        "abstract": "We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at \\url{https://github.com/daijifeng001/TA-FCN}.",
        "date": "November 2016",
        "authors": [
            "Yi Li",
            "Haozhi Qi",
            "Jifeng Dai",
            "Xiangyang Ji"
        ],
        "references": [
            319769910,
            319770430,
            319770331,
            319770168,
            319769911,
            316588757,
            312727767,
            311610893,
            311610461,
            311610151
        ]
    },
    {
        "id": 308278279,
        "title": "SSD: Single Shot MultiBox Detector",
        "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https:// github. com/ weiliu89/ caffe/ tree/ ssd.",
        "date": "October 2016",
        "authors": [
            "Wei Liu",
            "Dragomir Anguelov",
            "Dumitru Erhan",
            "Christian Szegedy"
        ],
        "references": [
            305638304,
            319770430,
            319770331,
            319770168,
            319769911,
            311611062,
            311609522,
            311609041,
            308808582,
            301921832
        ]
    },
    {
        "id": 259212328,
        "title": "Scalable Object Detection Using Deep Neural Networks",
        "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.",
        "date": "December 2013",
        "authors": [
            "Dumitru Erhan",
            "Christian Szegedy",
            "Alexander Toshev",
            "Dragomir Anguelov"
        ],
        "references": [
            289917319,
            267958868,
            262270555,
            259441043,
            319770260,
            319770183,
            267960550,
            258374356,
            244706524,
            237417270
        ]
    },
    {
        "id": 343466724,
        "title": "Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Shifeng Zhang",
            "Cheng Chi",
            "Yongqiang Yao",
            "Zhen Lei"
        ],
        "references": [
            339561869,
            338503788,
            335056781,
            334773154,
            329743392,
            324166929,
            323867543,
            321241658,
            321180719,
            320797032
        ]
    },
    {
        "id": 338509297,
        "title": "Co-Occurrent Features in Semantic Segmentation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Hang Zhang",
            "Han Zhang",
            "Chenguang Wang",
            "Junyuan Xie"
        ],
        "references": [
            329743873,
            322749812,
            347508984,
            329741355,
            329740332,
            328128947,
            324078024,
            324005873,
            323846355,
            323027025
        ]
    },
    {
        "id": 329743873,
        "title": "Context Encoding for Semantic Segmentation",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Hang Zhang",
            "Kristin J. Dana",
            "Jianping Shi",
            "Zhongyue Zhang"
        ],
        "references": [
            322749812,
            320967995,
            329748363,
            329745796,
            329740202,
            322060368,
            322060088,
            320971174,
            320968233,
            320968206
        ]
    },
    {
        "id": 329740252,
        "title": "Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Henghui Ding",
            "Xudong Jiang",
            "Bing Shuai",
            "Ai Qun Liu"
        ],
        "references": [
            332814678,
            325529707,
            322749812,
            329744331,
            329743448,
            324996175,
            323433964,
            322058130,
            320968233,
            320968206
        ]
    },
    {
        "id": 320971612,
        "title": "Richer Convolutional Features for Edge Detection",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Yun Liu",
            "Ming-Ming Cheng",
            "Xiaowei Hu",
            "Kai Wang"
        ],
        "references": [
            315178093,
            319770430,
            319770420,
            319770291,
            319770284,
            319770183,
            319770168,
            319769911,
            318439989,
            314405838
        ]
    },
    {
        "id": 317230093,
        "title": "CASENet: Deep Category-Aware Semantic Edge Detection",
        "abstract": "Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.",
        "date": "May 2017",
        "authors": [
            "Zhiding Yu",
            "Chen Feng",
            "Ming-Yu Liu",
            "Srikumar Ramalingam"
        ],
        "references": [
            315178093,
            314258679,
            319770430,
            319770420,
            319770349,
            319770168,
            318439989,
            312915478,
            311610325,
            311609706
        ]
    },
    {
        "id": 314182547,
        "title": "Label Refinement Network for Coarse-to-Fine Semantic Segmentation",
        "abstract": "We consider the problem of semantic image segmentation using deep convolutional neural networks. We propose a novel network architecture called the label refinement network that predicts segmentation labels in a coarse-to-fine fashion at several resolutions. The segmentation labels at a coarse resolution are used together with convolutional features to obtain finer resolution segmentation labels. We define loss functions at several stages in the network to provide supervisions at different stages. Our experimental results on several standard datasets demonstrate that the proposed model provides an effective way of producing pixel-wise dense image labeling.",
        "date": "March 2017",
        "authors": [
            "Md Amirul Islam",
            "Shujon Naha",
            "Mrigank Rochan",
            "Neil D. B. Bruce"
        ],
        "references": [
            322749812,
            305196650,
            319770183,
            319770168,
            319769911,
            312727767,
            311344466,
            308849673,
            308808582,
            307664547
        ]
    },
    {
        "id": 346770482,
        "title": "Object-Contextual Representations for Semantic Segmentation",
        "abstract": "In this paper, we study the context aggregation problem in semantic segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. We empirically demonstrate our method achieves competitive performance on various benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission \u201cHRNet + OCR + SegFix\u201d achieves the \\({1}^{\\mathrm {st}}\\) place on the Cityscapes leaderboard by the ECCV 2020 submission deadline. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR.",
        "date": "November 2020",
        "authors": [
            "Yuhui Yuan",
            "Xilin Chen",
            "Jingdong Wang"
        ],
        "references": [
            338509297,
            335467236,
            335463644,
            335463411,
            329743873,
            329740318,
            329442528,
            328308268,
            347509008,
            345324756
        ]
    },
    {
        "id": 321242021,
        "title": "BlockDrop: Dynamic Inference Paths in Residual Networks",
        "abstract": "Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\\% on average, going as high as 36\\% for some images, while maintaining the same 76.4\\% top-1 accuracy on ImageNet.",
        "date": "January 2018",
        "authors": [
            "Zuxuan Wu",
            "Tushar Nagarajan",
            "Abhishek Kumar",
            "Steven Rennie"
        ],
        "references": [
            321124772,
            320971183,
            329743447,
            322060396,
            320971540,
            320965171,
            319770334,
            319770191,
            319770183,
            319524838
        ]
    },
    {
        "id": 283471087,
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
        "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. We show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/",
        "date": "November 2015",
        "authors": [
            "Vijay Badrinarayanan",
            "Alex Kendall",
            "Roberto Cipolla"
        ],
        "references": [
            319769910,
            319770305,
            319770291,
            319770284,
            319770272,
            319770198,
            319770168,
            314100361,
            311610893,
            308849673
        ]
    },
    {
        "id": 276923248,
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "date": "May 2015",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "references": [
            268689640,
            264979485,
            263471626,
            319770430,
            319770183,
            319770168,
            272027131,
            267960550,
            267709055,
            265385906
        ]
    },
    {
        "id": 220659463,
        "title": "The Pascal Visual Object Classes (VOC) challenge",
        "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.",
        "date": "June 2010",
        "authors": [
            "Mark Everingham",
            "Luc Van Gool",
            "Christopher K. I. Williams",
            "John M. Winn"
        ],
        "references": [
            319770820,
            316240097,
            313527089,
            313086331,
            312466400,
            312457795,
            308161149,
            269953350,
            267473326,
            248516115
        ]
    },
    {
        "id": 346160386,
        "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling",
        "abstract": "We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally.",
        "date": "October 2020",
        "authors": [
            "Zichao Guo",
            "Xiangyu Zhang",
            "Haoyuan Mu",
            "Wen Heng"
        ],
        "references": [
            337211284,
            329740382,
            265295439,
            341075580,
            339554815,
            338510249,
            338509389,
            329745708,
            329745117,
            329740282
        ]
    },
    {
        "id": 339559337,
        "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Zilong Huang",
            "Xinggang Wang",
            "Lichao Huang",
            "Chang Huang"
        ],
        "references": [
            329743873,
            329740252,
            329442528,
            314115448,
            311222763,
            302305068,
            301880609,
            281670742,
            276923248,
            259772760
        ]
    },
    {
        "id": 339554584,
        "title": "Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Xin Chen",
            "Lingxi Xie",
            "Jun Wu",
            "Qi Tian"
        ],
        "references": [
            337211284,
            329740382,
            319770123,
            309738510,
            305196650,
            265295439,
            221361415,
            338681644,
            338509164,
            329745708
        ]
    },
    {
        "id": 323655313,
        "title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild",
        "abstract": "Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.",
        "date": "March 2018",
        "authors": [
            "Yuhua Chen",
            "Wen Li",
            "Christos Sakaridis",
            "Dengxin Dai"
        ],
        "references": [
            322060135,
            322058612,
            320163409,
            329748675,
            322058653,
            322058594,
            322058433,
            322058208,
            321417854,
            319770820
        ]
    },
    {
        "id": 321324947,
        "title": "On the Robustness of Semantic Segmentation Models to Adversarial Attacks",
        "abstract": "Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.",
        "date": "November 2017",
        "authors": [
            "Anurag Arnab",
            "Ondrej Miksik",
            "Philip Hilaire Torr"
        ],
        "references": [
            322059321,
            320891224,
            320321492,
            318729572,
            322058778,
            320968206,
            320055388,
            319770183,
            319770168,
            318671233
        ]
    },
    {
        "id": 319312265,
        "title": "Semantic Foggy Scene Understanding with Synthetic Data",
        "abstract": "This work addresses the problem of semantic foggy scene understanding (SFSU). Although extensive research has been performed on image dehazing and on semantic scene understanding with weather-clear images, little attention has been paid to SFSU. Due to the difficulty of collecting and annotating foggy images, we choose to generate synthetic fog on real images that depict weather-clear outdoor scenes, and then leverage these synthetic data for SFSU by employing state-of-the-art convolutional neural networks (CNN). In particular, a complete pipeline to generate synthetic fog on real, weather-clear images using incomplete depth information is developed. We apply our fog synthesis on the Cityscapes dataset and generate Foggy Cityscapes with 20550 images. SFSU is tackled in two fashions: 1) with typical supervised learning, and 2) with a novel semi-supervised learning, which combines 1) with an unsupervised supervision transfer from weather-clear images to their synthetic foggy counterparts. In addition, this work carefully studies the usefulness of image dehazing for SFSU. For evaluation, we present Foggy Driving, a dataset with 101 real-world images depicting foggy driving scenes, which come with ground truth annotations for semantic segmentation and object detection. Extensive experiments show that 1) supervised learning with our synthetic data significantly improves the performance of state-of-the-art CNN for SFSU on Foggy Driving; 2) our semi-supervised learning strategy further improves performance; and 3) image dehazing marginally benefits SFSU with our learning strategy. The datasets, models and code will be made publicly available to encourage further research in this direction.",
        "date": "September 2018",
        "authors": [
            "Christos Sakaridis",
            "Dengxin Dai",
            "Luc Van Gool"
        ],
        "references": [
            320975841,
            318868246,
            316270100,
            320968233,
            320968206,
            319770327,
            319769911,
            319769909,
            318696955,
            314100361
        ]
    },
    {
        "id": 316911609,
        "title": "Safety Verification of Deep Neural Networks",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "Xiaowei Huang",
            "Marta Kwiatkowska",
            "Sen Wang",
            "Min Wu"
        ],
        "references": [
            313394663,
            307560165,
            303486514,
            301879941,
            301875216,
            348496402,
            319770291,
            304226143,
            303032467,
            291437554
        ]
    },
    {
        "id": 316270100,
        "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art",
        "abstract": "Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.",
        "date": "April 2017",
        "authors": [
            "Joel Janai",
            "Fatma Guney",
            "Aseem Behl",
            "Andreas Geiger"
        ],
        "references": [
            320971795,
            320967536,
            320971948,
            320971443,
            320971048,
            320968379,
            320968233,
            320964395,
            319770820,
            319770430
        ]
    },
    {
        "id": 313713721,
        "title": "On Detecting Adversarial Perturbations",
        "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.",
        "date": "February 2017",
        "authors": [
            "Jan Hendrik Metzen",
            "Tim Genewein",
            "Volker Fischer",
            "Bastian Bischoff"
        ],
        "references": [
            309460742,
            309192145,
            307307101,
            306304648,
            305186613,
            319770378,
            317919653,
            311610675,
            306226844,
            305779814
        ]
    },
    {
        "id": 311222763,
        "title": "Wider or Deeper: Revisiting the ResNet Model for Visual Recognition",
        "abstract": "The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp",
        "date": "November 2016",
        "authors": [
            "Zifeng Wu",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "references": [
            316970253,
            308964584,
            306357649,
            319770414,
            319770291,
            319770183,
            319770168,
            309766071,
            308277743,
            308277201
        ]
    },
    {
        "id": 334512882,
        "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
        "abstract": "It is commonly assumed that a person\u2019s emotional state can be readily inferred from his or her facial movements, typically called emotional expressions or facial expressions. This assumption influences legal judgments, policy decisions, national security protocols, and educational practices; guides the diagnosis and treatment of psychiatric illness, as well as the development of commercial applications; and pervades everyday social interactions as well as research in other scientific fields such as artificial intelligence, neuroscience, and computer vision. In this article, we survey examples of this widespread assumption, which we refer to as the common view, and we then examine the scientific evidence that tests this view, focusing on the six most popular emotion categories used by consumers of emotion research: anger, disgust, fear, happiness, sadness, and surprise. The available scientific evidence suggests that people do sometimes smile when happy, frown when sad, scowl when angry, and so on, as proposed by the common view, more than what would be expected by chance. Yet how people communicate anger, disgust, fear, happiness, sadness, and surprise varies substantially across cultures, situations, and even across people within a single situation. Furthermore, similar configurations of facial movements variably express instances of more than one emotion category. In fact, a given configuration of facial movements, such as a scowl, often communicates something other than an emotional state. Scientists agree that facial movements convey a range of information and are important for social communication, emotional or otherwise. But our review suggests an urgent need for research that examines how people actually move their faces to express emotions and other social information in the variety of contexts that make up everyday life, as well as careful study of the mechanisms by which people perceive instances of emotion in one another. We make specific research recommendations that will yield a more valid picture of how people move their faces to express emotions and how they infer emotional meaning from facial movements in situations of everyday life. This research is crucial to provide consumers of emotion research with the translational information they require.",
        "date": "July 2019",
        "authors": [
            "Lisa Feldman Barrett",
            "Ralph Adolphs",
            "Stacy Marsella",
            "Aleix M. Martinez"
        ],
        "references": [
            348972616,
            333685797,
            330941751,
            329825604,
            350442415,
            344472047,
            331103751,
            330860463,
            329768563,
            329648042
        ]
    },
    {
        "id": 330879115,
        "title": "Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media",
        "abstract": "Artificial Intelligence (AI) in the form of different machine learning models is applied to Big Data as a way to turn data into valuable knowledge. The rhetoric is that ensuing predictions work well\u2014with a high degree of autonomy and automation. We argue that we need to analyze the process of applying machine learning in depth and highlight at what point human knowledge production takes place in seemingly autonomous work. This article reintroduces classification theory as an important framework for understanding such seemingly invisible knowledge production in the machine learning development and design processes. We suggest a framework for studying such classification closely tied to different steps in the work process and exemplify the framework on two experiments with machine learning applied to Facebook data from one of our labs. By doing so we demonstrate ways in which classification and potential discrimination take place in even seemingly unsupervised and autonomous models. Moving away from concepts of non-supervision and autonomy enable us to understand the underlying classificatory dispositifs in the work process and that this form of analysis constitutes a first step towards governance of artificial intelligence.",
        "date": "January 2019",
        "authors": [
            "Anja Bechmann",
            "Geoffrey C Bowker"
        ],
        "references": [
            321371928,
            320061136,
            334384387,
            330692906,
            328468455,
            325779498,
            323983704,
            323321910,
            319912549,
            319770291
        ]
    },
    {
        "id": 322877702,
        "title": "Emotion Fingerprints or Emotion Populations? A Meta-Analytic Investigation of Autonomic Features of Emotion Categories",
        "abstract": "The classical view of emotion hypothesizes that certain emotion categories have a specific autonomic nervous system (ANS) \u201cfingerprint\u201d that is distinct from other categories. Substantial ANS variation within a category is presumed to be epiphenomenal. The theory of constructed emotion hypothesizes that an emotion category is a population of context-specific, highly variable instances that need not share an ANS fingerprint. Instead, ANS variation within a category is a meaningful part of the nature of emotion. We present a meta-analysis of 202 studies measuring ANS reactivity during lab-based inductions of emotion in nonclinical samples of adults, using a random effects, multilevel meta-analysis and multivariate pattern classification analysis to test our hypotheses. We found increases in mean effect size for 59.4% of ANS variables across emotion categories, but the pattern of effect sizes did not clearly distinguish 1 emotion category from another. We also observed significant variation within emotion categories; heterogeneity accounted for a moderate to substantial percentage (i.e., I2 \u2265 30%) of variability in 54% of these effect sizes. Experimental moderators epiphenomenal to emotion, such as induction type (e.g., films vs. imagery), did not explain a large portion of the variability. Correction for publication bias reduced estimated effect sizes even further, increasing heterogeneity of effect sizes for certain emotion categories. These findings, when considered in the broader empirical literature, are more consistent with population thinking and other principles from evolutionary biology found within the theory of constructed emotion, and offer insights for developing new hypotheses to understand the nature of emotion.",
        "date": "February 2018",
        "authors": [
            "Erika Siegel",
            "Molly K. Sands",
            "Wim Van den Noortgate",
            "Paul Condon"
        ],
        "references": [
            325671148,
            316633386,
            313117282,
            312917228,
            323424582,
            320182358,
            313610258,
            309590895,
            309590894,
            308760403
        ]
    },
    {
        "id": 329927208,
        "title": "Artificial Unintelligence: How Computers Misunderstand the World",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Meredith Broussard"
        ],
        "references": []
    },
    {
        "id": 329748610,
        "title": "Eye in the Sky: Real-Time Drone Surveillance System (DSS) for Violent Individuals Identification Using ScatterNet Hybrid Deep Learning Network",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Amarjot Singh",
            "Devendra Patil",
            "Omkar S N"
        ],
        "references": [
            323008113,
            319391723,
            319391371,
            313671597,
            313642484,
            286840424,
            278048481,
            277334861,
            273316746,
            272092231
        ]
    },
    {
        "id": 319394978,
        "title": "Artificial Intelligence --- A Modern Approach",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "Stuart J Russell",
            "Peter Norvig"
        ],
        "references": []
    },
    {
        "id": 249983357,
        "title": "How Did Fear Become a Scientific Object and What Kind of Object Is It?",
        "abstract": "This essay provides an analysis of the role of posed facial expressions and of the facial image in the development of the science of fear.",
        "date": "May 2010",
        "authors": [
            "Ruth Leys"
        ],
        "references": [
            234837419,
            209436299,
            26674080,
            12086759,
            11220159,
            8126364,
            8096655,
            7543628,
            298869676,
            290614028
        ]
    },
    {
        "id": 248819167,
        "title": "The Body and the Archive",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "Allan Sekula"
        ],
        "references": [
            245122024
        ]
    },
    {
        "id": 247122070,
        "title": "Pattern Recognition as Rule-Guided Inference",
        "abstract": "The determination of pattern recognition rules is viewed as a problem of inductive inference, guided by generalization rules, which control the generalization process, and problem knowledge rules, which represent the underlying semantics relevant to the recognition problem under consideration. The paper formulates the theoretical framework and a method for inferring general and optimal (according to certain criteria) descriptions of object classes from examples of classification or partial descriptions. An experimental computer implementation of the method is briefly described and illustrated by an example.",
        "date": "July 1980",
        "authors": [
            "Ryszard S. Michalski"
        ],
        "references": [
            220815627,
            242327530,
            220604383,
            220546542
        ]
    },
    {
        "id": 345324463,
        "title": "Analyzing Demographic Bias in Artificially Generated Facial Pictures",
        "abstract": "Artificial generation of facial images is increasingly popular, with machine learning achieving photo-realistic results. Yet, there is a concern that the generated images might not fairly represent all demographic groups. We use a state-of-the-art method to generate 10,000 facial images and find that the generated images are skewed towards young people, especially white women. We provide recommendations to reduce demographic bias in artificial image generation.",
        "date": "April 2020",
        "authors": [
            "Joni Salminen",
            "Soon-Gyo Jung",
            "Shammur Absar Chowdhury",
            "Bernard J. Jansen"
        ],
        "references": [
            337365602,
            336570060,
            328683848,
            327500836,
            338962150,
            337678765,
            329754197,
            329743528,
            328375007,
            328109701
        ]
    },
    {
        "id": 329748624,
        "title": "FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Yu Chen",
            "Ying Tai",
            "Xiaoming Liu",
            "Chunhua Shen"
        ],
        "references": [
            322059851,
            322058760,
            320971420,
            320971182,
            320968363,
            320968273,
            320796380,
            319770414,
            319770411,
            319770355
        ]
    },
    {
        "id": 329745710,
        "title": "New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Yijie Bei",
            "Alex Damian",
            "Shijia Hu",
            "Sachit Menon"
        ],
        "references": [
            329744858,
            323627282,
            321902002,
            318337451,
            333964638,
            329748512,
            329743903,
            320968363,
            319284588,
            319277381
        ]
    },
    {
        "id": 326723251,
        "title": "To Learn Image Super-Resolution, Use a GAN to Learn How to Do Image Degradation First",
        "abstract": "This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling). We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low-and high-resolution images. Our main result is that this network can be now used to effectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories.",
        "date": "September 2018",
        "authors": [
            "Adrian Bulat",
            "Jing Yang",
            "Georgios Tzimiropoulos"
        ],
        "references": [
            329748624,
            325633269,
            322060135,
            321902002,
            329748512,
            329743903,
            322060260,
            322060106,
            322058760,
            322057608
        ]
    },
    {
        "id": 321374755,
        "title": "FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors",
        "abstract": "Face Super-Resolution (SR) is a domain-specific super-resolution problem. The specific facial prior knowledge could be leveraged for better super-resolving face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes full use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To further generate realistic faces, we propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Moreover, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive benchmark experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively. Code will be made available upon publication.",
        "date": "November 2017",
        "authors": [
            "Yu Chen",
            "Ying Tai",
            "Xiaoming Liu",
            "Chunhua Shen"
        ],
        "references": [
            322060207,
            322059851,
            322059217,
            322058760,
            320971420,
            320971182,
            320968363,
            320968273,
            320967385,
            320796380
        ]
    },
    {
        "id": 339555522,
        "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Rameen Abdal",
            "Yipeng Qin",
            "Peter Wonka"
        ],
        "references": [
            338511887,
            331104071,
            322060458,
            322060135,
            321417717,
            318675590,
            318572189,
            313910272,
            310610633,
            308026381
        ]
    },
    {
        "id": 339483013,
        "title": "FairGAN + : Achieving Fair Data Generation and Classification through Generative Adversarial Nets",
        "abstract": "",
        "date": "December 2019",
        "authors": [
            "Depeng Xu",
            "Shuhan Yuan",
            "Lu Zhang",
            "Xintao Wu"
        ],
        "references": [
            315456455,
            314152854,
            310673887,
            310673366,
            280243276,
            263012109,
            228975972,
            224440330,
            220766841,
            334844029
        ]
    },
    {
        "id": 338962150,
        "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
        "date": "January 2020",
        "authors": [
            "Tero Karras",
            "Samuli Laine",
            "Timo Aila"
        ],
        "references": []
    },
    {
        "id": 338514531,
        "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Tero Karras",
            "Samuli Laine",
            "Timo Aila"
        ],
        "references": [
            335678842,
            323217470,
            321796129,
            321417929,
            318830253,
            318572189,
            317930102,
            317356551,
            317300265,
            317087692
        ]
    },
    {
        "id": 320968363,
        "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Christian Ledig",
            "Lucas Theis",
            "Ferenc Huszar",
            "Jose Caballero"
        ],
        "references": [
            313910272,
            305779418,
            305654682,
            305196650,
            302569301,
            301836893,
            286301796,
            284219455,
            284097296,
            279068412
        ]
    },
    {
        "id": 309640954,
        "title": "Fully-Convolutional Siamese Networks for Object Tracking",
        "abstract": "The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object\u2019s appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.",
        "date": "October 2016",
        "authors": [
            "Luca Bertinetto",
            "Jack Valmadre",
            "Joao Henriques",
            "Andrea Vedaldi"
        ],
        "references": [
            319769907,
            311905092,
            319770272,
            319770183,
            319277106,
            313550366,
            313550197,
            311611370,
            311611188,
            311610678
        ]
    },
    {
        "id": 306187421,
        "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on CIFAR-10 and CIFAR-100 datasets where we demonstrate new state-of-the-art results below 4\\% and 19\\%, respectively. Our source code is available at https://github.com/loshchil/SGDR.",
        "date": "August 2016",
        "authors": [
            "Ilya Loshchilov",
            "Frank Hutter"
        ],
        "references": [
            315765130,
            308981007,
            306885833,
            306186068,
            319770414,
            319770334,
            319770239,
            319770183,
            313181074,
            311609041
        ]
    },
    {
        "id": 221620245,
        "title": "Signature Verification Using a Siamese Time Delay Neural Network.",
        "abstract": "",
        "date": "January 1993",
        "authors": [
            "Jane Bromley",
            "Isabelle Guyon",
            "Yann Lecun",
            "Eduard S\u00e4ckinger"
        ],
        "references": [
            30770288,
            248848344,
            243766097,
            243742616,
            224104314,
            220603073,
            220029087,
            216792884,
            216792871,
            216721999
        ]
    },
    {
        "id": 4246277,
        "title": "Dimensionality Reduction by Learning an Invariant Mapping",
        "abstract": "Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.",
        "date": "February 2006",
        "authors": [
            "Raia Hadsell",
            "Sumit Chopra",
            "Yann Lecun"
        ],
        "references": [
            270283023,
            314733943,
            304827012,
            285599593,
            284035345,
            279384963,
            263696764,
            246365307,
            233530439,
            232970799
        ]
    },
    {
        "id": 343456678,
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "abstract": "",
        "date": "June 2020",
        "authors": [
            "Kaiming He",
            "Haoqi Fan",
            "Yuxin Wu",
            "Saining Xie"
        ],
        "references": [
            323931846,
            301880609,
            301837491,
            277023095,
            263471626,
            221361415,
            221346269,
            220659463,
            220320709,
            4246277
        ]
    },
    {
        "id": 339559093,
        "title": "Unsupervised Pre-Training of Image Features on Non-Curated Data",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Mathilde Caron",
            "Piotr Bojanowski",
            "Julien Mairal",
            "Armand Joulin"
        ],
        "references": [
            323931846,
            318337409,
            317100995,
            316948359,
            307536552,
            301876671,
            301837491,
            279839496,
            277023095,
            275974132
        ]
    },
    {
        "id": 338503604,
        "title": "Unsupervised Embedding Learning via Invariant and Spreading Instance Feature",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Mang Ye",
            "Xu Zhang",
            "P C Yuen",
            "S. Chang"
        ],
        "references": [
            324104512,
            322160369,
            322059505,
            321241844,
            320075450,
            315514719,
            305652274,
            305196650,
            301837491,
            301817268
        ]
    },
    {
        "id": 329743791,
        "title": "Unsupervised Feature Learning via Non-parametric Instance Discrimination",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Zhirong Wu",
            "Yuanjun Xiong",
            "Stella X. Yu",
            "Dahua Lin"
        ],
        "references": [
            316505674,
            316437893,
            314234312,
            311106922,
            305881526,
            301837491,
            279839496,
            277023095,
            265295439,
            263471626
        ]
    },
    {
        "id": 315454672,
        "title": "Mask R-CNN",
        "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
        "date": "March 2017",
        "authors": [
            "Kaiming He",
            "Georgia Gkioxari",
            "Piotr Doll\u00e1r",
            "Ross Girshick"
        ],
        "references": [
            308833537,
            308277256,
            301880946,
            272521784,
            269332682,
            268689640,
            262270555,
            320971842,
            320971540,
            320971305
        ]
    },
    {
        "id": 323931846,
        "title": "Unsupervised Representation Learning by Predicting Image Rotations",
        "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
        "date": "March 2018",
        "authors": [
            "Spyros Gidaris",
            "Praveer Singh",
            "Nikos Komodakis"
        ],
        "references": [
            311106922,
            320968627,
            319770355,
            319327462,
            319235502,
            316235160,
            311736860,
            311611060,
            311610615,
            308813785
        ]
    },
    {
        "id": 301837491,
        "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
        "abstract": "In this paper we study the problem of image representation learning without human annotation. Following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a Siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN is a more compact version of AlexNet, but with the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their cor-rect spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. The performance in object detection of features extracted from the CFN is the highest (51.8%) among unsupervisedly trained features, and very close to that of supervisedly trained features (56.5%). In object classification the CFN features achieve also the best accuracy (38.1%) among unsupervisedly trained features on the ImageNet 2012 dataset.",
        "date": "March 2016",
        "authors": [
            "Mehdi Noroozi",
            "Paolo Favaro"
        ],
        "references": [
            319770352,
            284579529,
            319770430,
            319770183,
            310752533,
            307822569,
            304409339,
            301816789,
            285599320,
            284035345
        ]
    },
    {
        "id": 277023095,
        "title": "Unsupervised Visual Representation Learning by Context Prediction",
        "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",
        "date": "May 2015",
        "authors": [
            "Carl Doersch",
            "Abhinav Gupta",
            "Alexei A. Efros"
        ],
        "references": [
            319770352,
            319770430,
            319770183,
            319770168,
            314727145,
            307955489,
            304409339,
            303606103,
            301921832,
            300358778
        ]
    },
    {
        "id": 263471626,
        "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks",
        "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).",
        "date": "June 2014",
        "authors": [
            "Alexey Dosovitskiy",
            "Jost Tobias Springenberg",
            "Martin Riedmiller",
            "Thomas Brox"
        ],
        "references": [
            319770430,
            319770395,
            319770357,
            319770305,
            319770183,
            319770168,
            313619815,
            313561101,
            310752533,
            308808582
        ]
    },
    {
        "id": 339555805,
        "title": "Local Aggregation for Unsupervised Learning of Visual Embeddings",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Chengxu Zhuang",
            "Alex Zhai",
            "Daniel Yamins"
        ],
        "references": [
            323931846,
            319035676,
            311586074,
            310953155,
            303822179,
            301837491,
            279839496,
            277023095,
            269723220,
            222464584
        ]
    },
    {
        "id": 334634583,
        "title": "Estimating the success of re-identifications in incomplete datasets using generative models",
        "abstract": "While rich medical, behavioral, and socio-demographic data are key to modern data-driven research, their collection and use raise legitimate privacy concerns. Anonymizing datasets through de-identification and sampling before sharing them has been the main tool used to address those concerns. We here propose a generative copula-based method that can accurately estimate the likelihood of a specific person to be correctly re-identified, even in a heavily incomplete dataset. On 210 populations, our method obtains AUC scores for predicting individual uniqueness ranging from 0.84 to 0.97, with low false-discovery rate. Using our model, we find that 99.98% of Americans would be correctly re-identified in any dataset using 15 demographic attributes. Our results suggest that even heavily sampled anonymized datasets are unlikely to satisfy the modern standards for anonymization set forth by GDPR and seriously challenge the technical and legal adequacy of the de-identification release-and-forget model.",
        "date": "July 2019",
        "authors": [
            "Luc Rocher",
            "Julien M. Hendrickx",
            "Yves-Alexandre de Montjoye"
        ],
        "references": [
            325000039,
            321873477,
            312571840,
            298909822,
            325250558,
            313095392,
            311919575,
            304535150,
            290937815,
            289999194
        ]
    },
    {
        "id": 333332997,
        "title": "Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical",
        "abstract": "",
        "date": "May 2019",
        "authors": [
            "Luciano Floridi"
        ],
        "references": [
            334166840,
            332974675,
            329192820,
            328292318,
            315705213,
            233806053,
            228133505,
            351401372,
            332811345,
            326167496
        ]
    },
    {
        "id": 318337409,
        "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks still increases linearly with orders of magnitude of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on any vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",
        "date": "July 2017",
        "authors": [
            "Chen Sun",
            "Abhinav Shrivastava",
            "Saurabh Singh",
            "Abhinav Gupta"
        ],
        "references": [
            319770352,
            311223153,
            319770430,
            319770328,
            319770001,
            312727767,
            312170738,
            310953055,
            309444714,
            308965018
        ]
    },
    {
        "id": 314285557,
        "title": "Beyond \u2018Revenge Porn\u2019: The Continuum of Image-Based Sexual Abuse",
        "abstract": "In the last few years, many countries have introduced laws combating the phenomenon colloquially known as \u2018revenge porn\u2019. While new laws criminalising this practice represent a positive step forwards, the legislative response has been piecemeal and typically focuses only on the practices of vengeful ex-partners. Drawing on Liz Kelly\u2019s (1988) pioneering work, we suggest that \u2018revenge porn\u2019 should be understood as just one form of a range of gendered, sexualised forms of abuse which have common characteristics, forming what we are conceptualising as the \u2018continuum of image-based sexual abuse\u2019. Further, we argue that image-based sexual abuse is on a continuum with other forms of sexual violence. We suggest that this twin approach may enable a more comprehensive legislative and policy response that, in turn, will better reflect the harms to victim-survivors and leads to more appropriate and effective educative and preventative strategies.",
        "date": "April 2017",
        "authors": [
            "Clare Mcglynn",
            "erika rackley",
            "Ruth Alice Houghton"
        ],
        "references": [
            312164958,
            311100762,
            310612916,
            348103697,
            344077641,
            333555653,
            320300944,
            308878469,
            305942764,
            304338281
        ]
    },
    {
        "id": 306023833,
        "title": "Deep Expectation of Real and Apparent Age from a Single Image Without Facial Landmarks",
        "abstract": "In this paper we propose a deep learning solution to age estimation from a single face image without the use of facial landmarks and introduce the IMDB-WIKI dataset, the largest public dataset of face images with age and gender labels. If the real age estimation research spans over decades, the study of apparent age estimation or the age as perceived by other humans from a face image is a recent endeavor. We tackle both tasks with our convolutional neural networks (CNNs) of VGG-16 architecture which are pre-trained on ImageNet for image classification. We pose the age estimation problem as a deep classification problem followed by a softmax expected value refinement. The key factors of our solution are: deep learned models from large data, robust face alignment, and expected value formulation for age regression. We validate our methods on standard benchmarks and achieve state-of-the-art results for both real and apparent age estimation.",
        "date": "April 2018",
        "authors": [
            "Rasmus Rothe",
            "Radu Timofte",
            "Luc Van Gool"
        ],
        "references": [
            309185766,
            305196650,
            319770430,
            319770183,
            312538118,
            310666547,
            309076254,
            308299971,
            305748100,
            305327283
        ]
    },
    {
        "id": 302515980,
        "title": "How Masculine Is a Flute? A Replication Study on Gender Stereotypes and Preferences for Musical Instruments among Young Children",
        "abstract": "Our study aimed at replicating the previous findings of Harrison and O\u2019Neill (2000) with a younger sample (N = 90, aged 4 to 6) and expanding the repeated measures design to detect the influence of different treatments on preference as well as gender stereotypes. During a priming phase, all children were accustomed with look and sound of six musical instruments. In the following intervention phase, groups 1 and 2 received concerts played by either gender-consistent or gender-inconsistent role models and group 3 received an episode of a TV-series for kids. Regarding both boys and girls, results reveal no significant changes in terms of instrumental preferences except for the guitar (\u2642: p = .026, \u2640: p = .001): all subjects tend to prefer this instrument more at the second time of measurement. Nevertheless, differences among the three groups are not significant in terms of preferences. Regarding gender stereotypes, boys (p = .001) as well as girls (p = .061) show significant differences or tendencies for flute. With regard to drums, girls show a significant interaction (p = .031) in reference to time and group. Nevertheless, post hoc tests do not indicate any significance among the three different groups. Regardless of sex, children assign all instruments rather according to their own gender. Our findings clearly differ from the previous, perhaps caused by confounding variables and methodological problems such as children\u2019s conversations between priming and intervention phase. However, we think that our results are mainly due to the fact that today\u2019s boys and girls have developed distinguished and surprisingly stable preferences for musical instruments even at this young age. Contacts beyond the interventions may explain the observed changes in preferences for the guitar. Nevertheless, gender identities and gender-typed activities correspond with social contexts and therefore each instrument can be appropriate, what offers best chances for early instrumental learning.",
        "date": "July 2016",
        "authors": [
            "Claudia Bullerjahn",
            "Katharina Heller",
            "Jan H. Hoffmann"
        ],
        "references": [
            257663820,
            312528237,
            298596122,
            285980681,
            284857953,
            283121792,
            270626559,
            258198928,
            258198313,
            257663835
        ]
    },
    {
        "id": 271196763,
        "title": "Unequal Representation and Gender Stereotypes in Image Search Results for Occupations",
        "abstract": "Information environments have the power to affect people\u2019s perceptions and behaviors. In this paper, we present the results of studies in which we characterize the gender bias present in image search results for a variety of occupations. We experimentally evaluate the effects of bias in image search results on the images people choose to represent those careers and on people\u2019s perceptions of the prevalence of men and women in each occupation. We find evidence for both stereotype exaggeration and systematic underrepresentation of women in search results. We also find that people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people\u2019s perceptions about real-world distributions. We also discuss tensions between desires for high-quality results and broader societal goals for equality of representation in this space.",
        "date": "April 2015",
        "authors": [
            "Matthew Kay",
            "Cynthia Matuszek",
            "Sean A. Munson"
        ],
        "references": [
            264436461,
            264435804,
            318850845,
            314610873,
            289963745,
            281453286,
            279542701,
            265231179,
            263678640,
            263566813
        ]
    },
    {
        "id": 322587393,
        "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Jieyu Zhao",
            "Tianlu Wang",
            "Mark Yatskar",
            "Vicente Ordonez"
        ],
        "references": [
            316973825,
            307747289,
            301445860,
            292676834,
            283471618,
            323983704,
            320964512,
            311611436,
            311609207,
            303367536
        ]
    },
    {
        "id": 315818258,
        "title": "Valence, arousal, familiarity, concreteness, and imageability ratings for 292 two-character Chinese nouns in Cantonese speakers in Hong Kong",
        "abstract": "Words are frequently used as stimuli in cognitive psychology experiments, for example, in recognition memory studies. In these experiments, it is often desirable to control for the words\u2019 psycholinguistic properties because differences in such properties across experimental conditions might introduce undesirable confounds. In order to avoid confounds, studies typically check to see if various affective and lexico-semantic properties are matched across experimental conditions, and so databases that contain values for these properties are needed. While word ratings for these variables exist in English and other European languages, ratings for Chinese words are not comprehensive. In particular, while ratings for single characters exist, ratings for two-character words\u2014which often have different meanings than their constituent characters, are scarce. In this study, ratings for 292 two-character Chinese nouns were obtained from Cantonese speakers in Hong Kong. Affective variables, including valence and arousal, and lexico-semantic variables, including familiarity, concreteness, and imageability, were rated in the study. The words were selected from a film subtitle database containing word frequency information that could be extracted and listed alongside the resulting ratings. Overall, the subjective ratings showed good reliability across all rated dimensions, as well as good reliability within and between the different groups of participants who each rated a subset of the words. Moreover, several well-established relationships between the variables found consistently in other languages were also observed in this study, demonstrating that the ratings are valid. The resulting word database can be used in studies where control for the above psycholinguistic variables is critical to the research design.",
        "date": "March 2017",
        "authors": [
            "Lydia T. S. Yee"
        ],
        "references": [
            312151732,
            306550226,
            283123157,
            281768543,
            273137309,
            304246676,
            288352391,
            285014368,
            283544666,
            277086910
        ]
    },
    {
        "id": 307473328,
        "title": "What makes ImageNet good for transfer learning?",
        "abstract": "The tremendous success of features learnt using the ImageNet classification task on a wide range of transfer tasks begs the question: what are the intrinsic properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?",
        "date": "August 2016",
        "authors": [
            "Minyoung Huh",
            "Pulkit Agrawal",
            "Alexei Efros"
        ],
        "references": [
            319770438,
            319770352,
            319770430,
            319770357,
            319770183,
            319770133,
            319769911,
            311610098,
            311609232,
            311609147
        ]
    },
    {
        "id": 303545933,
        "title": "When corporations come to define the visual politics of gender: The case of Getty Images",
        "abstract": "While stock photographs have come to saturate media and have been mocked for their clich\u00e9d nature, for example where women are pictured laughing alone with salad, a powerful corporation like Getty Images that disseminates commercial imagery globally has sought to challenge these stereotypes by making more politicized images. This article examines one such case, that is, Getty\u2019s Genderblend visual trend, which claims to portray gender identities and relations in ways that are both more inclusive and diverse, harnessing feminist theory as part of its promotion. Taking a multimodal discourse and visual design approach, the article looks at how corporate imagery can be styled as political and, in turn, how a politics of difference itself is shaped in the interests of the ideologies of consumer capitalism.",
        "date": "March 2016",
        "authors": [
            "Giorgia Aiello",
            "Anna Woodhouse"
        ],
        "references": [
            275449771,
            270720775,
            263678467,
            311788750,
            293428138,
            293427464,
            276935749,
            275449590,
            274330635,
            242206025
        ]
    },
    {
        "id": 301844872,
        "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
        "abstract": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
        "date": "May 2017",
        "authors": [
            "Ranjay Krishna",
            "Yuke Zhu",
            "Oliver Groth",
            "Justin Johnson"
        ],
        "references": [
            319770439,
            319770430,
            319770353,
            319770249,
            319770183,
            319769911,
            314100361,
            310828080,
            310439564,
            308871896
        ]
    },
    {
        "id": 295853816,
        "title": "Thirty shades of offensiveness: L1 and LX English users\u2019 understanding, perception and self-reported use of negative emotion-laden words",
        "abstract": "Previous research on multilinguals\u2019 emotion-laden words has shown that these have more emotional weight in the first language(s) than in languages acquired later in life (Dewaele, 2013). The present study investigates this further with a list of 30 emotion-laden words extracted from the British National Corpus that range in emotional valence from mildly negative to extremely negative. An analysis of data collected via an online questionnaire from 1159 native English (L1) users and 1165 English foreign language (LX) users revealed, surprisingly, that LX users overestimated the offensiveness of most words, with the exception of the most offensive one in the list. It is suggested that when coming across these words in a classroom, learners are warned about them and they attach a red flag to them reminding them of their power. As a result they generally overestimate the power they fail to perceive accurately themselves. LX users were significantly less sure about the exact meaning of most words compared to the L1 users and reported more frequent use of relatively less offensive words while the L1 users reported higher use of more taboo words. Variation among LX users was linked to having (or not) lived in English-speaking environments, to context of acquisition and to self-perceived level of proficiency in English LX. Download for free from: http://authors.elsevier.com/a/1Sbr-1L-nh2thD",
        "date": "February 2016",
        "authors": [
            "Jean-Marc Dewaele"
        ],
        "references": [
            298435975,
            291836445,
            291835354,
            285760188,
            282052407,
            313369235,
            298435974,
            285441683,
            284618054,
            283226486
        ]
    },
    {
        "id": 284219429,
        "title": "Censoring Representations with an Adversary",
        "abstract": "In practice, there are often constraints on the decisions that may be made for a decision problem, or in communicating data. One example of such a constraint is that a decision must not favour a particular group. Another is that data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this in the specific contexts of making decisions free from discrimination and removing private information from images.",
        "date": "November 2015",
        "authors": [
            "Harrison Edwards",
            "Amos Storkey"
        ],
        "references": [
            283532090,
            280243276,
            277333816,
            319649103,
            311699861,
            303256841,
            289561252,
            278733352,
            272825857,
            269935192
        ]
    },
    {
        "id": 280530043,
        "title": "Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Brendan Klare",
            "Ben Klein",
            "Emma Taborsky",
            "Austin Blanton"
        ],
        "references": [
            280530166,
            269326213,
            265125544,
            265051004,
            263697736,
            261392568,
            286651051,
            280530237,
            266298783,
            263564119
        ]
    },
    {
        "id": 341693469,
        "title": "Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI",
        "abstract": "",
        "date": "April 2020",
        "authors": [
            "Michael Madaio",
            "Luke Stark",
            "Jennifer Wortman Vaughan",
            "Hanna Wallach"
        ],
        "references": [
            333874695,
            333796160,
            332828046,
            332742200,
            352867381,
            344708255,
            335579286,
            333942616,
            332771223,
            332741841
        ]
    },
    {
        "id": 338403092,
        "title": "Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-oriented Data Annotation Practices",
        "abstract": "In this paper, we analyze the relation between biased data- driven outcomes and practices of data annotation for vision models, by placing them in the context of market economy. Understanding data annotation as a sense-making process, we investigate which goals are prioritized by decision-makers throughout the annotation of datasets. Following a qualitative design, the study is based on 24 interviews with relevant ac- tors and extensive participatory observations, including sev- eral weeks of fieldwork at two companies dedicated to data annotation for machine learning in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented val- ues over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field: profit, standardization, and opacity. Finally, we intro- duce three elements, namely transparency, education, and regulations, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes.",
        "date": "January 2020",
        "authors": [
            "Gunay Kazimzade",
            "Milagros Miceli"
        ],
        "references": [
            332748249,
            330297860,
            326048577,
            288669223,
            266859800,
            325981161,
            317996556,
            286813583,
            270960227,
            262358421
        ]
    },
    {
        "id": 332748249,
        "title": "How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation",
        "abstract": "With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",
        "date": "April 2019",
        "authors": [
            "Michael Muller",
            "Ingrid Lange",
            "Dakuo Wang",
            "David Piorkowski"
        ],
        "references": [
            338375532,
            326861053,
            326321359,
            325827444,
            324069323,
            324016889,
            336388239,
            335273310,
            327565927,
            326499262
        ]
    },
    {
        "id": 329990585,
        "title": "Understanding and Mitigating Worker Biases in the Crowdsourced Collection of Subjective Judgments",
        "abstract": "Crowdsourced data acquired from tasks that comprise a subjective component (e.g. opinion detection, sentiment analysis) is potentially affected by the inherent bias of crowd workers who contribute to the tasks. This can lead to biased and noisy ground-truth data, propagating the undesirable bias and noise when used in turn to train machine learning models or evaluate systems. In this work, we aim to understand the influence of workers' own opinions on their performance in the subjective task of bias detection. We analyze the influence of workers' opinions on their annotations corresponding to different topics. Our findings reveal that workers with strong opinions tend to produce biased annotations. We show that such bias can be mitigated to improve the overall quality of the data collected. Experienced crowd workers also fail to distance themselves from their own opinions to provide unbiased annotations.",
        "date": "December 2018",
        "authors": [
            "Christoph Hube",
            "Besnik Fetahu",
            "Ujwal Gadiraju"
        ],
        "references": [
            325330277,
            324641488,
            322209540,
            319632874,
            319280264,
            317989672,
            333555828,
            333191247,
            328953368,
            326221106
        ]
    },
    {
        "id": 325594485,
        "title": "Baselines and a datasheet for the Cerema AWP dataset",
        "abstract": "This paper presents the recently published Cerema AWP (Adverse Weather Pedestrian) dataset for various machine learning tasks and its exports in machine learning friendly format. We explain why this dataset can be interesting (mainly because it is a greatly controlled and fully annotated image dataset) and present baseline results for various tasks. Moreover, we decided to follow the very recent suggestions of datasheets for dataset, trying to standardize all the available information of the dataset, with a transparency objective.",
        "date": "June 2018",
        "authors": [
            "Ismaila Seck",
            "Khouloud Dahmane",
            "Pierre Duthon",
            "Ga\u00eblle Loosli"
        ],
        "references": []
    },
    {
        "id": 324670607,
        "title": "Gender Recognition or Gender Reductionism?: The Social Implications of Embedded Gender Recognition Systems",
        "abstract": "Automatic Gender Recognition (AGR) refers to various computational methods that aim to identify an individual's gender by extracting and analyzing features from images, video, and/or audio. Applications of AGR are increasingly being explored in domains such as security, marketing, and social robotics. However, little is known about stakeholders' perceptions and attitudes towards AGR and how this technology might disproportionately affect vulnerable communities. To begin to address these gaps, we interviewed 13 transgender individuals, including three transgender technology designers, about their perceptions and attitudes towards AGR. We found that transgender individuals have overwhelmingly negative attitudes towards AGR and fundamentally question whether it can accurately recognize such a subjective aspect of their identity. They raised concerns about privacy and potential harms that can result from being incorrectly gendered, or misgendered, by technology. We present a series of recommendations on how to accommodate gender diversity when designing new digital systems.",
        "date": "April 2018",
        "authors": [
            "Foad Hamidi",
            "Morgan Klaus Scheuerman",
            "Stacy Branham"
        ],
        "references": [
            344575773,
            309089022,
            305109810,
            304710082,
            303563452,
            318107214,
            316712454,
            316706136,
            308659406,
            302073924
        ]
    },
    {
        "id": 317888465,
        "title": "Networked Employment Discrimination",
        "abstract": "Employers often struggle to assess qualified applicants, particularly in contexts where they receive hundreds of applications for job openings. In an effort to increase efficiency and improve the process, many have begun employing new tools to sift through these applications, looking for signals that a candidate is \u201cthe best fit.\u201d Some companies use tools that offer algorithmic assessments of workforce data to identify the variables that lead to stronger employee performance, or to high employee attrition rates, while others turn to third party ranking services to identify the top applicants in a labor pool. Still others eschew automated systems, but rely heavily on publicly available data to assess candidates beyond their applications. For example, some HR managers turn to LinkedIn to determine if a candidate knows other employees or to identify additional information about them or their networks. Although most companies do not intentionally engage in discriminatory hiring practices (particularly on the basis of protected classes), their reliance on automated systems, algorithms, and existing networks systematically benefits some at the expense of others, often without employers even recognizing the biases of such mechanisms. The intersection of hiring practices and the Big Data phenomenon has not produced inherently new challenges. While this paper addresses issues of privacy, fairness, transparency, accuracy, and inequality under the rubric of discrimination, it does not pivot solely around the legal definitions of discrimination under current federal anti-discrimination law. Rather, it describes a number of areas where issues of inherent bias intersect with, or come into conflict with, socio-cultural notions of fairness.",
        "date": "January 2014",
        "authors": [
            "Alex Rosenblat",
            "Tamara Kneese",
            "danah boyd"
        ],
        "references": [
            273599032,
            228164835,
            323983704,
            314469713,
            295556936,
            258825673,
            246950863,
            235356809,
            23149941
        ]
    },
    {
        "id": 297664844,
        "title": "Thinking critically about and researching algorithms",
        "abstract": "More and more aspects of our everyday lives are being mediated, augmented, produced and regulated by software-enabled technologies. Software is fundamentally composed of algorithms: sets of defined steps structured to process instructions/data to produce an output. This paper synthesises and extends emerging critical thinking about algorithms and considers how best to research them in practice. Four main arguments are developed. First, there is a pressing need to focus critical and empirical attention on algorithms and the work that they do given their increasing importance in shaping social and economic life. Second, algorithms can be conceived in a number of ways \u2013 technically, computationally, mathematically, politically, culturally, economically, contextually, materially, philosophically, ethically \u2013 but are best understood as being contingent, ontogenetic and performative in nature, and embedded in wider socio-technical assemblages. Third, there are three main challenges that hinder research about algorithms (gaining access to their formulation; they are heterogeneous and embedded in wider systems; their work unfolds contextually and contingently), which require practical and epistemological attention. Fourth, the constitution and work of algorithms can be empirically studied in a number of ways, each of which has strengths and weaknesses that need to be systematically evaluated. Six methodological approaches designed to produce insights into the nature and work of algorithms are critically appraised. It is contended that these methods are best used in combination in order to help overcome epistemological and practical challenges.",
        "date": "February 2016",
        "authors": [
            "Rob Kitchin"
        ],
        "references": [
            306224576,
            346624964,
            346447137,
            344486679,
            336971981,
            309043557,
            298883947,
            297735227,
            288780647,
            285685562
        ]
    },
    {
        "id": 288669223,
        "title": "The Politics of Measurement and Action",
        "abstract": "Contemporary decisions about the management of populations, public services, security, and the environment are increasingly made through knowledge gleaned from \"big data\" and its attendant infrastructures and algorithms. Though often described as \"raw,\" this data is produced by techniques of measurement that are imbued with judgments and values that dictate what is counted and what is not, what is considered the best unit of measurement, and how different things are grouped together and \"made\" into a measureable entity. In this paper, we analyze these politics of measurement and how they relate to action through two case studies involving high stake public health measurements where experts intentionally leverage measurement to change definitions of harm and health. That is, they use measurement for activism. The case studies offer a framework for thinking about of how the politics of measurement are present in user interfaces. It is usually assumed that the human element has been scrubbed from the database and that significant political and subjective interventions come from the analysis or use of data after the fact. Instead, we argue that human-computer interactions start before the data reaches the computer because various measurement interfaces are the invisible premise of data and databases, and these measurements are political.",
        "date": "April 2015",
        "authors": [
            "Max Liboiron",
            "Kathleen Pine"
        ],
        "references": [
            346624651,
            346447137,
            333555794,
            321877775,
            311805038,
            291776274,
            275995045,
            273089461,
            272721982,
            269758508
        ]
    },
    {
        "id": 271647061,
        "title": "Leveraging In-Batch Annotation Bias for Crowdsourced Active Learning",
        "abstract": "Data annotation bias is found in many situations. Often it can be ignored as just another component of the noise floor. However, it is especially prevalent in crowdsourcing tasks and must be actively managed. Annotation bias on single data items has been studied with regard to data diffi-culty, annotator bias, etc., while annotation bias on batches of multiple data items simultaneously presented to anno-tators has not been studied. In this paper, we verify the existence of \" in-batch annotation bias \" between data items in the same batch. We propose a factor graph based batch annotation model to quantitatively capture the in-batch an-notation bias, and measure the bias during a crowdsourcing annotation process of inappropriate comments in LinkedIn. We discover that annotators tend to make polarized annota-tions for the entire batch of data items in our task. We fur-ther leverage the batch annotation model to propose a novel batch active learning algorithm. We test the algorithm on a real crowdsourcing platform and find that it outperforms in-batch bias na\u00a8ve algorithms.",
        "date": "February 2015",
        "authors": [
            "Honglei Zhuang",
            "Joel Young"
        ],
        "references": [
            262160717,
            312893045,
            303783362,
            291249011,
            290276373,
            269087905,
            266797230,
            262392516,
            262242277,
            262236825
        ]
    },
    {
        "id": 339768441,
        "title": "Video Representation Learning by Dense Predictive Coding",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Tengda Han",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "references": [
            323931846,
            321325134,
            339561787,
            338512546,
            335302673,
            329745706,
            328143578,
            323412769,
            322058314,
            322057985
        ]
    },
    {
        "id": 329750748,
        "title": "Improvements to Context Based Self-Supervised Learning",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "T. Nathan Mundhenk",
            "Daniel Ho",
            "Barry Y. Chen"
        ],
        "references": [
            325002984,
            322975889,
            322060714,
            322058914,
            322058314,
            322057985,
            320968627,
            320964865,
            319770291,
            319770183
        ]
    },
    {
        "id": 316098571,
        "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning",
        "abstract": "We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the output distribution. Virtual adversarial loss is defined as the robustness of the model's posterior distribution against local perturbation around each input data point. Our method is similar to adversarial training, but differs from adversarial training in that it determines the adversarial direction based only on the output distribution and that it is applicable to a semi-supervised setting. Because the directions in which we smooth the model are virtually adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward and backpropagations. In our experiments, we applied VAT to supervised and semi-supervised learning on multiple benchmark datasets. With additional improvement based on entropy minimization principle, our VAT achieves the state-of-the-art performance on SVHN and CIFAR-10 for semi-supervised learning tasks.",
        "date": "April 2017",
        "authors": [
            "Takeru Miyato",
            "Shin-ichi Maeda",
            "Masanori Koyama",
            "Shin Ishii"
        ],
        "references": [
            305881127,
            303993074,
            301845925,
            319770414,
            319770387,
            319770355,
            308964680,
            286794765,
            286784019,
            284476459
        ]
    },
    {
        "id": 315747848,
        "title": "Self-Supervised Video Representation Learning With Odd-One-Out Networks",
        "abstract": "We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called odd-one-out learning. In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream con-volutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition. On action classification, our method obtains 60.3% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.",
        "date": "July 2017",
        "authors": [
            "Basura Fernando",
            "Hakan Bilen",
            "Efstratios Gavves",
            "Stephen Gould"
        ],
        "references": [
            315882843,
            311411639,
            310020983,
            305779667,
            305006910,
            311344466,
            309630116,
            309076254,
            308716424,
            304843249
        ]
    },
    {
        "id": 312492399,
        "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks",
        "abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",
        "date": "January 2017",
        "authors": [
            "Lars Mescheder",
            "Sebastian Nowozin",
            "Andreas Geiger"
        ],
        "references": [
            320971134,
            311299462,
            311299352,
            310235433,
            309551617,
            322715582,
            319770355,
            313058712,
            311668858,
            311514837
        ]
    },
    {
        "id": 308295512,
        "title": "Ambient Sound Provides Supervision for Visual Learning",
        "abstract": "The sound of crashing waves, the roar of fast-moving cars \u2013 sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds.",
        "date": "October 2016",
        "authors": [
            "Andrew Owens",
            "Jiajun Wu",
            "Josh H. McDermott",
            "William T. Freeman"
        ],
        "references": [
            288713304,
            284579529,
            319770399,
            319770183,
            313544906,
            311610098,
            308161116,
            304409339,
            300412509,
            284579516
        ]
    },
    {
        "id": 339555791,
        "title": "How Do Neural Networks See Depth in Single Images?",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Tom van Dijk",
            "Guido de Croon"
        ],
        "references": [
            339558556,
            329750745,
            324271713,
            321574879,
            320968448,
            320968191,
            320915709,
            319770337,
            314100361,
            310752533
        ]
    },
    {
        "id": 305881526,
        "title": "Matching Networks for One Shot Learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
        "date": "June 2016",
        "authors": [
            "Oriol Vinyals",
            "Charles Blundell",
            "Timothy Lillicrap",
            "Koray Kavukcuoglu"
        ],
        "references": [
            305196650,
            303367197,
            319770357,
            319770183,
            312607081,
            311469848,
            306281834,
            285764506,
            285648386,
            284476538
        ]
    },
    {
        "id": 279839496,
        "title": "Learning Deep Features for Scene Recognition using Places Database",
        "abstract": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",
        "date": "May 2015",
        "authors": [
            "Bolei Zhou",
            "\u00c0gata Lapedriza",
            "Jianxiong Xiao",
            "Antonio Torralba"
        ],
        "references": [
            319770352,
            264979485,
            261100864,
            257672261,
            319770276,
            319770183,
            286976996,
            267960550,
            261306213,
            257409887
        ]
    },
    {
        "id": 320920333,
        "title": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks",
        "abstract": "Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",
        "date": "November 2017",
        "authors": [
            "Zhao Chen",
            "Vijay Badrinarayanan",
            "Chen-Yu Lee",
            "Andrew Rabinovich"
        ],
        "references": [
            322749812,
            322059850,
            317040443,
            315454938,
            329747447,
            328955291,
            322582564,
            320964427,
            319770291,
            314100361
        ]
    },
    {
        "id": 328159324,
        "title": "Exploring the Limits of Weakly Supervised Pretraining: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II",
        "abstract": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards \u201csmall\u201d. Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",
        "date": "September 2018",
        "authors": [
            "Dhruv Mahajan",
            "Ross Girshick",
            "Vignesh Ramanathan",
            "Kaiming He"
        ],
        "references": [
            321325808,
            319770352,
            319770123,
            311969727,
            307473328,
            306885833,
            265295439,
            329747533,
            329745708,
            322060396
        ]
    },
    {
        "id": 325448855,
        "title": "Colorless Green Recurrent Networks Dream Hierarchically",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Kristina Gulordava",
            "Piotr Bojanowski",
            "Edouard Grave",
            "Tal Linzen"
        ],
        "references": [
            321347616,
            318741956,
            315748956,
            306187579,
            283666865,
            271944021,
            221489926,
            221486318,
            14795144,
            13853244
        ]
    },
    {
        "id": 322590138,
        "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
        "date": "September 2017",
        "authors": [
            "Alexis Conneau",
            "Douwe Kiela",
            "Holger Schwenk",
            "Lo\u00efc Barrault"
        ],
        "references": [
            311990865,
            305334586,
            304408934,
            301408978,
            284576917,
            279068396,
            266201822,
            262484733,
            261100864,
            257882504
        ]
    },
    {
        "id": 322582488,
        "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Bjarke Felbo",
            "Alan Mislove",
            "Anders S\u00f8gaard",
            "Iyad Rahwan"
        ],
        "references": [
            312250769,
            311990263,
            307512566,
            305767979,
            305342002,
            305334406,
            305334401,
            278654387,
            265382496,
            259261310
        ]
    },
    {
        "id": 319770381,
        "title": "How transferable are features in deep neural networks?",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
        "date": "January 2014",
        "authors": [
            "Jason Yosinski",
            "Jeff Clune",
            "Y. Bengio",
            "Hod Lipson"
        ],
        "references": []
    },
    {
        "id": 319770357,
        "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
        "date": "October 2013",
        "authors": [
            "Jeff Donahue",
            "Yangqing Jia",
            "Oriol Vinyals",
            "Judy Hoffman"
        ],
        "references": []
    },
    {
        "id": 319770231,
        "title": "Pointer Sentinel Mixture Models",
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
        "date": "April 2017",
        "authors": [
            "Stephen Merity",
            "Caiming Xiong",
            "James Bradbury",
            "Richard Socher"
        ],
        "references": []
    },
    {
        "id": 318849650,
        "title": "Learned in Translation: Contextualized Word Vectors",
        "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
        "date": "July 2017",
        "authors": [
            "Bryan McCann",
            "James Bradbury",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "references": [
            332551012,
            316591288,
            313481320,
            312216100,
            311458944,
            310329039,
            308361705,
            306093640,
            305334586,
            304018244
        ]
    },
    {
        "id": 332717934,
        "title": "A Simple Approximation to the Area Under Standard Normal Curve",
        "abstract": "Of all statistical distributions, the standard normal is perhaps the most popular and widely used. Its use often involves computing the area under its probability curve. Unlike many other statistical distributions, there is no closed form theoretical expression for this area in case of the normal distribution. Consequently it has to be approximated. While there are a number of highly complex but accurate algorithms, some simple ones have also been proposed in literature. Even though the simple ones may not be very accurate, they are nevertheless useful as accuracy has to be gauged vis-\u00e0-vis simplicity. In this short paper, we present another simple approximation formula to the cumulative distribution function of standard normal distribution. This new formula is fairly good when judged vis-\u00e0-vis its simplicity.",
        "date": "January 2014",
        "authors": [
            "Amit Choudhury"
        ],
        "references": [
            233010375,
            324350486,
            271760337,
            271760208,
            271686558,
            270365572,
            269024002,
            268299777,
            239652405,
            222675750
        ]
    },
    {
        "id": 319770320,
        "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
        "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-of-the-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.",
        "date": "April 2017",
        "authors": [
            "David Krueger",
            "Tegan Maharaj",
            "J\u00e1nos Kram\u00e1r",
            "Mohammad Pezeshki"
        ],
        "references": [
            309424668,
            301879329,
            301878902,
            301840001,
            300412439,
            319769994,
            307861335,
            303409493,
            286794765,
            286512696
        ]
    },
    {
        "id": 305119419,
        "title": "Generalizing and Improving Weight Initialization",
        "abstract": "We propose a new weight initialization suited for arbitrary nonlinearities by generalizing previous weight initializations. The initialization corrects for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Consequently, this initialization does not require computing mini-batch statistics nor weight pre-initialization. This simple method enables improved accuracy over previous initializations, and it allows for training highly regularized neural networks where previous initializations lead to poor convergence.",
        "date": "July 2016",
        "authors": [
            "Dan Hendrycks",
            "Kevin Gimpel"
        ],
        "references": [
            319770123,
            319769813,
            304506026,
            284476548,
            220873867,
            319770272,
            301872762,
            265748773
        ]
    },
    {
        "id": 303821573,
        "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
        "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-of-the-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.",
        "date": "June 2016",
        "authors": [
            "David Krueger",
            "Tegan Maharaj",
            "J\u00e1nos Kram\u00e1r",
            "Mohammad Pezeshki"
        ],
        "references": [
            301879329,
            301878902,
            301840001,
            300412439,
            312607081,
            311609041,
            303409493,
            287249598,
            287034172,
            286794765
        ]
    },
    {
        "id": 303409435,
        "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks",
        "abstract": "In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.",
        "date": "May 2016",
        "authors": [
            "Andreas Veit",
            "Michael Wilber",
            "Serge Belongie"
        ],
        "references": [
            305196650,
            301879329,
            268079628,
            319770414,
            310752533,
            286512696,
            285058764,
            272832134,
            272194743,
            267960550
        ]
    },
    {
        "id": 301879329,
        "title": "Deep Networks with Stochastic Depth",
        "abstract": "Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91%) on CIFAR-10.",
        "date": "March 2016",
        "authors": [
            "Gao Huang",
            "Yu Sun",
            "Zhuang Liu",
            "Daniel Sedra"
        ],
        "references": [
            305196650,
            319770411,
            319770264,
            319770183,
            308846031,
            306218037,
            291735245,
            286794765,
            286512696,
            286271944
        ]
    },
    {
        "id": 319770257,
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.",
        "date": "January 2014",
        "authors": [
            "Andrew M Saxe",
            "James L Mcclelland",
            "Surya Ganguli"
        ],
        "references": []
    },
    {
        "id": 286794765,
        "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
        "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. \u00a9 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.",
        "date": "June 2014",
        "authors": [
            "Nitish Srivastava",
            "Geoffrey Hinton",
            "Alex Krizhevsky",
            "Ilya Sutskever"
        ],
        "references": [
            281764031,
            246546737,
            235639035,
            227716504,
            221620570,
            221346269,
            220016377,
            41040261,
            221345874,
            216792879
        ]
    },
    {
        "id": 301404923,
        "title": "Machine Comprehension with Syntax, Frames, and Semantics",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Hai Wang",
            "Mohit Bansal",
            "Kevin Gimpel",
            "David McAllester"
        ],
        "references": [
            278641739,
            301405017,
            301404729,
            301404448,
            299487682,
            297123726,
            286965813,
            282162224,
            278656150,
            275504235
        ]
    },
    {
        "id": 319770348,
        "title": "Teaching Machines to Read and Comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
        "date": "June 2015",
        "authors": [
            "Karl Moritz Hermann",
            "Tom\u00e1vs Kovcisk\u00fd",
            "Edward Grefenstette",
            "Lasse Espeholt"
        ],
        "references": []
    },
    {
        "id": 318497709,
        "title": "My Computer Is an Honor Student \u2014 but How Intelligent Is It? Standardized Tests as a Measure of AI",
        "abstract": "Given the well-known limitations of the Turing Test, there is a need for objective tests to both focus attention on, and measure progress towards, the goals of AI. In this paper we argue that machine performance on standardized tests should be a key component of any new measure of AI, because attaining a high level of performance requires solving significant AI problems involving language understanding and world modeling - critical skills for any machine that lays claim to intelligence. In addition, standardized tests have all the basic requirements of a practical test: they are accessible, easily comprehensible, clearly measurable, and offer a graduated progression from simple tasks to those requiring deep understanding of the world. Here we propose this task as a challenge problem for the community, summarize our state-of-the-art results on math and science tests, and provide supporting datasets",
        "date": "April 2016",
        "authors": [
            "Peter Clark",
            "Oren Etzioni"
        ],
        "references": [
            256199468,
            220813820,
            220355269,
            1895883,
            329975850,
            318494451,
            301446025,
            301445855,
            301405087,
            281666782
        ]
    },
    {
        "id": 312607081,
        "title": "Building a large annotated corpus of english: The penn treebank",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "Mitchell Marcus",
            "B. Santorini",
            "M.A. Marcinkiewicz"
        ],
        "references": []
    },
    {
        "id": 309438804,
        "title": "N-gram IDF: A Global Term Weighting Scheme Based on Information Distance",
        "abstract": "This paper first reveals the relationship between Inverse Document Frequency (IDF), a global term weighting scheme, and information distance, a universal metric defined by Kolmogorov complexity. We concretely give a theoretical explanation that the IDF of a term is equal to the distance between the term and the empty string in the space of information distance in which the Kolmogorov complexity is approximated using Web documents and the Shannon-Fano coding. Based on our findings, we propose N-gram IDF, a theoretical extension of IDF for handling words and phrases of any length. By comparing weights among N-grams of any N, N-gram IDF enables us to determine dominant N-grams among overlapping ones and extract key terms of any length from texts without using any NLP techniques. To efficiently compute the weight for all possible N-grams, we adopt two string processing techniques, i.e., maximal substring extraction using enhanced suffix array and document listing using wavelet tree. We conducted experiments on key term extraction and Web search query segmentation, and found that N-gram IDF was competitive with state-of-the-art methods that were designed for each application using additional resources and efforts. The results exemplified the potential of N-gram IDF.",
        "date": "May 2015",
        "authors": [
            "Masumi Shirakawa",
            "Takahiro Hara",
            "Shojiro Nishio"
        ],
        "references": [
            319393405,
            313170906,
            242554375,
            238123710,
            228752754,
            221299409,
            221226686,
            221101542,
            221024134,
            220907261
        ]
    },
    {
        "id": 306094228,
        "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
        "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.",
        "date": "June 2016",
        "authors": [
            "Danqi Chen",
            "Jason Bolton",
            "Christopher D. Manning"
        ],
        "references": [
            305342143,
            301404923,
            284576917,
            284476550,
            283659163,
            279310212,
            220479903,
            319769995,
            306093209,
            301404907
        ]
    },
    {
        "id": 301445887,
        "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Yi Yang",
            "Wen-tau Yih",
            "Christopher Meek"
        ],
        "references": [
            269117118,
            257882504,
            256708662,
            221012718,
            2888359,
            319770369,
            289964736,
            270877716,
            262416109,
            241185808
        ]
    },
    {
        "id": 301405087,
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Mohammad Javad Hosseini",
            "Hannaneh Hajishirzi",
            "Oren Etzioni",
            "Nate Kushman"
        ],
        "references": [
            265073815,
            230876730,
            221605406,
            221405119,
            221346130,
            221012726,
            220875019,
            215470704,
            200044364,
            2488725
        ]
    },
    {
        "id": 301404907,
        "title": "Modeling Biological Processes for Reading Comprehension",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Jonathan Berant",
            "Vivek Srikumar",
            "Pei-Chun Chen",
            "Abby Vander Linden"
        ],
        "references": [
            272091377,
            262310014,
            242582900,
            234792292,
            228663165,
            224890460,
            224890459,
            221605724,
            221013034,
            220873694
        ]
    },
    {
        "id": 301404729,
        "title": "Learning Answer-Entailing Structures for Machine Comprehension",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Mrinmaya Sachan",
            "Avinava Dubey",
            "Eric Xing",
            "Matthew Richardson"
        ],
        "references": [
            313621629,
            274078591,
            270878554,
            267839565,
            266201822,
            221654624,
            221653970,
            221346169,
            221345875,
            221037807
        ]
    },
    {
        "id": 314818211,
        "title": "Quantitative evaluation of passage retrieval algorithms for question answering",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "Stefanie Tellex",
            "Boris Katz",
            "Jimmy Lin",
            "Aaron Fernandes"
        ],
        "references": []
    },
    {
        "id": 301408902,
        "title": "Linguistic Regularities in Sparse and Explicit Word Representations",
        "abstract": "Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.\u2019s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.",
        "date": "January 2014",
        "authors": [
            "Omer Levy",
            "Yoav Goldberg"
        ],
        "references": [
            267433170,
            266201822,
            258082321,
            257882504,
            234827624,
            234131319,
            228520439,
            224246503,
            221112660,
            220873681
        ]
    },
    {
        "id": 289176734,
        "title": "Word Embeddings through Hellinger PCA",
        "abstract": "Word embeddings resulting from neural language models have been shown to be a great asset for a large variety of NLP tasks. However, such architecture might be difficult and time-consuming to train. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.",
        "date": "January 2014",
        "authors": [
            "R\u00e9mi Lebret",
            "Ronan Collobert"
        ],
        "references": [
            266439415,
            266201822,
            230786023,
            221618573,
            221345058,
            220873867,
            220873681,
            220355244,
            45904528,
            3296950
        ]
    },
    {
        "id": 285896121,
        "title": "Learning word embeddings efficiently with noise-contrastive estimation",
        "abstract": "Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.",
        "date": "January 2013",
        "authors": [
            "A. Mnih",
            "K. Kavukcuoglu"
        ],
        "references": [
            222099363,
            5462889,
            228095628
        ]
    },
    {
        "id": 285895924,
        "title": "Linguistic regularities in continuous space word representations",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "T. Mikolov",
            "W.-T. Yih",
            "G. Zweig"
        ],
        "references": [
            228520439
        ]
    },
    {
        "id": 270878536,
        "title": "Better Word Representations with Recursive Neural Networks for Morphology",
        "abstract": "",
        "date": "August 2013",
        "authors": [
            "L\u01b0\u01a1ng Chi\u1ebfn Th\u1eafng",
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "references": []
    },
    {
        "id": 270877599,
        "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
        "abstract": "Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.",
        "date": "June 2014",
        "authors": [
            "Marco Baroni",
            "Georgiana Dinu",
            "Germ\u00e1n Kruszewski"
        ],
        "references": [
            281458937,
            267433170,
            266201822,
            257882504,
            254464318,
            234826128,
            234131319,
            234131208,
            228548010,
            221760580
        ]
    },
    {
        "id": 267709055,
        "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
        "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmen-tation of neuronal structures depicted in stacks of electron microscopy (EM) im-ages. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succes-sion of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.",
        "date": "January 2012",
        "authors": [
            "Dan C Cirean",
            "Alessandro Giusti",
            "Luca Maria Gambardella",
            "Schmidhuber"
        ],
        "references": [
            252064636,
            224579199,
            224135949,
            221392298,
            221363296,
            221362771,
            221080312,
            220860992,
            220812758,
            51715241
        ]
    },
    {
        "id": 329975395,
        "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences",
        "abstract": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.",
        "date": "December 2014",
        "authors": [
            "Richard Socher",
            "Andrej Karpathy",
            "Quoc V. Le",
            "Christopher D. Manning"
        ],
        "references": [
            266225209,
            255482849,
            235221286,
            234131208,
            228452494,
            224135964,
            221361415,
            221345755,
            221345149,
            221303985
        ]
    },
    {
        "id": 319770465,
        "title": "Sequence to Sequence Learning with Neural Networks",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.7 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a strong phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which beats the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "date": "September 2014",
        "authors": [
            "Ilya Sutskever",
            "Oriol Vinyals",
            "Quoc V Le"
        ],
        "references": []
    },
    {
        "id": 319770249,
        "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
        "abstract": "We present a model that generates free-form natural language descriptions of image regions. Our model leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between text and visual data. Our approach is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate the effectiveness of our alignment model with ranking experiments on Flickr8K, Flickr30K and COCO datasets, where we substantially improve on the state of the art. We then show that the sentences created by our generative model outperform retrieval baselines on the three aforementioned datasets and a new dataset of region-level annotations.",
        "date": "January 2014",
        "authors": [
            "Andrej Karpathy",
            "Li Fei-Fei"
        ],
        "references": []
    },
    {
        "id": 308837339,
        "title": "Associating neural word embeddings with deep image representations using Fisher Vectors",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Benjamin Klein",
            "Guy Lev",
            "Gil Sadeh",
            "Lior Wolf"
        ],
        "references": [
            319770160,
            308034527,
            307747289,
            257882504,
            257672261,
            234131319,
            228102719,
            221303952,
            221303896,
            2985446
        ]
    },
    {
        "id": 301409028,
        "title": "ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment",
        "abstract": "This paper presents our approach to semantic relatedness and textual entailment subtasks organized as task 1 in SemEval 2014. Specifically, we address two questions: (1) Can we solve these two subtasks together? (2) Are features proposed for textual entailment task still effective for semantic relatedness task? To address them, we extracted seven types of features including text difference measures proposed in entailment judgement subtask, as well as common text similarity measures used in both subtasks. Then we exploited the same feature set to solve the both subtasks by considering them as a regression and a classification task respectively and performed a study of influence of different features. We achieved the first and the second rank for relatedness and entailment task respectively.",
        "date": "January 2014",
        "authors": [
            "Jiang Zhao",
            "Tiantian Zhu",
            "Man Lan"
        ],
        "references": [
            305386496,
            264003485,
            259181798,
            234801088,
            234075845,
            232645326,
            230600388,
            221101423,
            220817260,
            51917095
        ]
    },
    {
        "id": 301408986,
        "title": "Illinois-LH: A Denotational and Distributional Approach to Semantics",
        "abstract": "This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations.",
        "date": "January 2014",
        "authors": [
            "Alice Lai",
            "Julia Hockenmaier"
        ],
        "references": [
            264003485,
            262484733,
            221900777,
            221012753,
            200179363,
            51917095,
            303721259,
            262367926,
            220017648,
            51119064
        ]
    },
    {
        "id": 283601334,
        "title": "Deep learning of representations for unsupervised and transfer learning",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "Y. Bengio"
        ],
        "references": []
    },
    {
        "id": 268265707,
        "title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning",
        "abstract": "Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an or-thonoramlity constraint to be enforced, which makes it difficult to learn overcom-plete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connec-tions between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets.",
        "date": "July 2015",
        "authors": [
            "Quoc V Le",
            "Alexandre Karpenko",
            "Jiquan Ngiam",
            "Andrew Y Ng"
        ],
        "references": [
            303137904,
            224716259,
            224579268,
            221620343,
            221620258,
            221364080,
            221346269,
            221304995,
            220499855,
            220321031
        ]
    },
    {
        "id": 258424423,
        "title": "Visualizing and Understanding Convolutional Neural Networks",
        "abstract": "Large Convolutional Neural Network models have recently demonstrated impressive classification performance on the ImageNet benchmark \\cite{Kriz12}. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
        "date": "November 2013",
        "authors": [
            "Matthew D Zeiler",
            "Rob Fergus"
        ],
        "references": [
            303137904,
            265022827,
            259367619,
            228102719,
            221364080,
            221361415,
            221346269,
            221138890,
            221110814,
            200744514
        ]
    },
    {
        "id": 258374356,
        "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "abstract": "Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40% (achieving a final mAP of 48% on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.",
        "date": "November 2013",
        "authors": [
            "Ross Girshick",
            "Jeff Donahue",
            "Trevor Darrell",
            "Jitendra Malik"
        ],
        "references": [
            307562031,
            289917319,
            267686145,
            265295439,
            264979485,
            262270555,
            261493845,
            260350533,
            240308781,
            240308775
        ]
    },
    {
        "id": 241637478,
        "title": "Strategies for training large scale neural network language models",
        "abstract": "We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the training data are sorted by their relevance. We introduce hash-based implementation of a maximum entropy model, that can be trained as a part of the neural network model. This leads to significant reduction of computational complexity. We achieved around 10% relative reduction of word error rate on English Broadcast News speech recognition task, against large 4-gram model trained on 400M tokens.",
        "date": "December 2011",
        "authors": [
            "Tomas Mikolov",
            "Anoop Deoras",
            "Daniel Povey",
            "Lukas Burget"
        ],
        "references": [
            228619939,
            228348202,
            224246503,
            224246437,
            221618573,
            311469848,
            242821088,
            228726495,
            224096978,
            222650733
        ]
    },
    {
        "id": 228348202,
        "title": "Hierarchical probabilistic neural network language model",
        "abstract": "In recent years, variants of a neural network ar-chitecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech rec-ognizers. The main advantage of these architec-tures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and pro-vide good generalization even when the num-ber of training examples is insufficient. How-ever, these models are extremely slow in com-parison to the more commonly used n-gram mod-els, both for training and recognition. As an al-ternative to an importance sampling method pro-posed to speed-up training, we introduce a hier-archical decomposition of the conditional proba-bilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical cluster-ing constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",
        "date": "January 2005",
        "authors": [
            "Frederic Morin",
            "Y. Bengio"
        ],
        "references": [
            304109482,
            303802749,
            301870716,
            269953350,
            230876151,
            225818196,
            223117716,
            222835593,
            222452871,
            222449846
        ]
    },
    {
        "id": 224246503,
        "title": "Extensions of recurrent neural network language model",
        "abstract": "We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.",
        "date": "June 2011",
        "authors": [
            "Tomas Mikolov",
            "Stefan Kombrink",
            "Lukas Burget",
            "J.H. Cernocky"
        ],
        "references": [
            228348202,
            221618573,
            221489926,
            221013094,
            220817490,
            220816701,
            239059398,
            229091480,
            222835593,
            222449846
        ]
    },
    {
        "id": 307174896,
        "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
        "abstract": "",
        "date": "January 1996",
        "authors": [
            "Stanley F Chen",
            "Joshua Goodman"
        ],
        "references": []
    },
    {
        "id": 288345724,
        "title": "Class-based n-gram models of natural language",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "P. Brown",
            "P. Della"
        ],
        "references": []
    },
    {
        "id": 270878020,
        "title": "Factored Language Model based on Recurrent Neural Network",
        "abstract": "Among various neural network language models (NNLMs), recurrent neural network-based language models (RNNLMs) are very competitive in many cases. Most current RNNLMs only use one single feature stream, i.e., surface words. However, previous studies proved that language models with additional linguistic information achieve better performance. In this study, we extend RNNLM by explicitly integrating additional linguistic information, including morphological, syntactic, or semantic factors. Our proposed RNNLM is called a factored RNNLM that is expected to enhance RNNLMs. A number of experiments are carried out that show the factored RNNLM improves the performance for all considered tasks: consistent perplexity and word error rate (WER) reductions. In the Penn Treebank corpus, the relative improvements over n-gram LM and RNNLM are 29.0% and 13.0%, respectively. In the IWSLT-2011 TED ASR test set, absolute WER reductions over RNNLM and n-gram LM reach 0.63 and 0.73 points.",
        "date": "December 2012",
        "authors": [
            "Youzheng Wu",
            "Xugang lu",
            "Hitoshi Yamamoto",
            "Shigeki Matsuda"
        ],
        "references": [
            281555625,
            262272503,
            239765773,
            228828379,
            228567121,
            224246503,
            224097026,
            221489926,
            221013253,
            221013220
        ]
    },
    {
        "id": 266458936,
        "title": "Statistical Language Models Based on Neural Networks",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "To A\u0161 Mikolov"
        ],
        "references": []
    },
    {
        "id": 261167314,
        "title": "Speed regularization and optimality in word classing",
        "abstract": "Word-classing has been used in language modeling for two distinct purposes: to improve the likelihood of the language model, and to improve the runtime speed. In particular, frequency-based heuristics have been proposed to improve the speed of recurrent neural network language models (RNN-LMs). In this paper, we present a dynamic programming algorithm for determining classes in a way that provably minimizes the runtime of the resulting class-based language models. However, we also find that the speed-based methods degrade the perplexity of the language models by 5-10% over traditional likelihood-based classing. We remedy this via the introduction of a speed-based regularization term in the likelihood objective function. This achieves a runtime close to that of the speed based methods without loss in perplexity performance. We demonstrate these improvements with both an RNN-LM and the Model M exponential language model, for three different tasks involving two different languages.",
        "date": "October 2013",
        "authors": [
            "Geoffrey Zweig",
            "Konstantin Makarychev"
        ],
        "references": [
            255564151,
            241637478,
            228348202,
            224246503,
            221618573,
            221491437,
            221489926,
            221481293,
            220733157,
            220355244
        ]
    },
    {
        "id": 242821088,
        "title": "A Maximum Entropy approach to adaptive statistical language modeling",
        "abstract": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework.",
        "date": "August 1996",
        "authors": [
            "Ronald Rosenfeld"
        ],
        "references": [
            313170906,
            234819841,
            234787143,
            230876734,
            224377724,
            220817158,
            220816887,
            3532169,
            3191800,
            2596889
        ]
    },
    {
        "id": 222800591,
        "title": "Structured language modeling",
        "abstract": "This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate\u2014word lattice rescoring\u2014over the standard 3-gram language model.",
        "date": "October 2000",
        "authors": [
            "Ciprian Chelba",
            "Frederick Jelinek"
        ],
        "references": [
            232622134,
            220874529,
            220816736,
            220355244,
            220017637,
            2827805,
            2572499,
            2480097,
            2448755,
            2299394
        ]
    },
    {
        "id": 258082321,
        "title": "Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase",
        "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).",
        "date": "October 2013",
        "authors": [
            "Peter David Turney"
        ],
        "references": [
            313768163,
            269403596,
            267433170,
            262238209,
            308797129,
            285895924,
            262566601,
            262367926,
            262334182,
            262284913
        ]
    },
    {
        "id": 234131319,
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
        "date": "January 2013",
        "authors": [
            "Tomas Mikolov",
            "Kai Chen",
            "G.s. Corrado",
            "Jeffrey Dean"
        ],
        "references": [
            266201822,
            257882504,
            241637478,
            298666972,
            284685610,
            266458936,
            262201777,
            255564430,
            239059398,
            230876626
        ]
    },
    {
        "id": 262367926,
        "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
        "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
        "date": "July 2012",
        "authors": [
            "Richard Socher",
            "Brody Huval",
            "Christopher D. Manning",
            "Andrew Y. Ng"
        ],
        "references": [
            234828967,
            228881189,
            228569700,
            228452494,
            221345755,
            221101423,
            221013013,
            221012753,
            220874934,
            220817516
        ]
    },
    {
        "id": 229091480,
        "title": "Learning Representations by Back Propagating Errors",
        "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.",
        "date": "October 1986",
        "authors": [
            "David E. Rumelhart",
            "Geoffrey E. Hinton",
            "Ronald J. Williams"
        ],
        "references": [
            345402887,
            275807826,
            237044580,
            233784963
        ]
    },
    {
        "id": 228095628,
        "title": "A Fast and Simple Algorithm for Training Neural Probabilistic Language Models",
        "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.",
        "date": "June 2012",
        "authors": [
            "Andriy Mnih",
            "Yee Whye Teh"
        ],
        "references": [
            228348202,
            222099363,
            221707483,
            221618573,
            221489926,
            221345755,
            220873681,
            220817490,
            220691633,
            220320709
        ]
    },
    {
        "id": 225818196,
        "title": "Neural Probabilistic Language Models",
        "abstract": "A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.",
        "date": "May 2006",
        "authors": [
            "Y. Bengio",
            "R\u00e9jean Ducharme",
            "Pascal Vincent",
            "Christian Jauvin"
        ],
        "references": [
            244436420,
            228739036,
            221618573,
            221532132,
            221492270,
            221491569,
            221489509,
            221489178,
            221481293,
            220355244
        ]
    },
    {
        "id": 277292831,
        "title": "The 2005 pascal visual object classes challenge",
        "abstract": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006). Details of the challenge, data, and evaluation are presented. Participants in the challenge submitted descriptions of their methods, and these have been included verbatim. This document should be considered preliminary, and subject to change.",
        "date": "January 2006",
        "authors": [
            "Mark Everingham",
            "Chris Williams",
            "Andrew Zisserman",
            "Luc Van Gool"
        ],
        "references": [
            228980010,
            228602850,
            225496431,
            316240097,
            313527089,
            313522833,
            308161149,
            248488976,
            242100776,
            228715647
        ]
    },
    {
        "id": 313527089,
        "title": "Distinctive image features from scale-invariant key points",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "D. Lowe"
        ],
        "references": []
    },
    {
        "id": 312457795,
        "title": "Semantic hierarchies for visual object recognition",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "M. Marszalek",
            "C. Schmid"
        ],
        "references": []
    },
    {
        "id": 269953350,
        "title": "WordNet: An Electronic Lexical Database",
        "abstract": "The abstract for this document is available on CSA Illumina.To view the Abstract, click the Abstract button above the document title.",
        "date": "September 2000",
        "authors": [
            "Adam Kilgarriff",
            "Christiane Fellbaum"
        ],
        "references": []
    },
    {
        "id": 242442798,
        "title": "Principles of Categorization",
        "abstract": "",
        "date": "January 1978",
        "authors": [
            "Eleanor Rosch"
        ],
        "references": [
            285469376
        ]
    },
    {
        "id": 233710354,
        "title": "The FERET database and evaluation procedure for face-recognition algorithms",
        "abstract": "The Face Recognition Technology (FERET) program database is a large database of facial images, divided into development and sequestered portions. The development portion is made available to researchers, and the sequestered portion is reserved for testing facerecognition algorithms. The FERET evaluation procedure is an independently administered test of face-recognition algorithms. The test was designed to: (1) allow a direct comparison between different algorithms, (2) identify the most promising approaches, (3) assess the state of the art in face recognition, (4) identify future directions of research, and (5) advance the state of the art in face recognition.",
        "date": "April 1998",
        "authors": [
            "Rallings C",
            "Thrasher M",
            "Gunter C",
            "P. Jonathon Phillips"
        ],
        "references": [
            228834755,
            220182212,
            28763103,
            3684468,
            3637747,
            3043085,
            2782316,
            258515867,
            256075302,
            252310255
        ]
    },
    {
        "id": 232630154,
        "title": "Learning Object Categories from Google?s Image Search",
        "abstract": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets.",
        "date": "October 2005",
        "authors": [
            "R. Fergus",
            "L. Fei-Fei",
            "P. Perona",
            "A. Zisserman"
        ],
        "references": [
            228602850,
            221620547,
            220660282,
            4082304,
            4038433,
            3940582,
            3906069,
            2941307,
            2832818,
            308161149
        ]
    },
    {
        "id": 230854795,
        "title": "WordNet \u2013 An Electronical Lexical Database",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "Christiane Fellbaum"
        ],
        "references": []
    },
    {
        "id": 221620372,
        "title": "A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels.",
        "abstract": "Biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy, low-fidelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and two-dimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of high-dimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets.",
        "date": "January 2006",
        "authors": [
            "Eizaburo Doi",
            "Doru Balcan",
            "Michael S. Lewicki"
        ],
        "references": [
            221617821,
            8655422,
            3302668,
            220499975,
            12774896
        ]
    },
    {
        "id": 221617957,
        "title": "A Theory of Retinal Population Coding.",
        "abstract": "Efficient coding models predict that the optimal code for natural images is a population of oriented Gabor receptive fields. These results match response properties of neurons in primary visual cortex, but not those in the retina. Does the retina use an optimal code, and if so, what is it optimized for? Previous theories of retinal coding have assumed that the goal is to encode the maximal amount of information about the sensory signal. However, the image sampled by retinal photoreceptors is degraded both by the optics of the eye and by the photoreceptor noise. Therefore, de-blurring and de-noising of the retinal signal should be important aspects of retinal coding. Furthermore, the ideal retinal code should be robust to neural noise and make optimal use of all available neurons. Here we present a theoretical framework to derive codes that simultaneously satisfy all of these desiderata. When optimized for natural images, the model yields filters that show strong similarities to retinal ganglion cell (RGC) receptive fields. Importantly, the characteristics of receptive fields vary with retinal eccentricities where the optical blur and the number of RGCs are significantly different. The proposed model provides a unified account of retinal coding, and more generally, it may be viewed as an extension of the Wiener filter with an arbitrary number of noisy units.",
        "date": "January 2007",
        "authors": [
            "Eizaburo Doi",
            "Michael S. Lewicki"
        ],
        "references": [
            221620372,
            221617821,
            20925549,
            313517478,
            284144629,
            246774921,
            243763408,
            232075035,
            230675458,
            228083862
        ]
    },
    {
        "id": 221364240,
        "title": "Fields of Experts: A Framework for Learning Image Priors.",
        "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.",
        "date": "January 2005",
        "authors": [
            "Stefan Roth",
            "Michael J Black"
        ],
        "references": [
            220660439,
            304109482,
            283360188,
            281309188,
            271512969,
            256424823,
            247131283,
            239582331,
            221619891,
            220182914
        ]
    },
    {
        "id": 312451443,
        "title": "Efficient learning of sparse representations with an energy-based model",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "C.S. Poultney",
            "S. Chopra",
            "Y.L. Cun"
        ],
        "references": []
    },
    {
        "id": 292215395,
        "title": "Neural networks and physical systems with emergent collective computational abilities",
        "abstract": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "date": "April 1982",
        "authors": [
            "John J Hopfield"
        ],
        "references": []
    },
    {
        "id": 239566324,
        "title": "Non-linear latent factor models for revealing structure in high-dimensional data",
        "abstract": "Real world data is not random: The variability in the data-sets that arise in computer vision, signal processing and other areas is often highly constrained and governed by a number of degrees of freedom that is much smaller than the superficial dimensionality of the data. Unsupervised learning methods can be used to automatically discover the \"true\", underlying structure in such data-sets and are therefore a central component in many systems that deal with high-dimensional data. In this thesis we develop several new approaches to modeling the low-dimensional structure in data. We introduce a new non-parametric framework for latent variable modelling, that in contrast to previous methods generalizes learned embeddings beyond the training data and its latent representatives. We show that the computational complexity for learning and applying the model is much smaller than that of existing methods, and we illustrate its applicability on several problems. We also show how we can introduce supervision signals into latent variable models using conditioning. Supervision signals make it possible to attach \"meaning\" to the axes of a latent representation and to untangle the factors that contribute to the variability in the data. We develop a model that uses conditional latent variables to extract rich distributed represen- tations of image transformations, and we describe a new model for learning transformation features in structured supervised learning problems.",
        "date": "January 2007",
        "authors": [
            "Roland Memisevic"
        ],
        "references": [
            252457602,
            224881745,
            221619544,
            221364240,
            221111597,
            220660459,
            220500224,
            220319920,
            216792753,
            44649467
        ]
    },
    {
        "id": 237044580,
        "title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "Rumelhart DE",
            "James L Mcclelland"
        ],
        "references": []
    },
    {
        "id": 230876626,
        "title": "Parallel Distributed Processing: Explorations in the Microstructures of Cognition",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "David E. Rumelhart",
            "James L Mcclelland",
            "The PDP Research Group"
        ],
        "references": [
            6026283,
            263917978,
            248457484,
            230875900,
            222630383,
            49552016
        ]
    },
    {
        "id": 222438607,
        "title": "Connectionist Learning Procedures",
        "abstract": "A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.",
        "date": "September 1989",
        "authors": [
            "Geoffrey E. Hinton"
        ],
        "references": [
            279233597,
            260869405,
            242609671,
            242509302,
            241560014,
            345793666,
            324110517,
            313161692,
            304541496,
            292740784
        ]
    },
    {
        "id": 334843952,
        "title": "Interpolation Consistency Training for Semi-supervised Learning",
        "abstract": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark dataset.",
        "date": "August 2019",
        "authors": [
            "Vikas Verma",
            "Alex Lamb",
            "Juho Kannala",
            "Y. Bengio"
        ],
        "references": []
    },
    {
        "id": 332069196,
        "title": "Born Again Neural Networks",
        "abstract": "Knowledge Distillation (KD) consists of transferring \u201cknowledge\u201d from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student\u2019s compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.",
        "date": "July 2018",
        "authors": [
            "Tommaso Furlanello",
            "Zachary Chase Lipton",
            "Michael Tschannen",
            "Laurent Itti"
        ],
        "references": [
            321325065,
            320968593,
            320966701,
            319135616,
            318740426,
            317615661,
            317194083,
            317101000,
            317061748,
            312607081
        ]
    },
    {
        "id": 318392350,
        "title": "Adversarial Dropout for Supervised and Semi-supervised Learning",
        "abstract": "Recently, the training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has been proved to improve generalization performance of neural networks. In contrast to the individually biased inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between the outputs from the network with the dropouts and the training supervisions. The identified adversarial dropout are used to reconfigure the neural network to train, and we demonstrated that training on the reconfigured sub-network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST and CIFAR-10. We analyzed the trained model to reason the performance improvement, and we found that adversarial dropout increases the sparsity of neural networks more than the standard dropout does.",
        "date": "July 2017",
        "authors": [
            "Sungrae Park",
            "Jun-Keon Park",
            "Su-Jin Shin",
            "Il Chul Moon"
        ],
        "references": [
            316098571,
            306093553,
            305881127,
            305186613,
            303993074,
            303521296,
            313763061,
            308964680,
            308277201,
            306218037
        ]
    },
    {
        "id": 313481233,
        "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
        "abstract": "We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
        "date": "February 2017",
        "authors": [
            "Zhilin Yang",
            "Junjie Hu",
            "Ruslan Salakhutdinov",
            "William W. Cohen"
        ],
        "references": [
            312283342,
            311106875,
            309738279,
            327455026,
            319770355,
            311612456,
            311067032,
            309738677,
            309738384,
            309606886
        ]
    },
    {
        "id": 308278012,
        "title": "Deep Networks with Stochastic Depth",
        "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10).",
        "date": "October 2016",
        "authors": [
            "Gao Huang",
            "Yu Sun",
            "Zhuang Liu",
            "Daniel Sedra"
        ],
        "references": [
            319770414,
            319770411,
            319770364,
            319770264,
            319770183,
            311610233,
            311609041,
            308846031,
            308277201,
            306218037
        ]
    },
    {
        "id": 305881127,
        "title": "Improved Techniques for Training GANs",
        "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
        "date": "June 2016",
        "authors": [
            "Tim Salimans",
            "Ian Goodfellow",
            "Wojciech Zaremba",
            "Vicki Cheung"
        ],
        "references": [
            301845925,
            301839500,
            284476470,
            319770102,
            306281834,
            302384745,
            301878493,
            301872762,
            285648386,
            284476553
        ]
    },
    {
        "id": 303698513,
        "title": "Dynamic Filter Networks",
        "abstract": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.",
        "date": "January 2016",
        "authors": [
            "Bert De Brabandere",
            "Xu Jia",
            "Tinne Tuytelaars",
            "Luc Van Gool"
        ],
        "references": [
            304409658,
            289588029,
            288889700,
            319770414,
            319770355,
            319770234,
            308812843,
            308278233,
            301876917,
            286512696
        ]
    },
    {
        "id": 283659096,
        "title": "Stacked Attention Networks for Image Question Answering",
        "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
        "date": "November 2015",
        "authors": [
            "Zichao Yang",
            "Xiaodong He",
            "Jianfeng Gao",
            "li Deng"
        ],
        "references": [
            319770160,
            307747289,
            319770465,
            319770353,
            319770345,
            319770291,
            319770249,
            319770183,
            309076254,
            308871635
        ]
    },
    {
        "id": 311990858,
        "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Yequan Wang",
            "Minlie Huang",
            "Xiaoyan Zhu",
            "Li Zhao"
        ],
        "references": [
            311990784,
            305334469,
            301404452,
            284576917,
            329978116,
            301446024,
            287249782,
            285648032,
            284039049,
            281487270
        ]
    },
    {
        "id": 343299346,
        "title": "Learning to Deceive with Attention-Based Explanations",
        "abstract": "",
        "date": "January 2020",
        "authors": [
            "Danish Pruthi",
            "Mansi Gupta",
            "Bhuwan Dhingra",
            "Graham Neubig"
        ],
        "references": [
            306376567,
            334115433,
            317558625,
            302879092,
            277959017,
            7647316
        ]
    },
    {
        "id": 336999161,
        "title": "Attention is not not Explanation",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Sarah Wiegreffe",
            "Yuval Pinter"
        ],
        "references": [
            331214506,
            303499206,
            273786930,
            265252627,
            220873867,
            13853244,
            335780510,
            334601988,
            330933090,
            325448981
        ]
    },
    {
        "id": 335780510,
        "title": "Is Attention Interpretable?",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Sofia Serrano",
            "Noah A. Smith"
        ],
        "references": []
    },
    {
        "id": 335778955,
        "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Kevin Clark",
            "Urvashi Khandelwal",
            "Omer Levy",
            "Christopher D. Manning"
        ],
        "references": [
            321069982,
            318741956,
            306187579,
            284576917,
            265252627,
            220017637,
            335781053,
            334600955,
            334118517,
            334117413
        ]
    },
    {
        "id": 334116956,
        "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Alex Wang",
            "Amanpreet Singh",
            "Julian Michael",
            "Felix Hill"
        ],
        "references": [
            318084717,
            279068396,
            221366753,
            325449053,
            325447291,
            320371042,
            316779617,
            312250707,
            247999003,
            221675867
        ]
    },
    {
        "id": 322583071,
        "title": "Interactive Visualization and Manipulation of Attention-based Neural Machine Translation",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Jaesong Lee",
            "Joong-Hwi Shin",
            "Jun-Seok Kim"
        ],
        "references": [
            313434311,
            304470296,
            287250272,
            228976696,
            311925796,
            309738548,
            308646556,
            306093962,
            291437483,
            277959376
        ]
    },
    {
        "id": 221110516,
        "title": "Objects in Context",
        "abstract": "In the task of visual object categorization, semantic con- text can play the very important role of reducing ambigu- ity in objects' visual appearance. In this work we propose to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model. Us- ing a conditional random field (CRF) framework, our ap- proach maximizes object label agreement according to con- textual relevance. We compare two sources of context: one learned from training data and another queried from Google Sets. The overall performance of the proposed framework is evaluated on the PASCAL and MSRC datasets. Our findings conclude that incorporating context into object categorization greatly improves categorization accuracy.",
        "date": "January 2007",
        "authors": [
            "Andrew Rabinovich",
            "Andrea Vedaldi",
            "Carolina Galleguillos",
            "Eric Wiewiora"
        ],
        "references": [
            225496431,
            224377747,
            319770829,
            279178907,
            268209561,
            244954854,
            242592369,
            239443282,
            230854795,
            228757213
        ]
    },
    {
        "id": 2985446,
        "title": "Gradient-Based Learning Applied to Document Recognition",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day",
        "date": "December 1998",
        "authors": [
            "Yann Lecun",
            "Leon Bottou",
            "Y. Bengio",
            "Patrick Haffner"
        ],
        "references": [
            270283023,
            313173689,
            309630116,
            285058764,
            279351140,
            272832134,
            268645196,
            261001961,
            256651264,
            256622736
        ]
    },
    {
        "id": 338511824,
        "title": "AutoAugment: Learning Augmentation Strategies From Data",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Ekin D. Cubuk",
            "Barret Zoph",
            "Dandelion Mane",
            "Vijay Vasudevan"
        ],
        "references": [
            325808679,
            320726697,
            319151271,
            316098571,
            315655760,
            309738510,
            309730015,
            306187421,
            305196650,
            303993074
        ]
    },
    {
        "id": 338510249,
        "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Mingxing Tan",
            "Bo Chen",
            "Ruoming Pang",
            "Vijay Vasudevan"
        ],
        "references": [
            329747614,
            328110752,
            324435700,
            316184205,
            309738510,
            301878495,
            286513835,
            265295439,
            329745708,
            329745162
        ]
    },
    {
        "id": 323411512,
        "title": "A DIRT-T Approach to Unsupervised Domain Adaptation",
        "abstract": "Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.",
        "date": "February 2018",
        "authors": [
            "Rui Shu",
            "Hung H. Bui",
            "Hirokazu Narui",
            "Stefano Ermon"
        ],
        "references": [
            320967151,
            317650088,
            316098571,
            314092286,
            320609139,
            319770355,
            317230288,
            314260711,
            308964680,
            308278101
        ]
    },
    {
        "id": 308716468,
        "title": "Variational Autoencoder for Deep Learning of Images, Labels and Captions",
        "abstract": "A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.",
        "date": "September 2016",
        "authors": [
            "Yunchen Pu",
            "Zhe Gan",
            "Ricardo Henao",
            "Xin Yuan"
        ],
        "references": [
            319770160,
            307747289,
            305196650,
            319770465,
            319770183,
            311610253,
            308804607,
            306218037,
            303721259,
            303027922
        ]
    },
    {
        "id": 280581078,
        "title": "Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks",
        "abstract": "We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With De-noising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.",
        "date": "July 2013",
        "authors": [
            "Dong-Hyun Lee"
        ],
        "references": [
            240308775,
            228467601,
            228339739,
            228102719,
            221346359,
            221346269,
            220320806,
            239724173,
            221346162,
            221345031
        ]
    },
    {
        "id": 279968088,
        "title": "Semi-Supervised Learning with Ladder Network",
        "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.",
        "date": "July 2015",
        "authors": [
            "Antti Rasmus",
            "Harri Valpola",
            "Mikko Honkala",
            "Mathias Berglund"
        ],
        "references": [
            280581078,
            279632719,
            277603270,
            319770378,
            303256841,
            295504538,
            286794765,
            286620929,
            285635838,
            277958889
        ]
    },
    {
        "id": 319770272,
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
        "date": "February 2015",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "references": []
    },
    {
        "id": 319770245,
        "title": "Order-Embeddings of Images and Language",
        "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "date": "November 2016",
        "authors": [
            "Ivan Vendrov",
            "Ryan Kiros",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "references": []
    },
    {
        "id": 312416396,
        "title": "Long Short-Term Memory-Networks for Machine Reading",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Jianpeng Cheng",
            "Li Dong",
            "Mirella Lapata"
        ],
        "references": [
            306094026,
            284576917,
            243781690,
            228452494,
            221489926,
            221366753,
            220874243,
            51140976,
            15421059,
            13853244
        ]
    },
    {
        "id": 305334471,
        "title": "Top-down Tree Long Short-Term Memory Networks",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Xingxing Zhang",
            "Liang Lu",
            "Mirella Lapata"
        ],
        "references": [
            319770160,
            312341241,
            307747289,
            284576917,
            279864597,
            278965988,
            273145632,
            273067823,
            272091377,
            257882504
        ]
    },
    {
        "id": 305334400,
        "title": "Learning Natural Language Inference with LSTM",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Shuohang Wang",
            "Jing Jiang"
        ],
        "references": [
            284576917,
            284218871,
            279068396,
            265252627,
            262484733,
            244447093,
            221366753,
            221012732,
            13853244,
            281487270
        ]
    },
    {
        "id": 301880883,
        "title": "Long Short-Term Memory-Networks for Machine Reading",
        "abstract": "Machine reading, the automatic understanding of text, remains a challenging task of great value for NLP applications. We propose a machine reader which processes text incrementally from left to right, while linking the current word to previous words stored in memory and implicitly discovering lexical dependencies facilitating understanding. The reader is equipped with a Long Short-Term Memory architecture, which differs from previous work in that it has a memory tape (instead of a memory cell) for adaptively storing past information without severe information compression. We also integrate our reader with a new attention mechanism in encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.",
        "date": "January 2016",
        "authors": [
            "Jianpeng Cheng",
            "Li Dong",
            "Mirella Lapata"
        ],
        "references": [
            306094026,
            284576917,
            281380823,
            281084636,
            273067823,
            243781690,
            228452494,
            221605724,
            221603581,
            221489926
        ]
    },
    {
        "id": 299487682,
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Danqi Chen",
            "Christopher D. Manning"
        ],
        "references": [
            270877878,
            266201822,
            257882504,
            234814829,
            234796040,
            228339739,
            228102719,
            221013171,
            221012897,
            220874833
        ]
    },
    {
        "id": 270878092,
        "title": "Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features",
        "abstract": "We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers. First, to resolve the error propagation problem of the traditional pipeline approach, we incorporate POS tagging into the syntactic parsing process. Second, to alleviate the negative influence of size differences among competing action sequences, we align parser states during beam-search decoding. Third, to enhance the power of parsing models, we enlarge the feature set with non-local features and semisupervised word cluster features. Experimental results show that these modifications improve parsing performance significantly. Evaluated on the Chinese Tree- Bank (CTB), our final performance reaches 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers.",
        "date": "June 2014",
        "authors": [
            "Zhiguo Wang",
            "Nianwen Xue"
        ],
        "references": [
            266376373,
            266376369,
            228362666,
            221013135,
            220875148,
            271231477,
            270878198,
            266034441,
            262355038,
            220875180
        ]
    },
    {
        "id": 270818464,
        "title": "Automatic Feature Selection for Agenda-Based Dependency Parsing",
        "abstract": "In this paper we present an in-depth study on automatic feature selection for beam-search depen-dency parsers. The search strategy is inherited from the one implemented in MaltOptimizer, but searches in a much larger set of feature templates that could lead to a higher number of combina-tions. Our models provide results that are on par with models trained with a larger set of feature templates, and this implies that our models provide faster training and parsing times. Moreover, the results establish the state of the art for some of the languages.",
        "date": "August 2014",
        "authors": [
            "Miguel Ballesteros",
            "Bernd Bohnet"
        ],
        "references": [
            329979161,
            271997811,
            270568602,
            265145044,
            265022743,
            262402839,
            268720443,
            267508924,
            266034441,
            262349677
        ]
    },
    {
        "id": 304533937,
        "title": "Learning representations by back propagating errors. Cogn",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "D.E. Rumelhart",
            "G.E. Hinton",
            "R.J. Williams"
        ],
        "references": []
    },
    {
        "id": 271231477,
        "title": "Phrase Parses Reranking Based on Higher-Order Lexical Dependencies",
        "abstract": "The existing works on parsing show that lexical dependencies are helpful for phrase tree parsing. However, only first-order lexical dependencies have been employed and investigated in previous research. This paper proposes a novel method for employing higher-order lexical dependencies for phrase tree evaluation. The method is based on a parse reranking framework, which provides a constrained search space (via N-best lists or parse forests) and enables the parser to employ relatively complicated lexical dependency features. The models are evaluated on the UPenn Chinese Treebank. The highest F1 score reaches 85.74% and has outperformed all previously reported state-of-the-art systems. The dependency accuracy of phrase trees generated by the parser has been significantly improved as well.",
        "date": "October 2012",
        "authors": [
            "Zhi-Guo Wang",
            "Chengqing Zong"
        ],
        "references": [
            221101559,
            220817360
        ]
    },
    {
        "id": 270877743,
        "title": "Low-Rank Tensors for Scoring Dependency Structures",
        "abstract": "Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations. A small subset of such features is often selected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations. We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also obtain the best published UAS results on 5 languages.",
        "date": "June 2014",
        "authors": [
            "Tao Lei",
            "Yu Xin",
            "Yuan Zhang",
            "Regina Barzilay"
        ],
        "references": [
            270568602,
            266439415,
            262289048,
            262217466,
            262207624,
            237533325,
            234131319,
            230668375,
            228627119,
            221619685
        ]
    },
    {
        "id": 269997813,
        "title": "Grammar as a Foreign Language",
        "abstract": "Syntactic parsing is a fundamental problem in computational linguistics and Natural Language Processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domain-independent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem.",
        "date": "December 2014",
        "authors": [
            "Oriol Vinyals",
            "Lukasz Kaiser",
            "Terry Koo",
            "Slav Petrov"
        ],
        "references": [
            307747289,
            266376373,
            221345755,
            221013135,
            221012688,
            220874688,
            220874045,
            220873863,
            220817587,
            220816667
        ]
    },
    {
        "id": 289176638,
        "title": "The Inside-Outside Recursive Neural Network model for Dependency Parsing",
        "abstract": "We propose the first implementation of an infinite-order generative dependency model. The model is based on a new recursive neural network architecture, the Inside-Outside Recursive Neural Network. This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also topdown. This is achieved by computing content as well as context representations for any constituent, and letting these representations interact. Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores.",
        "date": "January 2014",
        "authors": [
            "Phong Le",
            "Willem Zuidema"
        ],
        "references": [
            266201822,
            262217466,
            329974423,
            310793417,
            284039049,
            270878821,
            270877599,
            270877541,
            262367926,
            262349927
        ]
    },
    {
        "id": 281812760,
        "title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems",
        "abstract": "We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-of-speech tagging and dependency parsing using our proposed models.",
        "date": "May 2015",
        "authors": [
            "Wang Ling",
            "Chris Dyer",
            "Alan W Black",
            "Isabel Trancoso"
        ],
        "references": [
            275213941,
            270878122,
            319770465,
            301404891,
            299487682,
            289942031,
            289758666,
            289098162,
            285932285,
            270878439
        ]
    },
    {
        "id": 279068962,
        "title": "Structured Training for Neural Network Transition-Based Parsing",
        "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.",
        "date": "June 2015",
        "authors": [
            "David Wei\u00df",
            "Christopher Alberti",
            "Michael Collins",
            "Slav Petrov"
        ],
        "references": [
            278965988,
            277560449,
            270878605,
            309186321,
            299487682,
            283842725,
            272746011,
            270878821,
            270878063,
            269997813
        ]
    },
    {
        "id": 283619991,
        "title": "Greed Is Good If Randomized: New Inference for Dependency Parsing",
        "abstract": "Dependency parsing with high-order features results in a provably hard decoding problem. A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems. In contrast, we explore, analyze, and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing: a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing; b) we show that, as a decoding algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets.",
        "date": "October 2014",
        "authors": [
            "Yuan Zhang",
            "Tao Lei",
            "Regina Barzilay",
            "Tommi S. Jaakkola"
        ],
        "references": [
            281090375,
            258818741,
            237533325,
            230875890,
            228627119,
            221102165,
            221013126,
            221012876,
            220873101,
            220816925
        ]
    },
    {
        "id": 220355244,
        "title": "Class-Based n-gram Models of Natural Language",
        "abstract": "We address the problem of predicting a word from previous words in a sample of text.In particular, we discuss n-gram models based on classes of words. We also discuss severalstatistical algorithms for assigning words to classes based on the frequency of their cooccurrencewith other words. We find that we are able to extract classes that have the flavor of eithersyntactically based groupings or semantically based groupings, depending on the nature of theunderlying statistics.1...",
        "date": "January 1992",
        "authors": [
            "Peter F. Brown",
            "Vincent joseph Dellapietra",
            "Peter V. de Souza",
            "Jennifer Lai"
        ],
        "references": [
            224377724,
            313201001,
            311708648,
            284145315,
            239059509,
            232128580,
            230876151,
            222479869,
            221995817,
            220049264
        ]
    },
    {
        "id": 3192689,
        "title": "Inducing Features of Random Fields",
        "abstract": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing",
        "date": "May 1997",
        "authors": [
            "Stephen Andrew Della Pietra",
            "Vincent joseph Dellapietra",
            "John Lafferty"
        ],
        "references": [
            243645162,
            310606656,
            288345724,
            265978139,
            245577013,
            240310918,
            239582331,
            235961479,
            223013541,
            221995817
        ]
    },
    {
        "id": 2596889,
        "title": "Adaptive Statistical Language Modeling: A Maximum Entropy Approach",
        "abstract": "Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge. In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy. Most existing statistical language models exploit the immediate past only. To extract information from further back in the document's history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from many sources must...",
        "date": "February 1970",
        "authors": [
            "Ronald Rosenfeld",
            "Jaime G. Carbonell",
            "Alexander Rudnicky",
            "Salim Roukos"
        ],
        "references": [
            232633438,
            220482445,
            270917716,
            246473416,
            239063154,
            235418978,
            232128580,
            230876473,
            230876022,
            50336131
        ]
    },
    {
        "id": 2418338,
        "title": "Cluster Expansions And Iterative Scaling For Maximum Entropy Language Models",
        "abstract": ". The maximum entropy method has recently been successfully introduced to a variety of natural language applications. In each of these applications, however, the power of the maximum entropy method is achieved at the cost of a considerable increase in computational requirements. In this paper we present a technique, closely related to the classical cluster expansion from statistical mechanics, for reducing the computational demands necessary to calculate conditional maximum entropy language models. 1. Introduction In this paper we present a computational technique that can enable faster calculation of maximum entropy models. The starting point for our method is an algorithm [1] for constructing maximum entropy distributions that is an extension of the generalized iterative scaling algorithm of Darroch and Ratcliff [2,3]. The extended algorithm relaxes the assumption of [2,3] that the constraint functions sum to a constant, and results in a set of decoupled polynomial equations, one f...",
        "date": "March 1999",
        "authors": [
            "John D. Lafferty",
            "Bernhard Suhm"
        ],
        "references": [
            234819841,
            234815897,
            224377724,
            38359320,
            3192689,
            240429666,
            234356583,
            220811987,
            38363908,
            3532137
        ]
    },
    {
        "id": 230876357,
        "title": "On Structuring Probabilistic Dependencies in Stochastic Language Modelling",
        "abstract": "In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.",
        "date": "January 1994",
        "authors": [
            "Hermann Ney",
            "Ute Essen",
            "Reinhard Kneser"
        ],
        "references": [
            3191800,
            223255907,
            3178307,
            3177932,
            3177586,
            3175558
        ]
    },
    {
        "id": 38363908,
        "title": "Generalized Iterative Scaling for Log-Linear Models",
        "abstract": "Say that a probability distribution $\\{p_i; i \\in I\\}$ over a finite set $I$ is in \"product form\" if (1) $p_i = \\pi_i\\mu \\prod^d_{s=1} \\mu_s^{b_si}$ where $\\pi_i$ and $\\{b_{si}\\}$ are given constants and where $\\mu$ and $\\{\\mu_s\\}$ are determined from the equations (2) $\\sum_{i \\in I} b_{si} p_i = k_s, s = 1, 2, \\cdots, d$; (3) $\\sum_{i \\in I} p_i = 1$. Probability distributions in product form arise from minimizing the discriminatory information $\\sum_{i \\in I} p_i \\log p_i/\\pi_i$ subject to (2) and (3) or from maximizing entropy or maximizing likelihood. The theory of the iterative scaling method of determining (1) subject to (2) and (3) has, until now, been limited to the case when $b_{si} = 0, 1$. In this paper the method is generalized to allow the $b_{si}$ to be any real numbers. This expands considerably the list of probability distributions in product form which it is possible to estimate by maximum likelihood.",
        "date": "October 1972",
        "authors": [
            "J. N. DARROCH",
            "D. RATCLIFF"
        ],
        "references": [
            271775547,
            243081893,
            220247016,
            38366114,
            38364508,
            38364499,
            18051412
        ]
    },
    {
        "id": 2626194,
        "title": "Putting It All Together: Language Model Combination",
        "abstract": "In the past several years, a number of di#erent language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, modified Kneser-Ney smoothing, and clustering. While all of these techniques have been studied separately, they have rarely been studied in combination. We find some significant interactions, especially with smoothing techniques. The combination of all techniques leads to up to a 45% perplexity reduction over a Katz smoothed trigram model with no count cuto#s, the highest such perplexity reduction reported. 1. INTRODUCTION For many years, simple trigram models, smoothed with Katz smoothing [5] or similar techniques, have represented the standard baseline for language modeling. During that time, many improvements over this simple model have been suggested, including skipping [11, 3, 10], clustering [1, 10], caching [6, 7, 8], higher-order n-grams, smoothing [2, 10], and sentence mixture models [4]. While each of ...",
        "date": "December 1999",
        "authors": [
            "Joshua T. Goodman"
        ],
        "references": [
            221484301,
            220355244,
            3703273,
            3191800,
            2596889,
            2416307,
            242821088,
            239059370,
            230876357,
            221102572
        ]
    },
    {
        "id": 2362847,
        "title": "Language Model Size Reduction By Pruning And Clustering",
        "abstract": "Several techniques are known for reducing the size of language models, including count cutoffs [1], Weighted Difference pruning [2], Stolcke pruning [3], and clustering [4]. We compare all of these techniques and show some surprising results. For instance, at low pruning thresholds, Weighted Difference and Stolcke pruning underperform count cutoffs. We then show novel clustering techniques that can be combined with Stolcke pruning to produce the smallest models at a given perplexity. The resulting models can be a factor of three or more smaller than models pruned with Stolcke pruning, at the same perplexity. The technique creates clustered models that are often larger than the unclustered models, but which can be pruned to models that are smaller than unclustered models with the same perplexity.",
        "date": "August 2000",
        "authors": [
            "Joshua Goodman",
            "Jianfeng Gao"
        ],
        "references": [
            220355244,
            230876148,
            3793872,
            3703274,
            2348437
        ]
    },
    {
        "id": 2328946,
        "title": "Efficient Training Methods For Maximum Entropy Language Modeling",
        "abstract": "Maximum entropy language modeling techniques combine different sources of statistical dependence, such as syntactic relationships, topic cohesiveness and collocation frequency, in a unified and effective language model. These techniques however are also computationally very intensive, particularly during model estimation, compared to the more prevalent alternative of interpolating several simple models, each capturing one type of dependency. In this paper we present ways which significantly reduce this complexity by reorganizing the required computations. We show that in case of a model with N-gram constraints, each iteration of the parameter estimation algorithm requires the same amount of computation as estimating a comparable back-off N-gram model. In general, the computational cost of each iteration in model estimation is linear in the number of distinct \"histories\" seen in the training corpus, times a model-class dependent factor. The reorganization focuses mainly on reducing this...",
        "date": "November 2000",
        "authors": [
            "Jun Wu",
            "Sanjeev Khudanpur"
        ],
        "references": [
            38359595,
            3908396,
            2418338,
            238270360,
            232643301,
            3532137
        ]
    },
    {
        "id": 283556656,
        "title": "An Effective Neural Network Model for Graph-based Dependency Parsing",
        "abstract": "Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can automatically learn high-order feature combinations using only atomic features by exploiting a novel activation function tanhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers.",
        "date": "January 2015",
        "authors": [
            "Wenzhe Pei",
            "Tao Ge",
            "Baobao Chang"
        ],
        "references": [
            301446330,
            289176638,
            270877660,
            266376264,
            266201822,
            262272503,
            234131319,
            319770439,
            299487682,
            284039049
        ]
    },
    {
        "id": 277022875,
        "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network",
        "abstract": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.",
        "date": "May 2015",
        "authors": [
            "Zhu Chenxi",
            "Xipeng Qiu",
            "Xinchi Chen",
            "Xuanjing Huang"
        ],
        "references": [
            301446330,
            289176638,
            270878122,
            266376264,
            266201822,
            311469848,
            307955489,
            299487682,
            284039049,
            270878508
        ]
    },
    {
        "id": 273067823,
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "abstract": "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
        "date": "February 2015",
        "authors": [
            "Kai Sheng Tai",
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "references": [
            319770160,
            307747289,
            284576917,
            329975395,
            319770465,
            301409028,
            301408986,
            299487682,
            286964721,
            284039049
        ]
    },
    {
        "id": 264003485,
        "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment",
        "abstract": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).",
        "date": "August 2014",
        "authors": [
            "Marco Marelli",
            "Luisa Bentivogli",
            "Marco Baroni",
            "Raffaella Bernardi"
        ],
        "references": [
            319395280,
            301409030,
            301408886,
            301404974,
            301409028,
            301408986,
            301408981,
            301408980,
            301405051,
            301404948
        ]
    },
    {
        "id": 228569700,
        "title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks",
        "abstract": "Natural language parsing has typically been done with small sets of discrete cate-gories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lex-icalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architec-ture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-sensitive re-cursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases \"decline to com-ment\" and \"would not disclose the terms\" are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1% on the Wall Street Journal dataset for sentences up to length 15.",
        "date": "January 2010",
        "authors": [
            "Richard Socher",
            "Christopher D. Manning",
            "Andrew Y Ng"
        ],
        "references": [
            221012623,
            220946819,
            220873863,
            220873681,
            220873596,
            225818196,
            222681214,
            221345848,
            220873245,
            220355517
        ]
    },
    {
        "id": 262274512,
        "title": "Knowledge Sources for Constituent Parsing of German, a Morphologically Rich and Less-Configurational Language",
        "abstract": "We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages.",
        "date": "March 2013",
        "authors": [
            "Alexander Fraser",
            "Helmut Schmid",
            "Rich\u00e1rd Farkas",
            "Renjing Wang"
        ],
        "references": [
            262408350,
            234824998,
            234804753,
            228629276,
            221102780,
            221101625,
            221013153,
            221013057,
            221012897,
            220947009
        ]
    },
    {
        "id": 255682142,
        "title": "Morphological Tagging: Data vs. Dictionaries",
        "abstract": "Part of Speech tagging for English seems to have reached the the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is still an open problem, and the results are far from being satisfactory. This paper presents results obtained by using a universalized exponential feature-based model for five such languages. It focuses on the data sparseness issue, which is especially severe for such languages (the more so that there are no extensive annotated data for those languages). In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances.",
        "date": "January 2000",
        "authors": [
            "Jan Haji\u010d"
        ],
        "references": [
            221152217,
            220874161,
            2578231,
            247697163,
            243697334,
            230875928,
            215470768,
            2778347
        ]
    },
    {
        "id": 221102585,
        "title": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging.",
        "abstract": "We present a HMM part-of-speech tag- ging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contex- tual POS probabilities of the HMM into a product of attribute probabilities, (2) esti- mation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.",
        "date": "January 2008",
        "authors": [
            "Helmut Schmid",
            "Florian Laws"
        ],
        "references": [
            278724277,
            226043864,
            225195066,
            221112612,
            220874161,
            220874018,
            2882867,
            2539686,
            1782255,
            288905957
        ]
    },
    {
        "id": 220875017,
        "title": "Practical very large scale CRFs",
        "abstract": "Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a l penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.",
        "date": "January 2010",
        "authors": [
            "Thomas Lavergne",
            "Olivier Capp\u00e9",
            "Fran\u00e7ois Yvon"
        ],
        "references": [
            234801831,
            225121454,
            221345409,
            221344720,
            220875020,
            220874186,
            220873596,
            220873150,
            220017637,
            46515750
        ]
    },
    {
        "id": 220874670,
        "title": "Guided Learning for Bidirectional Sequence Classification.",
        "abstract": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learn- ing the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algo- rithm. We apply this novel learning algo- rithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.",
        "date": "January 2007",
        "authors": [
            "Libin Shen",
            "Giorgio Satta",
            "Aravind K. Joshi"
        ],
        "references": [
            230920580,
            220816928,
            220017637,
            2930135,
            2882867,
            2799191,
            288905957,
            248208758,
            234543089,
            230876410
        ]
    },
    {
        "id": 254675183,
        "title": "Minimizers, Maximizers and the Rhetoric of Scalar Reasoning",
        "abstract": "This paper addresses two basic questions about polarity items: what sorts of meaning can such forms encode and why should such forms exist in the first place. My starting point is the Scalar Model of Polarity (Israel 1996, 1998), which predicts a reliable correlation between a polarity item's sensitivity and its scalar semantic properties: specifically, it predicts that forms denoting a minimal scalar degree may be emphatic negative polarity items (NPIs), while forms denoting maximal degrees can be emphatic positive polarity items. A variety of anomalous polarity items are discussed which flout this prediction, including both emphatic NPIs denoting maximal degrees (e.g. for all the tea in China, wild horses) and emphatic PPIs denoting minimal degrees (e.g. for a pittance, in a jiffy). The exceptional behavior of these forms is shown to be a direct function of the participant roles they denote, reflecting the fact that different roles license different kinds of scalar inferences depending on how they contribute to the likelihood of an expressed proposition. In addition to establishing a link between thematic structure and the lexical semantics of polarity sensitivity, this result is shown to have important implications for the nature of scalar reasoning generally, and for the role it plays in structuring rhetorical discourse.",
        "date": "November 2001",
        "authors": [
            "Michael Israel"
        ],
        "references": [
            285977158,
            285102407,
            273014193,
            252524625,
            318494453,
            284430590,
            275999985,
            275998984,
            271650889,
            271213714
        ]
    },
    {
        "id": 303475895,
        "title": "Sentiment composition",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "K. Moilanen",
            "S. Pulman"
        ],
        "references": []
    },
    {
        "id": 284025120,
        "title": "Information, relevance and social decision-making",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "A. Merin"
        ],
        "references": []
    },
    {
        "id": 225955045,
        "title": "Denial and contrast: A relevance theoretic analysis ofbut",
        "abstract": "",
        "date": "February 1989",
        "authors": [
            "Diane Lesley Blakemore"
        ],
        "references": [
            259810910,
            246430630,
            243677541,
            240752088,
            313005855,
            284046340,
            271213760,
            270151899,
            269425203,
            256279346
        ]
    },
    {
        "id": 286902441,
        "title": "OxLM: A Neural Language Modelling Framework for Machine Translation",
        "abstract": "This paper presents an open source implementation1 of a neural language model for machine translation. Neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. The language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. Neural language models are notoriously slow to train and test. Our framework is designed with scalability in mind and provides two optional techniques for reducing the computational cost: the so-called class decomposition trick and a training algorithm based on noise contrastive estimation. Our models may be extended to incorporate direct n-gram features to learn weights for every n-gram in the training data. Our framework comes with wrappers for the cdec and Moses translation toolkits, allowing our language models to be incorporated as normalized features in their decoders (inside the beam search).",
        "date": "January 2014",
        "authors": [
            "Baltescu Paul",
            "Blunsom Phil",
            "Hoang Hieu"
        ],
        "references": [
            259239818,
            241637478,
            228882059,
            228348202,
            301870716,
            285432248,
            270878029,
            262416331,
            228782453,
            228095628
        ]
    },
    {
        "id": 277560449,
        "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
        "abstract": "We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.",
        "date": "May 2015",
        "authors": [
            "Chris Dyer",
            "Miguel Ballesteros",
            "Wang Ling",
            "Austin Matthews"
        ],
        "references": [
            289176638,
            281812760,
            279068962,
            329975395,
            319770465,
            319770387,
            307955489,
            299487682,
            284039049,
            283619991
        ]
    },
    {
        "id": 270878122,
        "title": "Tailoring Continuous Word Representations for Dependency Parsing",
        "abstract": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity.",
        "date": "June 2014",
        "authors": [
            "Mohit Bansal",
            "Kevin Gimpel",
            "K. Livescu"
        ],
        "references": [
            270878323,
            266439415,
            266201822,
            319770439,
            306160949,
            303802749,
            289942031,
            270877781,
            267448320,
            265562039
        ]
    },
    {
        "id": 257870129,
        "title": "Transition-based Dependency Parsing with Selectional Branching",
        "abstract": "We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transition-based parser that uses beam search.",
        "date": "August 2013",
        "authors": [
            "Jinho D. Choi",
            "Andrew Mccallum"
        ],
        "references": [
            324940566,
            262402839,
            262311224,
            262240815,
            262217466,
            270878151,
            268720443,
            262389533,
            262319307,
            262294216
        ]
    },
    {
        "id": 251574306,
        "title": "The Stanford typed dependencies representation",
        "abstract": "This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward de- scription of grammatical relations for any user who could benefit from automatic text understanding. For such purposes, we ar- gue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the rela- tions. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stan- ford scheme for parser evaluation.",
        "date": "January 2008",
        "authors": [
            "Marie-Catherine de Marneffe",
            "Christopher D. Manning"
        ],
        "references": [
            261843006,
            250377014,
            242080024,
            228526428,
            313244022,
            265081885,
            260065124,
            234820797,
            230876748,
            228584658
        ]
    },
    {
        "id": 228707544,
        "title": "Deterministic dependency parsing of English text",
        "abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal sec-tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces la-beled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II an-notation scheme. The best overall accuracy ob-tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).",
        "date": "January 2004",
        "authors": [
            "Joakim Nivre",
            "Mario Scholz"
        ],
        "references": [
            235990461,
            232031795,
            228916842,
            278651717,
            277288972,
            245589455,
            245588735,
            239056767,
            238663823,
            225879575
        ]
    },
    {
        "id": 221618802,
        "title": "Modeling the effects of memory on human online sentence processing with particle filters",
        "abstract": "Language comprehension in humans is significantly constrai ned by memory, yet rapid, highly incremental, and capable of utilizing a wide r ange of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and fi elded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehen- sion which involves an adaptation of the particle filter, a se quential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the first rational account of an outstanding proble m in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information.",
        "date": "January 2008",
        "authors": [
            "Roger P. Levy",
            "Florencia Reali",
            "Thomas L. Griffiths"
        ],
        "references": [
            230876277,
            230875913,
            313757208,
            308468610,
            305263025,
            268237865,
            243673325,
            239051972,
            234785116,
            233756936
        ]
    },
    {
        "id": 221013207,
        "title": "Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing.",
        "abstract": "Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.",
        "date": "January 2010",
        "authors": [
            "Phil Blunsom",
            "Trevor Cohn"
        ],
        "references": [
            269031809,
            252967672,
            234824953,
            307175021,
            301858375,
            265369049,
            259800336,
            252929091,
            239582331,
            235210576
        ]
    },
    {
        "id": 221013182,
        "title": "Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing.",
        "abstract": "We describe a generative model for non-projective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature. We then develop a dynamic programming parsing algorithm for our model, and derive an inside-outside algorithm that can be used for unsupervised learning of non-projective dependency trees.",
        "date": "January 2011",
        "authors": [
            "Shay B. Cohen",
            "Carlos G\u00f3mez-Rodr\u00edguez",
            "Giorgio Satta"
        ],
        "references": [
            244527537,
            237533325,
            234809510,
            228916842,
            228882434,
            245724574,
            234795361,
            228669927,
            222506919,
            221013277
        ]
    },
    {
        "id": 221013171,
        "title": "Fast and Robust Multilingual Dependency Parsing with a Generative Latent Variable Model.",
        "abstract": "We use a generative history-based model to predict the most likely derivation of a dependency parse. Our probabilistic model is based on Incremental Sigmoid Belief Networks, a recently proposed class of latent variable models for structure prediction. Their ability to automatically induce features results in multilingual parsing which is robust enough to achieve accuracy well above the average for each individual language in the multilingual track of the CoNLL-2007 shared task. This robustness led to the third best overall average labeled attachment score in the task, despite using no discriminative methods. We also demonstrate that the parser is quite fast, and can provide even faster parsing times without much loss of accuracy.",
        "date": "January 2007",
        "authors": [
            "Ivan Titov",
            "James Henderson"
        ],
        "references": [
            242375525,
            237533325,
            220874766,
            220874688,
            220874045,
            251347641,
            238319783,
            238211182,
            234829084,
            228892601
        ]
    },
    {
        "id": 221012805,
        "title": "Multi-Source Transfer of Delexicalized Dependency Parsers.",
        "abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.",
        "date": "January 2011",
        "authors": [
            "Ryan T. McDonald",
            "Slav Petrov",
            "Keith B Hall"
        ],
        "references": [
            237533325,
            230875890,
            228822214,
            221345468,
            221102262,
            221013207,
            269031283,
            234802196,
            228379274,
            221013260
        ]
    },
    {
        "id": 220874833,
        "title": "Transition-based Dependency Parsing with Rich Non-local Features.",
        "abstract": "Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transition-based parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.",
        "date": "January 2011",
        "authors": [
            "Yue Zhang",
            "Joakim Nivre"
        ],
        "references": [
            234814829,
            228916842,
            226080200,
            221013054,
            221012897,
            269031283,
            268720443,
            228824508,
            221012972,
            221012880
        ]
    },
    {
        "id": 220874243,
        "title": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency.",
        "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
        "date": "January 2004",
        "authors": [
            "Dan Klein",
            "Christopher D. Manning"
        ],
        "references": [
            221618388,
            221102754,
            307175021,
            301864384,
            289827888,
            246981102,
            242430108,
            239535201,
            230875831,
            221523376
        ]
    },
    {
        "id": 266797103,
        "title": "BLEU deconstructed: Designing a Better MT Evaluation Metric",
        "abstract": "",
        "date": "March 2013",
        "authors": [
            "Xingyi Song",
            "Trevor Cohn",
            "Lucia Specia"
        ],
        "references": [
            307915669,
            286920377,
            267379415,
            262368557,
            317444544,
            312607081,
            309347291,
            296425392,
            262397696,
            262366271
        ]
    },
    {
        "id": 221012675,
        "title": "Domain Adaptation via Pseudo In-Domain Data Selection",
        "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora \u2010 1% the size of the original \u2010 can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.",
        "date": "January 2011",
        "authors": [
            "Amittai Axelrod",
            "Xiaodong He",
            "Jianfeng Gao"
        ],
        "references": [
            252868319,
            228846991,
            228740822,
            228361002,
            228355180,
            228347509,
            221013146,
            240911908,
            221013211,
            220875255
        ]
    },
    {
        "id": 233409624,
        "title": "Sequence Transduction with Recurrent Neural Networks",
        "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.",
        "date": "November 2012",
        "authors": [
            "Alex Graves"
        ],
        "references": [
            221620610,
            221620570,
            221619960,
            221364406,
            221346365,
            13853244,
            3316656,
            3302208,
            2985446,
            2562741
        ]
    },
    {
        "id": 221013286,
        "title": "Better Evaluation Metrics Lead to Better Machine Translation.",
        "abstract": "Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better human-judged translation quality than the BLEU-tuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems.",
        "date": "January 2011",
        "authors": [
            "Chang Liu",
            "Daniel Dahlmeier",
            "Hwee Tou Ng"
        ],
        "references": [
            238769089,
            228668187,
            228346240,
            228341145,
            221013242,
            220874816,
            220874513,
            220816978,
            220419015,
            2588204
        ]
    },
    {
        "id": 271513141,
        "title": "Support Vector Networks",
        "abstract": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.",
        "date": "September 1995",
        "authors": [
            "Corinna Cortes",
            "VN Vapnik"
        ],
        "references": [
            246929280,
            260869480,
            248857140,
            243743707,
            239059398,
            236736807,
            38366305,
            2439558
        ]
    },
    {
        "id": 341530631,
        "title": "An Introduction to Neural Networks",
        "abstract": "This chapter gives an introduction to the important topic of neural networks, computing systems based loosely on the connections between the neurons in the brain, which are increasingly widely used in data mining as well as other areas. A feed-forward neural network with backpropagation is described and its operation explained in detail. It is shown how such a network can be used for classification tasks and experimental results with two well-known datasets are presented. The chapter ends with some cautionary remarks about the drawbacks involved in using a neural net, relating mainly to inscrutability and the risk of inadvertently building bias into the system\u2019s conclusions.",
        "date": "May 2020",
        "authors": [
            "Max Bramer"
        ],
        "references": []
    },
    {
        "id": 313601183,
        "title": "Optimal brain damage. in",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "Y. LeCun",
            "John Denker",
            "S.A. Solla",
            "R.E. Howard"
        ],
        "references": []
    },
    {
        "id": 312986214,
        "title": "Some studies in machine learning using the game of checkers",
        "abstract": "",
        "date": "January 1959",
        "authors": [
            "A.L. Samuel"
        ],
        "references": [
            288384866
        ]
    },
    {
        "id": 291743705,
        "title": "Connectionist music composition based on melodic, stylistic, and psychophysical constraints",
        "abstract": "",
        "date": "January 1991",
        "authors": [
            "M.C. Mozer"
        ],
        "references": []
    },
    {
        "id": 286375944,
        "title": "Supervised learning systems with excess degrees of freedom",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "Michael Jordan",
            "Robert Jacobs"
        ],
        "references": []
    },
    {
        "id": 285483611,
        "title": "Learning to generate artificial fovea trajectories for target detection",
        "abstract": "",
        "date": "January 1991",
        "authors": [
            "J. Schmidhuber",
            "R. Huber"
        ],
        "references": []
    },
    {
        "id": 273130022,
        "title": "Incremental Learning, or The Importance of Starting Small",
        "abstract": "",
        "date": "March 1991",
        "authors": [
            "Jeffrey Elman"
        ],
        "references": []
    },
    {
        "id": 266850402,
        "title": "Cognitive Map Construction and Use: A Parallel Distributed Processing Approach",
        "abstract": "The Connectionist Navigational Map (CNM) is a parallel distributed processing architecture for the learning and use of robot spatial maps. It is shown here how a robot can, using a recurrent network (the CNM predictive map), learn a model of its environment that allows it to predict what sensations it would have if it were to move in a particular way. It is shown how this predictive ability can be used (via the CNM orienting system) to enable the robot to determine its current location. This ability, in turn, can be used, when given a desired sensation, to generate sequences of goal states that provide a route to a place with the desired sensory properties. This sequence is given to the CNM's inverse model, which in turn generates a sequence of actions that effects the desired state transitions, thus providing a sort of \u201ccontent-addressable\u201d planning capability. Finally, the theoretical motivation behind this work is discussed.",
        "date": "January 1990",
        "authors": [
            "Ron Chrisley"
        ],
        "references": [
            243652967,
            221617942,
            313125996,
            311569941,
            245838218,
            243786282,
            243764382,
            243743707,
            239328207,
            239059649
        ]
    },
    {
        "id": 265461524,
        "title": "Neural networks. An introduction",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "Berndt M\u00fcller",
            "Joachim Reinhardt"
        ],
        "references": []
    },
    {
        "id": 258247515,
        "title": "On Fast Dropout and its Applicability to Recurrent Networks",
        "abstract": "Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overfitting has seen considerably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them when overfitting and vanishes at minima of an unregularized training loss. The derivatives of that regularizer are exclusively based on the training error signal. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets and a natural language processing (NLP) task.",
        "date": "November 2013",
        "authors": [
            "Justin Bayer",
            "Christian Osendorfer",
            "Nutan Chen",
            "Sebastian Urban"
        ],
        "references": [
            256446199,
            246546737,
            287034172,
            286271944,
            267706055,
            262395872,
            261159435,
            258818168,
            255173850,
            248512106
        ]
    },
    {
        "id": 234140324,
        "title": "Knowledge Matters: Importance of Prior Information for Optimization",
        "abstract": "We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.",
        "date": "January 2013",
        "authors": [
            "Caglar Gulcehre",
            "Y. Bengio"
        ],
        "references": [
            303137904,
            319770387,
            319770174,
            303256841,
            288023219,
            271528918,
            270663502,
            267960550,
            266230588,
            258304769
        ]
    },
    {
        "id": 233866039,
        "title": "High-dimensional sequence transduction",
        "abstract": "We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.",
        "date": "December 2012",
        "authors": [
            "Nicolas Boulanger-Lewandowski",
            "Y. Bengio",
            "Pascal Vincent"
        ],
        "references": [
            239571798,
            228095594,
            313060232,
            304109482,
            262395872,
            261466508,
            243743707,
            233409624,
            230876435,
            224576777
        ]
    },
    {
        "id": 233753224,
        "title": "Theano: new features and speed improvements",
        "abstract": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.",
        "date": "November 2012",
        "authors": [
            "Frederic Bastien",
            "Pascal Lamblin",
            "Razvan Pascanu",
            "James Bergstra"
        ],
        "references": [
            3422935,
            319770411,
            303256841,
            264890087,
            243743707,
            230876435,
            221480736,
            221344668,
            213877848,
            3422938
        ]
    },
    {
        "id": 228467601,
        "title": "The Manifold Tangent Classifier",
        "abstract": "We combine three important ideas present in previous work for building classi-fiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concen-trates near low-dimensional manifolds), and the manifold hypothesis for classifi-cation (different classes correspond to disjoint manifolds separated by low den-sity). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained.",
        "date": "January 2011",
        "authors": [
            "Salah Rifai",
            "Yann N Dauphin",
            "Pascal Vincent",
            "Y. Bengio"
        ],
        "references": [
            303137904,
            224716259,
            224579211,
            221618747,
            285599593,
            285599267,
            284035345,
            228338321,
            223450457,
            221619959
        ]
    },
    {
        "id": 228095594,
        "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription",
        "abstract": "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.",
        "date": "June 2012",
        "authors": [
            "Nicolas Boulanger-Lewandowski",
            "Y. Bengio",
            "Pascal Vincent"
        ],
        "references": [
            239571798,
            238835966,
            221664805,
            314823957,
            304109482,
            243743707,
            234798442,
            230876435,
            228370618,
            221619549
        ]
    },
    {
        "id": 263086337,
        "title": "Edinburgh's Phrase-based Machine Translation Systems for WMT-14",
        "abstract": "This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.",
        "date": "June 2014",
        "authors": [
            "Nadir Durrani",
            "Barry Haddow",
            "Philipp Koehn",
            "Kenneth Heafield"
        ],
        "references": [
            263086526,
            263086334,
            263086333,
            262047259,
            257943462,
            257943391,
            257943381,
            257943380,
            270878669,
            270878335
        ]
    },
    {
        "id": 221620298,
        "title": "LSTM can solve hard long time lag problems",
        "abstract": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",
        "date": "January 1996",
        "authors": [
            "Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            243781690,
            243698906,
            243683010,
            226171158,
            221620263,
            313702304,
            243763246,
            239594484,
            239062408,
            224740344
        ]
    },
    {
        "id": 221346365,
        "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural 'networks",
        "abstract": "Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.",
        "date": "January 2006",
        "authors": [
            "Alex Graves",
            "Santiago Fern\u00e1ndez",
            "Faustino Gomez",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            230875873,
            221080352,
            220320057,
            13853244,
            11294744,
            245123229,
            230876429,
            230875884,
            221620501,
            215721451
        ]
    },
    {
        "id": 51968606,
        "title": "Building high-level features using large scale unsupervised learning",
        "abstract": "We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative im- provement over the previous state-of-the-art.",
        "date": "December 2011",
        "authors": [
            "Quoc V. Le",
            "Marc'Aurelio Ranzato",
            "Rajat Monga",
            "Matthieu Devin"
        ],
        "references": [
            284683101,
            224716259,
            224260345,
            285058764,
            243716653,
            233820854,
            230675350,
            229043457,
            224254831,
            222029751
        ]
    },
    {
        "id": 5583935,
        "title": "Learning long-term dependencies with gradient descent is difficult",
        "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.",
        "date": "February 1994",
        "authors": [
            "Y. Bengio",
            "Patrice Y. Simard",
            "Paolo Frasconi"
        ],
        "references": [
            262294229,
            224663842,
            224663746,
            279351140,
            246567598,
            243781476,
            243766097,
            239041167,
            233822447,
            230876435
        ]
    },
    {
        "id": 227447179,
        "title": "Simulation-Based Optimization with Stochastic Approximation Using Common Random Numbers",
        "abstract": "The method of Common Random Numbers is a technique used to reduce the variance of difference estimates in simulation optimization problems. These differences are commonly used to estimate gradients of objective functions as part of the process of determining optimal values for parameters of a simulated system. Asymptotic results exist which show that using the Common Random Numbers method in the iterative Finite Difference Stochastic Approximation optimization algorithm (FDSA) can increase the optimal rate of convergence of the algorithm from the typical rate of k<sup>-1/3</sup> to the faster k<sup>-1/2</sup>, where k is the algorithm's iteration number. Simultaneous Perturbation Stochastic Approximation (SPSA) is a newer and often much more efficient optimization algorithm, and we will show that this algorithm, too, converges faster when the Common Random Numbers method is used. We will also provide multivariate asymptotic covariance matrices for both the SPSA and FDSA errors.",
        "date": "November 1999",
        "authors": [
            "Nathan L Kleinman",
            "James C. Spall",
            "Daniel Naiman"
        ],
        "references": [
            279958061,
            265461765,
            256595241,
            236736791,
            227446412,
            224656789,
            224388656,
            38367465,
            38367320,
            38365010
        ]
    },
    {
        "id": 221345414,
        "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
        "abstract": "Recently, several learning algorithms rely- ing on models with deep architectures have been proposed. Though they have demon- strated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a con- trolled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experi- ments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",
        "date": "January 2007",
        "authors": [
            "Hugo Larochelle",
            "Dumitru Erhan",
            "Aaron Courville",
            "James Bergstra"
        ],
        "references": [
            303137904,
            216792737,
            200744506,
            2924845,
            228715647,
            220320616,
            216792879,
            7017915,
            6912170,
            5920308
        ]
    },
    {
        "id": 220319875,
        "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
        "abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main quest ion investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre- training guides the learning towards basins of attraction o f minima that support better generalization from the training data set; the evidence from these results s upports a regularization explanation for the effect of pre-training.",
        "date": "February 2010",
        "authors": [
            "Dumitru Erhan",
            "Y. Bengio",
            "Aaron Courville",
            "Pierre-Antoine Manzagol"
        ],
        "references": [
            303137904,
            265022827,
            236736855,
            228339739,
            311469666,
            304109482,
            301346768,
            285599593,
            245993331,
            238681227
        ]
    },
    {
        "id": 215991023,
        "title": "Learning Deep Architectures for AI",
        "abstract": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can repre- sent high-level abstractions (e.g. in vision, language, an d other AI-level tasks), one needs deep architec- tures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult opti mization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper d iscusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
        "date": "January 2009",
        "authors": [
            "Y. Bengio"
        ],
        "references": [
            326332457,
            316805467,
            312538118,
            312451443,
            312370418,
            311469666,
            310606656,
            309076254,
            304109482,
            303802749
        ]
    },
    {
        "id": 45130828,
        "title": "Response Surface Methodology for Optimizing Hyper Parameters",
        "abstract": "The performance of an algorithm often largely depends on some hyper parameter which should be optimized before its usage. Since most conventional optimization methods suffer from some drawbacks, we developed an alternative way to find the best hyper parameter values. Contrary to the well known procedures, the new optimization algorithm is based on statistical methods since it uses a combination of Linear Mixed Effect Models and Response Surface Methodology techniques. In particular, the Method of Steepest Ascent which is well known for the case of an Ordinary Least Squares setting and a linear response surface has been generalized to be applicable for repeated measurements situations and for response surfaces of order o",
        "date": "February 2006",
        "authors": [
            "Claus Weihs",
            "Karsten Luebke",
            "Irina Czogiel"
        ],
        "references": [
            329476461,
            302970924,
            281115042,
            280939936,
            278651717,
            268250757,
            266995177,
            242504926,
            235710901,
            221997066
        ]
    },
    {
        "id": 10710734,
        "title": "Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)",
        "abstract": "This paper presents a novel evolutionary optimization strategy based on the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). This new approach is intended to reduce the number of generations required for convergence to the optimum. Reducing the number of generations, i.e., the time complexity of the algorithm, is important if a large population size is desired: (1) to reduce the effect of noise; (2) to improve global search properties; and (3) to implement the algorithm on (highly) parallel machines. Our method results in a highly parallel algorithm which scales favorably with large numbers of processors. This is accomplished by efficiently incorporating the available information from a large population, thus significantly reducing the number of generations needed to adapt the covariance matrix. The original version of the CMA-ES was designed to reliably adapt the covariance matrix in small populations but it cannot exploit large populations efficiently. Our modifications scale up the efficiency to population sizes of up to 10n, where n is the problem dimension. This method has been applied to a large number of test problems, demonstrating that in many cases the CMA-ES can be advanced from quadratic to linear time complexity.",
        "date": "February 2003",
        "authors": [
            "Nikolaus Hansen",
            "Sibylle D M\u00fcller",
            "Petros Koumoutsakos"
        ],
        "references": [
            234811153,
            3418513,
            2450762,
            265458990,
            220693804,
            220375075,
            34205020,
            2826833
        ]
    },
    {
        "id": 6026283,
        "title": "Optimization by Simulated Annealing",
        "abstract": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.",
        "date": "June 1983",
        "authors": [
            "Scott Kirkpatrick",
            "C. D. Jr. Gelatt",
            "M. P. Jr. Vecchi"
        ],
        "references": [
            233971739,
            243531012,
            242601838,
            235562503,
            235543488,
            235491650,
            234213316,
            231920631,
            227445032,
            226464504
        ]
    },
    {
        "id": 321133827,
        "title": "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code",
        "abstract": "Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.",
        "date": "January 1979",
        "authors": [
            "M D McKay",
            "Richard Beckman",
            "William Conover"
        ],
        "references": []
    },
    {
        "id": 261119155,
        "title": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition",
        "abstract": "Convolutional Neural Networks (CNN) have showed success in achieving translation invariance for many image processing tasks. The success is largely attributed to the use of local filtering and max-pooling in the CNN architecture. In this paper, we propose to apply CNN to speech recognition within the framework of hybrid NN-HMM model. We propose to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance. In our method, a pair of local filtering layer and max-pooling layer is added at the lowest end of neural network (NN) to normalize spectral variations of speech signals. In our experiments, the proposed CNN architecture is evaluated in a speaker independent speech recognition task using the standard TIMIT data sets. Experimental results show that the proposed CNN method can achieve over 10% relative error reduction in the core TIMIT test sets when comparing with a regular NN using the same number of hidden layers and weights. Our results also show that the best result of the proposed CNN model is better than previously published results on the same TIMIT test sets that use a pre-trained deep NN model.",
        "date": "May 2012",
        "authors": [
            "Ossama Abdel-Hamid",
            "Abdel-rahman Mohamed",
            "Hui Jiang",
            "Gerald Penn"
        ],
        "references": [
            228446017,
            224216007,
            221620570,
            221619818,
            216792820,
            4082304,
            257267653,
            239983422,
            221487297,
            3175691
        ]
    },
    {
        "id": 255564594,
        "title": "Revisiting Recurrent Neural Networks for Robust ASR",
        "abstract": "In this paper, we show how new training principles and optimization techniques for neural networks can be used for different network structures. In particular, we revisit the Recurrent Neural Network (RNN), which explicitly models the Markovian dynamics of a set of observations through a non-linear function with a much larger hidden state space than traditional sequence models such as an HMM. We apply pretraining principles used for Deep Neural Networks (DNNs) and second-order optimization techniques to train an RNN. Moreover, we explore its application in the Aurora2 speech recognition task under mismatched noise conditions using a Tandem approach. We observe top performance on clean speech, and under high noise conditions, compared to multi-layer perceptrons (MLPs) and DNNs, with the added benefit of being a \"deeper\" model than an MLP but more compact than a DNN.",
        "date": "May 2012",
        "authors": [
            "Oriol Vinyals",
            "Suman Ravuri",
            "Daniel Povey"
        ],
        "references": [
            232655969,
            228871422,
            221491058,
            221489926,
            312468272,
            311469848,
            309349955,
            304826998,
            242101249,
            224226885
        ]
    },
    {
        "id": 230875873,
        "title": "Connectionist Speech Recognition: A Hybrid Approach",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "Herve Bourlard",
            "Nelson Morgan"
        ],
        "references": [
            247847810,
            243698906,
            267162080,
            260673067,
            245577667,
            244418125,
            243504243,
            242919081,
            242620672,
            238287992
        ]
    },
    {
        "id": 224216007,
        "title": "Acoustic Modeling Using Deep Belief Networks",
        "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.",
        "date": "February 2012",
        "authors": [
            "Abdel-rahman Mohamed",
            "George E. Dahl",
            "Geoffrey Hinton"
        ],
        "references": [
            303137904,
            312248705,
            309349955,
            309186321,
            306218037,
            304109482,
            272746011,
            265748773,
            243729901,
            239666378
        ]
    },
    {
        "id": 224149891,
        "title": "Discriminatively estimated joint acoustic, duration, and language model for speech recognition",
        "abstract": "We introduce a discriminative model for speech recognition that integrates acoustic, duration and language components. In the framework of finite state machines, a general model for speech recognition G is a finite state transduction from acoustic state sequences to word sequences (e.g., search graph in many speech recognizers). The lattices from a baseline recognizer can be viewed as an a posteriori version of G after having observed an utterance. So far, discriminative language models have been proposed to correct the output side of G and is applied on the lattices. The acoustic state sequences on the input side of these lattice can also be exploited to improve the choice of the best hypotheses through the lattice. Taking this view, the model proposed in this paper jointly estimates the parameters for acoustic and language components in a discriminative setting. The resulting model can be factored as corrections for the input and the output sides of the general model G. This formulation allows us to incorporate duration cues seamlessly. Empirical results on a large vocabulary Arabic GALE task demonstrate that the proposed model improves word error rate substantially, with a gain of 1.6% absolute. Through a series of experiments we analyze the contributions from and interactions between acoustic, duration and language components to find that duration cues play an important role in Arabic task.",
        "date": "April 2010",
        "authors": [
            "Maider Lehr",
            "Izhak Shafran"
        ],
        "references": [
            224177631,
            221481269,
            220736420,
            220655768,
            240429959,
            222702595,
            222397975,
            4249216
        ]
    },
    {
        "id": 221620610,
        "title": "Offline Arabic Handwriting Recognition with Multidimensional Recurrent Neural Networks",
        "abstract": "Offline handwriting recognition\u2014the transcription of images of handwritten text\u2014is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisti- cated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two re- cent innovations in neural networks\u2014multidimensional recurrent neural networks and connectionist temporal classification\u2014this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is pro- vided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.",
        "date": "January 2008",
        "authors": [
            "Alex Graves",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            249032122,
            228904501,
            221620190,
            221619960,
            309760586,
            243716653,
            240156067,
            232616729,
            230875884,
            222555492
        ]
    },
    {
        "id": 221619960,
        "title": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks.",
        "abstract": "In online handwriting recognition the trajectory of the pen is recorded during writ- ing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisti- cated preprocessing techniques to put the inputs into a more localised form. How- ever these techniques require considerable human effort, and are specific to par- ticular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced re- current neural network with an output layer designed for sequence labelling, com- bined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.",
        "date": "January 2007",
        "authors": [
            "Alex Graves",
            "Santiago Fern\u00e1ndez",
            "Marcus Liwicki",
            "Horst Bunke"
        ],
        "references": [
            224626482,
            221346365,
            220860992,
            13853244,
            292813908,
            285008105,
            240156067,
            230875884,
            222555492,
            7647316
        ]
    },
    {
        "id": 221617663,
        "title": "Forward-backward retraining of recurrent neural networks.",
        "abstract": "This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The network is trained by backpropagation through time. The supervised training algorithm requires target outputs to be provided for each frame. Three methods for deriving these targets are presented. A novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate. 1 Introduction In the field of off-line handwriting recognition, the goal is to read a handwritten document and produce a machine transcription. Such a system could be used for a variety of purposes, from cheque processing and postal sorting to personal correspondence reading for the blind or historical document reading. In a previous publication (Senior 1994) we have descr...",
        "date": "January 1995",
        "authors": [
            "Andrew W. Senior",
            "Tony Robinson"
        ],
        "references": []
    },
    {
        "id": 221484545,
        "title": "Investigation of full-sequence training of deep belief networks for speech recognition",
        "abstract": "Recently, Deep Belief Networks (DBNs) have been proposed for phone recognition and were found to achieve highly competitive performance. In the original DBNs, only frame-level information was used for training DBN weights while it has been known for long that sequential or full-sequence information can be helpful in improving speech recognition accuracy. In this paper we investigate approaches to optimizing the DBN weights, state-to-state transition parameters, and language model scores using the sequential discriminative training criterion. We describe and analyze the proposed training algorithm and strategy, and discuss practical issues and how they affect the final results. We show that the DBNs learned using the sequence-based training criterion outperform those with frame-based criterion using both three-layer and six-layer models, but the optimization procedure for the deeper DBN is more difficult for the former criterion.",
        "date": "September 2010",
        "authors": [
            "Abdel-rahman Mohamed",
            "Dong Yu",
            "li Deng"
        ],
        "references": [
            228871422,
            224567821,
            224150224,
            312468272,
            242405653,
            232650964,
            231919170,
            228763343,
            224640918,
            224567822
        ]
    },
    {
        "id": 227458453,
        "title": "The Minimum Description Length Principle",
        "abstract": "The minimum description length (MDL) principle is a powerful method of inductive inference, the basis of statistical modeling, pattern recognition, and machine learning. It holds that the best explanation, given a limited set of observed data, is the one that permits the greatest compression of the data. MDL methods are particularly well-suited for dealing with model selection, prediction, and estimation problems in situations where the models under consideration can be arbitrarily complex, and overfitting the data is a serious concern. This extensive, step-by-step introduction to the MDL Principle provides a comprehensive reference (with an emphasis on conceptual issues) that is accessible to graduate students and researchers in statistics, pattern classification, machine learning, and data mining, to philosophers interested in the foundations of statistics, and to researchers in other applied sciences that involve model selection, including biology, econometrics, and experimental psychology. Part I provides a basic introduction to MDL and an overview of the concepts in statistics and information theory needed to understand MDL. Part II treats universal coding, the information-theoretic notion on which MDL is built, and part III gives a formal treatment of MDL theory as a theory of inductive inference based on universal coding. Part IV provides a comprehensive overview of the statistical theory of exponential families with an emphasis on their information-theoretic properties. The text includes a number of summaries, paragraphs offering the reader a \"fast track\" through the material, and boxes highlighting the most important concepts.",
        "date": "January 2007",
        "authors": [
            "Peter Daniel Gr\u00fcnwald"
        ],
        "references": [
            319393405,
            316805467,
            309310624,
            284399661,
            268606444,
            266217430,
            259864433,
            255563931,
            239546749,
            239032578
        ]
    },
    {
        "id": 224626482,
        "title": "IAM-OnDB - An on-line English sentence database acquired from handwritten text on a whiteboard",
        "abstract": "In this paper we present IAM-OnDB - a new large online handwritten sentences database. It is publicly available and consists of text acquired via an electronic interface from a whiteboard. The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers. We also describe a recognizer for unconstrained English text that was trained and tested using this database. This recognizer is based on hidden Markov models (HMMs). In our experiments we show that by using larger training sets we can significantly increase the word recognition rate. This recognizer may serve as a benchmark reference for future research.",
        "date": "October 2005",
        "authors": [
            "Marcus Liwicki",
            "H. Bunke"
        ],
        "references": [
            224741547,
            312922200,
            266268339,
            260840446,
            240363829,
            240156067,
            236157558,
            235890211,
            226662568,
            222990759
        ]
    },
    {
        "id": 220320057,
        "title": "Learning Precise Timing with LSTM Recurrent Networks",
        "abstract": "Abstract The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \u201cpeephole connections\u201d from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement,or generation of time intervals. Keywords: Recurrent Neural Networks, Long Short-Term Memory, Timing.",
        "date": "January 2002",
        "authors": [
            "Felix Gers",
            "Nicol N. Schraudolph",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            243781690,
            243683010,
            222482534,
            221619503,
            221480064,
            285008105,
            239028212,
            223164217,
            222806838,
            221620501
        ]
    },
    {
        "id": 220017637,
        "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
        "abstract": "The abstract for this document is available on CSA Illumina.To view the Abstract, click the Abstract button above the document title.",
        "date": "July 2002",
        "authors": [
            "Mitchell Marcus",
            "Mary Ann Marcinkiewicz",
            "Beatrice Santorini"
        ],
        "references": [
            229078364,
            220817275,
            220816719,
            3505297,
            242609803,
            242494825,
            242456331,
            234717687,
            230876022,
            230875928
        ]
    },
    {
        "id": 3302208,
        "title": "An analysis of noise in recurrent neural networks: Convergence and generalization",
        "abstract": "Concerns the effect of noise on the performance of feedforward neural nets. We introduce and analyze various methods of injecting synaptic noise into dynamically driven recurrent nets during training. Theoretical results show that applying a controlled amount of noise during training may improve convergence and generalization performance. We analyze the effects of various noise parameters and predict that best overall performance can be achieved by injecting additive noise at each time step. Noise contributes a second-order gradient term to the error function which can be viewed as an anticipatory agent to aid convergence. This term appears to find promising regions of weight space in the beginning stages of training when the training error is large and should improve convergence on error surfaces with local minima. The first-order term is a regularization term that can improve generalization. Specifically, it can encourage internal representations where the state nodes operate in the saturated regions of the sigmoid discriminant function. While this effect can improve performance on automata inference problems with binary inputs and target outputs, it is unclear what effect it will have on other types of problems. To substantiate these predictions, we present simulations on learning the dual parity grammar from temporal strings for all noise models, and present simulations on learning a randomly generated six-state grammar using the predicted best noise model",
        "date": "December 1996",
        "authors": [
            "Kam-Chuen Jim",
            "C. Lee Giles",
            "William G. Horne"
        ],
        "references": [
            242509302,
            231074804,
            313763061,
            313725459,
            304533936,
            288373415,
            239574281,
            234979932,
            234803006,
            222451035
        ]
    },
    {
        "id": 234809510,
        "title": "Experiments with a multilanguage non-projective dependency parser",
        "abstract": "Parsing natural language is an essential step in several applications that involve document analysis, e.g. knowledge extraction, question answering, summarization, filtering. The best performing systems at the TREC Question Answering track employ parsing for analyzing sentences in order to identify the query focus, to extract relations and to disambiguate meanings of words.",
        "date": "January 2006",
        "authors": [
            "Giuseppe Attardi"
        ],
        "references": [
            267797457,
            260291394,
            232031795,
            228774597,
            260239801,
            242367463,
            239546749,
            234799099,
            225740821,
            223000072
        ]
    },
    {
        "id": 234796040,
        "title": "Very high accuracy and fast dependency parsing is not a contradiction",
        "abstract": "In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation.",
        "date": "January 2010",
        "authors": [
            "Bernd Bohnet"
        ],
        "references": [
            234828197,
            234809510,
            230876751,
            230876749,
            230876740,
            230876736,
            228916842,
            228657745,
            226080200,
            309532479
        ]
    },
    {
        "id": 230876740,
        "title": "The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages",
        "abstract": "For the 11th straight year, the Conference on Computational Natural Language Learn- ing has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntac- tic and semantic dependencies in multiple lan- guages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems.",
        "date": "June 2009",
        "authors": [
            "Jan Haji\u010d",
            "Massimiliano Ciaramita",
            "Richard Johansson",
            "Daisuke Kawahara"
        ],
        "references": [
            252498559,
            234831906,
            230876736,
            228828451,
            228791188,
            228630024,
            312607081,
            247220471,
            243764074,
            228726975
        ]
    },
    {
        "id": 230876736,
        "title": "A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for Multiple Languages",
        "abstract": "Motivated by the large number of languages (seven) and the short development time (two months) of the 2009 CoNLL shared task, we exploited latent variables to avoid the costly process of hand-crafted feature engineering, allowing the latent variables to induce features from the data. We took a pre-existing gener- ative latent variable model of joint syntactic- semantic dependency parsing, developed for English, and applied it to six new languages with minimal adjustments. The parser's ro- bustness across languages indicates that this parser has a very general feature set. The parser's high performance indicates that its la- tent variables succeeded in inducing effective features. This system was ranked third overall with a macro averaged F1 score of 82.14%, only 0.5% worse than the best system.",
        "date": "June 2009",
        "authors": [
            "Andrea Gesmundo",
            "James Henderson",
            "Paola Merlo",
            "Ivan Titov"
        ],
        "references": [
            310793690,
            230876740,
            228630024,
            228629276,
            228627119,
            221013171,
            220874766,
            220874688,
            220813639,
            220746393
        ]
    },
    {
        "id": 268720443,
        "title": "Inductive Dependency Parsing",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "Joakim Nivre"
        ],
        "references": []
    },
    {
        "id": 266034441,
        "title": "Incremental Joint POS Tagging and Dependency Parsing in Chinese",
        "abstract": "We address the problem of joint part-of-speech (POS) tagging and dependency parsing in Chi-nese. In Chinese, some POS tags are often hard to disambiguate without considering long-range syntactic information. Also, the traditional pipeline approach to POS tagging and depen-dency parsing may suffer from the problem of error propagation. In this paper, we propose the first incremental approach to the task of joint POS tagging and dependency parsing, which is built upon a shift-reduce parsing framework with dy-namic programming. Although the incremental approach encounters difficulties with underspeci-fied POS tags of look-ahead words, we overcome this issue by introducing so-called delayed fea-tures. Our joint approach achieved substantial improvements over the pipeline and baseline sys-tems in both POS tagging and dependency pars-ing task, achieving the new state-of-the-art per-formance on this joint task.",
        "date": "January 2011",
        "authors": [
            "Jun Hatori",
            "Takuya Matsuzaki",
            "Yusuke Miyao",
            "Jun'ichi Tsujii"
        ],
        "references": [
            221101429,
            221013254,
            221013125,
            220874833,
            220874503,
            220874372,
            220355552,
            2434823,
            321607348,
            222465819
        ]
    },
    {
        "id": 262355038,
        "title": "Incremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese",
        "abstract": "We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.",
        "date": "July 2012",
        "authors": [
            "Jun Hatori",
            "Takuya Matsuzaki",
            "Yusuke Miyao",
            "Jun'ichi Tsujii"
        ],
        "references": [
            230802652,
            221013254,
            221013125,
            220874856,
            220874833,
            220874503,
            220874372,
            220873331,
            2884220,
            2572548
        ]
    },
    {
        "id": 234828464,
        "title": "Investigating multilingual dependency parsing",
        "abstract": "In this paper, we describe a system for the CoNLL-X shared task of multilingual dependency parsing. It uses a baseline Nivre's parser (Nivre, 2003) that first identifies the parse actions and then labels the dependency arcs. These two steps are implemented as SVM classifiers using LIBSVM. Features take into account the static context as well as relations dynamically built during parsing. We experimented two main additions to our implementation of Nivre's parser: N-best search and bidirectional parsing. We trained the parser in both left-right and right-left directions and we combined the results. To construct a single-head, rooted, and cycle-free tree, we applied the Chu-Liu/Edmonds optimization algorithm. We ran the same algorithm with the same parameters on all the languages.",
        "date": "January 2006",
        "authors": [
            "Richard Johansson",
            "Pierre Nugues"
        ],
        "references": [
            267797457,
            260291394,
            232031795,
            228916842,
            228774597,
            228708442,
            228636931,
            220875062,
            220874766,
            220873277
        ]
    },
    {
        "id": 221481293,
        "title": "Improved clustering techniques for class-based statistical language modelling",
        "abstract": "",
        "date": "January 1993",
        "authors": [
            "Reinhard Kneser",
            "Hermann Ney"
        ],
        "references": []
    },
    {
        "id": 221013125,
        "title": "Joint Models for Chinese POS Tagging and Dependency Parsing",
        "abstract": "Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement.",
        "date": "January 2011",
        "authors": [
            "Zhenghua Li",
            "Min Zhang",
            "Wanxiang Che",
            "Ting Liu"
        ],
        "references": [
            234814829,
            234812849,
            230876740,
            228627119,
            221102937,
            221102044,
            242432988,
            237544016,
            228057950,
            221112561
        ]
    },
    {
        "id": 288905957,
        "title": "SVMTool: A general POS tagger generator based on support vector machines",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "Jesus Gimenez",
            "Llu\u00eds M\u00e0rquez"
        ],
        "references": []
    },
    {
        "id": 285905347,
        "title": "A conditional random field word segmenter",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "H. Tseng",
            "P. Chang",
            "G. Andrew",
            "D. Jurafsky"
        ],
        "references": []
    },
    {
        "id": 269031283,
        "title": "A tale of two parsers",
        "abstract": "Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.",
        "date": "January 2008",
        "authors": [
            "Yue Zhang",
            "Stephen Clark"
        ],
        "references": [
            237533325,
            228916842,
            221012876,
            221012782,
            221012737,
            220873585,
            220873229,
            220873101,
            220816913,
            47465198
        ]
    },
    {
        "id": 234812408,
        "title": "An efficient method for determining bilingual word classes",
        "abstract": "In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation.",
        "date": "June 1999",
        "authors": [
            "Franz Josef Och"
        ],
        "references": [
            230875890,
            221481293,
            221102262,
            220355462,
            220355244,
            243776118,
            222496856,
            222469785,
            3693631,
            3567710
        ]
    },
    {
        "id": 226270700,
        "title": "Bagging Predictors",
        "abstract": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.",
        "date": "August 1996",
        "authors": [
            "Leo Breiman"
        ],
        "references": [
            248284447,
            38359478,
            2770021,
            313728240,
            312817853,
            261583986,
            246596439,
            242389644,
            242376908,
            240310918
        ]
    },
    {
        "id": 221101487,
        "title": "Word-based and Character-based Word Segmentation Models: Comparison and Combination.",
        "abstract": "We present a theoretical and empirical comparative analysis of the two domi- nant categories of approaches in Chinese word segmentation: word-based models and character-based models. We show that, in spite of similar performance over- all, the two models produce different dis- tribution of segmentation errors, in a way that can be explained by theoretical prop- erties of the two models. The analysis is further exploited to improve segmentation accuracy by integrating a word-based seg- menter and a character-based segmenter. A Bootstrap Aggregating model is pro- posed. By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff.",
        "date": "January 2010",
        "authors": [
            "Weiwei Sun"
        ],
        "references": [
            228878707,
            228380054,
            228355560,
            221102138,
            221012740,
            220874650,
            220873561,
            220817420,
            220816644,
            220320111
        ]
    },
    {
        "id": 221013109,
        "title": "A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing.",
        "abstract": "Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies",
        "date": "January 2008",
        "authors": [
            "Yue Zhang",
            "Stephen Clark"
        ],
        "references": []
    },
    {
        "id": 220874244,
        "title": "Revision Learning and its Application to Part-of-Speech Tagging",
        "abstract": "This paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost. This method uses a high capacity model to revise the output of a small cost model. We apply this method to English part-of-speech tagging and Japanese morphological analysis, and show that the method performs well.",
        "date": "January 2002",
        "authors": [
            "Tetsuji Nakagawa",
            "Taku Kudo",
            "Yuji Matsumoto"
        ],
        "references": [
            223459192,
            220706860,
            30379353,
            285278428,
            275270590,
            246542768,
            235961479,
            220694713,
            220691539,
            220343922
        ]
    },
    {
        "id": 220482445,
        "title": "Parsing English with a Link Grammar",
        "abstract": "We develop a formal grammatical system called a link grammar, show how English grammar can be encoded in such a system, and give algorithms for efficiently parsing with a link grammar. Although the expressive power of link grammars is equivalent to that of context free grammars, encoding natural language grammars appears to be much easier with the new system. We have written a program for general link parsing and written a link grammar for the English language. The performance of this preliminary system -- both in the breadth of English phenomena that it captures and in the computational resources used -- indicates that the approach may have practical uses as well as linguistic significance. Our program is written in C and may be obtained through the internet.",
        "date": "January 1995",
        "authors": [
            "Daniel Sleator",
            "David Temperley"
        ],
        "references": [
            220423031,
            279373704,
            259344042,
            250753641,
            247589435,
            247442902,
            239958307,
            230876316,
            228057764,
            227254886
        ]
    },
    {
        "id": 301864384,
        "title": "Three New Probabilistic Models for Dependency Parsing: An Exploration",
        "abstract": "After presenting a novel O(n^3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative (i.e., top-down) model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.",
        "date": "June 1997",
        "authors": [
            "Jason Eisner"
        ],
        "references": [
            242394905,
            220875062,
            220873033,
            220482445,
            2292363,
            307175021,
            260730920,
            246769833,
            230875928,
            224619517
        ]
    },
    {
        "id": 260730920,
        "title": "An Empirical Comparison of Probability Models for Dependency Grammar",
        "abstract": "This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper, with details of how the results were obtained. Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependency grammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n3) parsing algorithm. The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences. As reported at the talk, the more extensive training yields greatly improved performance, cutting in half the error rate of Eisner (1996). Nearly half the sentences are parsed with no misattachments; two-thirds of sentences are parsed with at most one misattachment. Of the models described in the original paper, the best score is obtained with the generative \"model C\", which attaches 87-88% of all words to the correct parent. However, better models are also explored, in particular, two simple variants on the comprehension \"model B.\" The better of these has an attachment accuracy of 93% and (unlike model C) tags words more accurately than the comparable trigram tagger. If tags are roughly known in advance, search error is all but eliminated and the new model attains an attachment accuracy of 93%. We find that the parser of Collins (1996) when combined with a highly, trained tagger, also achieves 93% when trained and tested on the same sentences. We briefly discuss the similarities and differences between Collins's model and ours, pointing out the strengths of each and noting that these strengths could be combined for either dependency parsing or phrase-structure parsing.",
        "date": "June 1997",
        "authors": [
            "Jason M Eisner"
        ],
        "references": [
            220482445,
            2683860,
            312607081,
            248593234,
            238198632,
            230876410,
            230875928,
            221101482,
            220874091,
            2454922
        ]
    },
    {
        "id": 221101482,
        "title": "Three New Probabilistic Models for Dependency Parsing: An Exploration.",
        "abstract": "After presenting a novel O(n^3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative (i.e., top-down) model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.",
        "date": "January 1996",
        "authors": [
            "Jason Eisner"
        ],
        "references": []
    },
    {
        "id": 220874091,
        "title": "A New Statistical Parser Based on Bigram Lexical Dependencies",
        "abstract": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95, Jelinek et al 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.",
        "date": "May 1996",
        "authors": [
            "Michael Collins"
        ],
        "references": [
            234799945,
            220873033,
            220817598,
            220816736,
            220017637,
            30379057,
            2473899,
            2472456,
            2292363,
            312607081
        ]
    },
    {
        "id": 220343911,
        "title": "Learning to Parse Natural Language with Maximum Entropy Models",
        "abstract": "This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In practice, the running time of the parser on a test sentence is linear with respect to the sentence length. We also demonstrate that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn. Furthermore, this paper shows that research into rescoring the top 20 parses returned by the parser might yield accuracies dramatically higher than the state-of-the-art.",
        "date": "February 1999",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "references": [
            234819841,
            220873033,
            220817598,
            220816736,
            30379057,
            3192689,
            2480097,
            261344688,
            242821088,
            239353830
        ]
    },
    {
        "id": 200033835,
        "title": "The Nature of Statistical Learning Theory",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "Vapnik",
            "Nahaba Vladimir"
        ],
        "references": []
    },
    {
        "id": 2538570,
        "title": "A Maximum-Entropy-Inspired Parser",
        "abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] \"stan- dard\" sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innova- tion is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.",
        "date": "May 2002",
        "authors": [
            "Eugene Charniak"
        ],
        "references": [
            2582686,
            2480097,
            260730920,
            38363908,
            2746025,
            2612734,
            2481752,
            2474245,
            2296449
        ]
    },
    {
        "id": 221013189,
        "title": "A Boosting Algorithm for Classification of Semi-Structured Text.",
        "abstract": "The focus of research in text classification has ex- panded from simple topic identification to more challenging tasks such as opinion/modality identi- fication. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representa- tion is required. Accordingly, learning algorithms must be created that can handle the structures ob- served in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting al- gorithm which employs the subtree-based decision stumps as weak learners. We also discuss the rela- tion between our algorithm and SVMs with tree ker- nel. Two experiments on opinion/modality classifi- cation confirm that subtree features are important.",
        "date": "January 2004",
        "authors": [
            "Taku Kudo",
            "Yuji Matsumoto"
        ],
        "references": [
            248832100,
            224952248,
            220343914,
            271308325,
            243134514,
            225070191,
            221996125,
            221653350,
            221344764,
            220875022
        ]
    },
    {
        "id": 220817026,
        "title": "Chunking with Support Vector Machines",
        "abstract": "In this paper, we apply Support Vector Machines (SVMs) to identify English base phrases (chunks). It is well-known that SVMs achieve high generalization perfor- mance even using input data with a high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out training with smaller com- putational cost independent of the dimensionality of the feature space. In order to improve accuracy, we also apply majority voting with 8 SVMs which are trained using distinct chunk representations. Experimental results show that our approach achieves better accuracy than other conventional frameworks.",
        "date": "June 2001",
        "authors": [
            "Taku Kudo",
            "Yuji Matsumoto"
        ],
        "references": [
            221604720,
            221344849,
            2459325,
            285278428,
            244498683,
            230876652,
            200033835,
            2566008,
            2539784,
            2461310
        ]
    },
    {
        "id": 221013334,
        "title": "Induction of Greedy Controllers for Deterministic Treebank Parsers.",
        "abstract": "Most statistical parsers have used the gram- mar induction approach, in which a stochastic grammar is induced from a treebank. An alter- native approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy con- trollers, which result in deterministic parsers. We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust, considering their speed and simplic- ity. They are almost as fast as current part-of- speech taggers, and considerably more accurate than a basic unlexicalized PCFG parser. We also describe Markov parsing models, a general framework for parser modeling and control, of which the parsers reported here are a special case.",
        "date": "January 2004",
        "authors": [
            "Tom Kalt"
        ],
        "references": [
            220017637,
            220017613,
            2921800,
            2473122,
            1782255,
            301867541,
            275270590,
            238312866,
            230876410,
            221303699
        ]
    },
    {
        "id": 2867292,
        "title": "Learning A Lightweight Robust Deterministic Parser",
        "abstract": "We describe a method for automatically learning a parser from labeled, bracketed corpora that results in a fast, robust, lightweight parser that is suitable for real-time dialog systems and similar applications. Unlike ordinary parsers, all grammatical knowledge is captured in the learned decision trees, so no explicit phrase-structure grammar is needed.",
        "date": "July 2003",
        "authors": [
            "Aboy Wong",
            "De Kai"
        ],
        "references": [
            220483796,
            2617139,
            2473899,
            1782255,
            312607081,
            247218600,
            239353830,
            220874091,
            37599853,
            2819188
        ]
    },
    {
        "id": 2547815,
        "title": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models",
        "abstract": "This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.",
        "date": "December 2002",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "references": [
            220875062,
            220873033,
            220817598,
            220816736,
            220017637,
            2480097,
            230876410,
            220874091,
            38363908,
            32205961
        ]
    },
    {
        "id": 2540346,
        "title": "The Necessity of Parsing for Predicate Argument Recognition",
        "abstract": "Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification.",
        "date": "October 2002",
        "authors": [
            "Daniel Gildea",
            "Martha Palmer"
        ],
        "references": [
            242594981,
            2635195,
            1782977,
            242402280,
            228057950,
            220017566,
            2803418,
            2571435,
            2529190,
            2416122
        ]
    },
    {
        "id": 221013081,
        "title": "Improving Dependency Parsing with Subtrees from Auto-Parsed Data",
        "abstract": "This paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto-parsed data. First, we use a baseline parser to parse large-scale unannotated data. Then we ex- tract subtrees from dependency parse trees in the auto-parsed data. Finally, we con- struct new subtree-based features for pars- ing algorithms. To demonstrate the ef- fectiveness of our proposed approach, we present the experimental results on the En- glish Penn Treebank and the Chinese Penn Treebank. These results show that our ap- proach significantly outperforms baseline systems. And, it achieves the best accu- racy for the Chinese data and an accuracy which is competitive with the best known systems for the English data.",
        "date": "January 2009",
        "authors": [
            "Wenliang Chen",
            "Jun'ichi Kazama",
            "Kiyotaka Uchimoto",
            "Kentaro Torisawa"
        ],
        "references": [
            237533325,
            234814829,
            228916842,
            228916032,
            228638915,
            228527702,
            221300315,
            221112561,
            221102943,
            221102894
        ]
    },
    {
        "id": 221012688,
        "title": "Self-Training with Products of Latent Variable Grammars.",
        "abstract": "We study self-training with products of latent variable grammars in this paper. We show that increasing the quality of the automatically parsed data used for self-training gives higher accuracy self-trained grammars. Our generative self-trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self-training. Additionally, we show that multiple self-trained grammars can be combined in a product model to achieve even higher accuracy. The product model is most effective when the individual underlying grammars are most diverse. Combining multiple grammars that were self-trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5% on the WSJ test set and 89.6% on our Broadcast News test set.",
        "date": "December 2010",
        "authors": [
            "Zhongqiang Huang",
            "Mary Harper",
            "Slav Petrov"
        ],
        "references": [
            262408350,
            228817682,
            221013249,
            281309188,
            266489133,
            256476402,
            237252052,
            233565189,
            226270700,
            221013263
        ]
    },
    {
        "id": 220873576,
        "title": "Web-Scale Features for Full-Scale Parsing.",
        "abstract": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
        "date": "January 2011",
        "authors": [
            "Mohit Bansal",
            "Dan Klein"
        ],
        "references": [
            269087874,
            247458245,
            221101689,
            268239449,
            262349927,
            221012931,
            221012880,
            220946924,
            220875180,
            220874891
        ]
    },
    {
        "id": 237533325,
        "title": "CoNLL-X Shared Task on Multilingual Dependency Parsing",
        "abstract": "Each year the Conference on Computational Natural Language Learning (CoNLL)1 features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multi-lingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?",
        "date": "January 2006",
        "authors": [
            "Sabine Buchholz",
            "Erwin Marsi"
        ],
        "references": [
            267797457,
            324385343,
            313024706,
            307175021,
            260239801,
            255671454,
            243776002,
            242367463,
            241360780,
            239223595
        ]
    },
    {
        "id": 221013054,
        "title": "Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Kenji Sagae",
            "Jun'ichi Tsujii"
        ],
        "references": [
            243532847,
            324352101,
            312607081,
            278651717,
            262184200,
            249903651,
            247921836,
            247889560,
            242370758,
            238319783
        ]
    },
    {
        "id": 221012782,
        "title": "Characterizing the Errors of Data-Driven Dependency Parsing Models.",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Ryan T. McDonald",
            "Joakim Nivre"
        ],
        "references": [
            237533325,
            228854286,
            220946858,
            220874766,
            220873101,
            307174938,
            234829084,
            221618883,
            220817360,
            220817037
        ]
    },
    {
        "id": 220874856,
        "title": "An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging.",
        "abstract": "In this paper, we present a discriminative word-character hybrid model for joint Chi- nese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error- driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training cor- pus. We describe an efficient framework for training our model based on the Mar- gin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves supe- rior performance compared to the state-of- the-art approaches reported in the litera- ture.",
        "date": "January 2009",
        "authors": [
            "Canasai Kruengkrai",
            "Kiyotaka Uchimoto",
            "Jun'ichi Kazama",
            "Yiou Wang"
        ],
        "references": [
            230662019,
            228630734,
            221101429,
            247057627,
            234810260,
            230876765,
            230875825,
            228776070,
            228603853,
            221101961
        ]
    },
    {
        "id": 220874801,
        "title": "Reranking and Self-Training for Parser Adaptation.",
        "abstract": "Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard \"Charniak parser\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus.This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.",
        "date": "January 2006",
        "authors": [
            "David McClosky",
            "Eugene Charniak",
            "Mark Johnson"
        ],
        "references": [
            262408350,
            220017637,
            262349927,
            243764074,
            230876697,
            228769981,
            223311595,
            221324370,
            220875180,
            220343911
        ]
    },
    {
        "id": 220873136,
        "title": "Cross Language Dependency Parsing using a Bilingual Lexicon.",
        "abstract": "This paper proposes an approach to en- hance dependency parsing in a language by using a translated treebank from an- other language. A simple statistical ma- chine translation method, word-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an en- semble method, the key information ex- tracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target lan- guage. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-of- the-art result.",
        "date": "January 2009",
        "authors": [
            "Hai Zhao",
            "Yan Song",
            "Chunyu Kit",
            "Guodong Zhou"
        ],
        "references": [
            230876750,
            228916842,
            228916032,
            221013054,
            221012782,
            221012737,
            234799099,
            228527702,
            221102894,
            221013118
        ]
    },
    {
        "id": 220873101,
        "title": "Online Large-Margin Training of Dependency Parsers.",
        "abstract": "We present an effective training al- gorithm for linearly-scored dependency parsers that implements online large- margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for de- pendency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.",
        "date": "January 2005",
        "authors": [
            "Ryan T. McDonald",
            "Koby Crammer",
            "Nanediri Chathumi Fernando"
        ],
        "references": [
            230662019,
            228916842,
            228707544,
            324620513,
            313037830,
            312607081,
            268237865,
            243763843,
            238625098,
            221619279
        ]
    },
    {
        "id": 221101811,
        "title": "A Computational Model Of Language Performance: Data Oriented Parsing.",
        "abstract": "Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing. Analyzing a new input means that the system attempts to find the most probable way to reconstruct the input out of fragments that already exist in the corpus. Disambiguation occurs as a side-effect DOP can be implemented by using conventional parsing strategies.",
        "date": "January 1992",
        "authors": [
            "Rens Bod"
        ],
        "references": [
            242394905,
            234799945,
            230875997,
            224377724,
            269855697,
            247370860,
            242368649,
            3533067
        ]
    },
    {
        "id": 220875062,
        "title": "Towards History-Based Grammars: Using Richer Models for Probabilistic Parsing.",
        "abstract": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.",
        "date": "January 1993",
        "authors": [
            "Ezra Black",
            "Frederick Jelinek",
            "John D. Lafferty",
            "David M. Magerman"
        ],
        "references": [
            220355244,
            3505297,
            267836625,
            261344688,
            242361757,
            237109467,
            234831828,
            234819176,
            220816781,
            3999023
        ]
    },
    {
        "id": 220874982,
        "title": "D-Tree Grammars.",
        "abstract": "DTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.",
        "date": "May 1995",
        "authors": [
            "Owen Rambow",
            "K. Vijay-Shanker",
            "David J. Weir"
        ],
        "references": [
            245516689,
            234790657,
            319395098,
            304049762,
            300126299,
            293224008,
            289680116,
            243764177,
            242374595,
            238727771
        ]
    },
    {
        "id": 220873033,
        "title": "Statistical Decision-Tree Models for Parsing",
        "abstract": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {$n$}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86\\% precision, 86\\% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91\\% precision, 90\\% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.",
        "date": "April 1995",
        "authors": [
            "David M. Magerman"
        ],
        "references": [
            220817598,
            220355244,
            1782255,
            310606656,
            303517845,
            242396917,
            220049264,
            3175558
        ]
    },
    {
        "id": 220816862,
        "title": "Parsing the Voyager Domain Using Pearl.",
        "abstract": "This paper* describes a .al, ural language p~rsi.g algorithm &)r un- resu'icl,ed I,ext which uses a probabilhq-hased scoring funct,iow I,o se- le(:l, I,he \"besC: parse of ~/ sent,enos acc:ording t,c~ a given gra0nunar. The parser~ \"Pearl~ ix a i,hne-asynciironous t)ol,l,orn-u 1) chart, parser with Earley-l,ype I,Ol)-dowil t)redic:l,ion which pursues I,he }6gtmsl,-s(:ori.g t,he- ory in I,he (:|larl,, where I,he set)re of a IJmory represe.l,s I,|le exl,e.l, I,o W|lic:|l L}le ~:on|,exl, ()~ I,|le seutl,euic:e predic:l;s I,}lat, int,erprel,at,ion. This parser {lifters front previous au,eUnl)l,s el, sto(:}la.st,i(: parsers in I,|lal, it, .ses a richer h)rm of condii,ional probabilil, ies |)a.~ed on conl,exl, l,o predict, likelihood. Pearl Mso provides a framework for in(:orporat,ing the resuh,s of previo.s work in part,-of-st)eeeh assig.tnenl,) InlkutowIi word u||od- els, and olJher l)rol)abilist,ic models (ff linguistfic feauJres im,o o.e pars- i.g U)t)), inl,erleaving I,}lese I,e(:imiques |.stead of usi.g I, he I,ra~lil,iona) pipeline archil,eci, ure. In I, esl~ perh~ruvled on I,|ie Voyager (lirecl,io.- finding domain, \"Pearl has been s.ccessful el, resolvi.g parl,-of-speech aunifiguhq, del,ermiufing cal,egories for umknow, words, and selecl,ing correcl, parses firsl, using a very loosely fil, l,lng covering grammar. ~",
        "date": "January 1991",
        "authors": [
            "David M. Magerman",
            "Mitchell Marcus"
        ],
        "references": [
            234828877,
            267836625,
            261344688,
            242609883,
            239365777,
            237109467,
            3178307
        ]
    },
    {
        "id": 220708270,
        "title": "Automatic Learning for Semantic Collocation.",
        "abstract": "The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering \"knowledge\". In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among \"words\" from sample corpora.The algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences, by a statistical approach. The algorithm requires a corpus and minimum linguistic knowledge (parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules).We conducted two experiments of applying the algorithm to diferent corpora to extract different types of semantic collocations. Though there are some unsolved problems, the results showed the effectiveness of the proposed algorithm.",
        "date": "January 1992",
        "authors": [
            "Satoshi Sekine",
            "Jeremy Carroll",
            "Sophia Ananiadou",
            "Jun'ichi Tsujii"
        ],
        "references": [
            234825624,
            221102382,
            2472456
        ]
    },
    {
        "id": 220685657,
        "title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression",
        "abstract": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known",
        "date": "July 1991",
        "authors": [
            "Ian Witten",
            "Timothy C. Bell"
        ],
        "references": [
            313184847,
            295407737,
            277509458,
            243766556,
            242580964,
            241515560,
            238757999,
            236396276,
            236340551,
            224104359
        ]
    },
    {
        "id": 220017613,
        "title": "Fast Exact Inference with a Factored Model for Natural Language Parsing",
        "abstract": "",
        "date": "January 2002",
        "authors": [
            "Dan Klein",
            "Christopher D. Manning"
        ],
        "references": [
            220873033,
            2850230,
            307175021,
            304109482,
            262349927,
            230875831,
            220874091,
            220355542,
            11207765,
            2628618
        ]
    },
    {
        "id": 216300475,
        "title": "Empirical Methods for Artificial Intelligence",
        "abstract": "Planning systems generate partially ordered sequences of actions (or plans) that solve a goal. They start from a specification of the valid actions (also called operators), which includes both the conditions under which an action applies (the preconditions) ...",
        "date": "January 1995",
        "authors": [
            "Paul R. Cohen"
        ],
        "references": []
    },
    {
        "id": 319770436,
        "title": "Efficient backprop",
        "abstract": "The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most \u201cclassical\u201d second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
        "date": "January 1998",
        "authors": [
            "Yann A LeCun",
            "L\u00e9on Bottou",
            "Genevieve B. Orr",
            "Klaus-Robert M\u00fcller"
        ],
        "references": []
    },
    {
        "id": 289690359,
        "title": "Factored soft source syntactic constraints for hierarchical machine translation",
        "abstract": "This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.",
        "date": "January 2013",
        "authors": [
            "Z. Huang",
            "J. Devlin",
            "R. Zbib"
        ],
        "references": [
            228355130,
            220874793,
            247645735
        ]
    },
    {
        "id": 287008400,
        "title": "An efficient language model using double-array structures",
        "abstract": "Ngram language models tend to increase in size with inflating the corpus size, and consume considerable resources. In this paper, we propose an efficient method for implementing ngram models based on double-array structures. First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.",
        "date": "January 2013",
        "authors": [
            "M. Yasuhara",
            "T. Tanaka",
            "J.-Y. Norimatsu",
            "M. Yamamoto"
        ],
        "references": [
            221013245,
            220770303,
            3187145
        ]
    },
    {
        "id": 285432248,
        "title": "Decoding with large-scale neural language models improves translation",
        "abstract": "We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.",
        "date": "January 2013",
        "authors": [
            "A. Vaswani",
            "Y. Zhao",
            "V. Fossum",
            "David Chiang"
        ],
        "references": [
            228882059,
            220733157,
            229100231,
            221480736
        ]
    },
    {
        "id": 283112108,
        "title": "Joint language and translation modeling with recurrent neural networks",
        "abstract": "We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.",
        "date": "January 2013",
        "authors": [
            "M. Auli",
            "Michel Galley",
            "C. Quirk",
            "G. Zweig"
        ],
        "references": [
            220732242,
            259080183,
            255563511,
            227313911,
            221013063,
            220344134
        ]
    },
    {
        "id": 277186684,
        "title": "Lexical Features for Statistical Machine Translation",
        "abstract": "",
        "date": "January 2009",
        "authors": [
            "Jacob Devlin"
        ],
        "references": [
            230875890,
            221102262,
            221101300,
            221013262,
            221012593,
            220873967,
            220873584,
            220873071,
            220816767,
            220685657
        ]
    },
    {
        "id": 220816913,
        "title": "Statistical Phrase-Based Translation.",
        "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.",
        "date": "January 2003",
        "authors": [
            "Philipp Koehn",
            "Franz Josef Och",
            "Daniel Marcu"
        ],
        "references": [
            230875890,
            220873497,
            2930546,
            248515926,
            246586079,
            230876149,
            230875959,
            228609491,
            222670794,
            2895052
        ]
    },
    {
        "id": 265657436,
        "title": "Automatic Sentence Segmentation and Punctuation Prediction for Spoken Language Translation",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "Evgeny Matusov",
            "Arne Mauser",
            "Hermann Ney"
        ],
        "references": [
            238580974,
            228339721,
            221490369,
            221480377,
            4084657,
            2887790,
            2876877,
            234812513,
            221490784,
            221481935
        ]
    },
    {
        "id": 237327924,
        "title": "Continuous space language models for the IWSLT 2006 task",
        "abstract": "The language model of the target language plays an impor- tant role in statistical machine translation systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projec- tion and the probability estimation. This kind of approach is in particular promising for tasks where a very limited amount of resources are available, like the BTEC corpus of tourism related questions. This language model is used in two state-of-the-art sta- tistical machine translation systems that were developed by UPC for the 2006 IWSLT evaluation campaign: a phrase- and an n-gram-based approach. An experimental evaluation for four different language pairs is provided (translation of Man- darin, Japanese, Arabic and Italian to English). The proposed method achieved improvements in the BLEU score of up to 3 points on the development data and of almost 2 points on the ofcial test data.",
        "date": "January 2006",
        "authors": [
            "Holger Schwenk",
            "Marta R. Costa-juss",
            "Jos\u00e9 A. R. Fonollosa"
        ],
        "references": [
            250880453,
            247921837,
            238281080,
            237294563,
            234809800,
            234801412,
            228934789,
            228463759,
            255563974,
            243777055
        ]
    },
    {
        "id": 237247141,
        "title": "The TALP Ngram-based SMT system for IWSLT 2007",
        "abstract": "This paper describes TALPtuples, the 2007 N-gram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Politecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years. Mainly, these include optimizing alignment parameters in function of translation metric scores and rescoring with a neural network language model. Results on two translation directions are reported, namely from Arabic and Chinese into English, thoroughly explaining all language-related preprocessing and translation schemes.",
        "date": "January 2007",
        "authors": [
            "Patrik Lambert",
            "Josep Maria Crego",
            "Maxim Khalilov",
            "B. Mari"
        ],
        "references": [
            251889356,
            250083420,
            237327924,
            234817432,
            228986234,
            224062186,
            221491005,
            265414448,
            234812408,
            228342831
        ]
    },
    {
        "id": 237247130,
        "title": "The TALP-UPC Ngram-based statistical machine translation system for ACL-WMT 2008",
        "abstract": "This paper reports on the participation of the TALP Research Center of the UPC (Universitat Polit\u00e8cnica de Catalunya) to the ACL WMT 2008 evaluation campaign. This year's system is the evolution of the one we em- ployed for the 2007 campaign. Main updates and extensions involve linguistically motivated word re- ordering based on the reordering patterns technique. In addition, this system introduces a target language model, based on linguistic classes (Part-of-Speech), morphology reduction for an inflectional language (Spanish) and an improved optimization procedure. Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented.",
        "date": "January 2008",
        "authors": [
            "Maxim Khalilov",
            "Marta Ruiz Costa-jussa",
            "Josep Maria Crego",
            "Carlos A. Henr\u00edquez Q."
        ],
        "references": [
            241185825,
            234832376,
            234766585,
            228976391,
            220873497,
            220418830,
            220355462,
            33422155,
            242402964,
            2538294
        ]
    },
    {
        "id": 234815897,
        "title": "A statistical approach to language translation",
        "abstract": "An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French.Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence.We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences.",
        "date": "August 1988",
        "authors": [
            "P. Brown",
            "J. Cocke",
            "Stephen Andrew Della Pietra",
            "Vincent joseph Dellapietra"
        ],
        "references": [
            310606656,
            242494825,
            234803144,
            221995817,
            221101757,
            3081191
        ]
    },
    {
        "id": 270878336,
        "title": "The Role of Syntax in Vector Space Models of Compositional Semantics",
        "abstract": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.",
        "date": "August 2013",
        "authors": [
            "Karl Moritz Hermann",
            "Phil Blunsom"
        ],
        "references": [
            267244277,
            228533748,
            225899934,
            220875209,
            220355331,
            3652002,
            51247552,
            2571940
        ]
    },
    {
        "id": 255563511,
        "title": "Context Dependent Recurrent Neural Network Language Model",
        "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.",
        "date": "December 2012",
        "authors": [
            "Tomas Mikolov",
            "Geoffrey Zweig"
        ],
        "references": [
            261091341,
            261091127,
            241637478,
            228828379,
            228677164,
            224246503,
            221618573,
            221489926,
            221013094,
            220733157
        ]
    },
    {
        "id": 3316656,
        "title": "Bidirectional recurrent neural networks",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported",
        "date": "December 1997",
        "authors": [
            "Mike Schuster",
            "Kuldip K. Paliwal"
        ],
        "references": [
            325817538,
            283749673,
            243778956,
            238833309,
            271512994,
            269295635,
            243743707,
            242919081,
            230876435,
            230876429
        ]
    },
    {
        "id": 258627767,
        "title": "Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost",
        "abstract": "We are interested in large-scale image classification and especially in the setting where images corresponding to new or existing classes are continuously added to the training set. Our goal is to devise classifiers which can incorporate such images and classes on-the-fly at (near) zero cost. We cast this problem into one of learning a metric which is shared across all classes and explore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers. We learn metrics on the ImageNet 2010 challenge data set, which contains more than 1.2M training images of 1K classes. Surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier, and has comparable performance to linear SVMs. We also study the generalization performance, among others by using the learned metric on the ImageNet-10K dataset, and we obtain competitive performance. Finally, we explore zero-shot classification, and show how the zero-shot model can be combined very effectively with small training datasets.",
        "date": "October 2012",
        "authors": [
            "Thomas Mensink",
            "Jakob Verbeek",
            "Florent Perronnin",
            "Gabriela Csurka"
        ],
        "references": [
            259882036,
            227943302,
            224254911,
            224135977,
            307065704,
            303517922,
            266041543,
            239317040,
            228671354,
            223621838
        ]
    },
    {
        "id": 40443832,
        "title": "A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation",
        "abstract": "While many models of biological object recognition share a common set of \"broad-stroke\" properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model--e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct \"parts\" have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3's IBM Cell Processor). In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.",
        "date": "November 2009",
        "authors": [
            "Nicolas Pinto",
            "David Doukhan",
            "James J Dicarlo",
            "David Cox"
        ],
        "references": [
            260687001,
            306150595,
            274202844,
            266476598,
            263269643,
            243716653,
            227633811,
            227040019,
            224982370,
            224386440
        ]
    },
    {
        "id": 236735729,
        "title": "No More Pesky Learning Rates",
        "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can in-crease as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively re-moves the need for learning rate tuning.",
        "date": "January 2013",
        "authors": [
            "Tom Schaul",
            "Sixn Zhang",
            "Yann Lecun"
        ],
        "references": [
            266463521,
            236736773,
            234131303,
            221618614,
            265748773,
            247931959,
            238293187,
            236736845,
            236736791,
            228095666
        ]
    },
    {
        "id": 225273758,
        "title": "No More Pesky Learning Rates",
        "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",
        "date": "June 2012",
        "authors": [
            "Tom Schaul",
            "Sixn Zhang",
            "Yann Lecun"
        ],
        "references": [
            266463521,
            236736773,
            234131303,
            265748773,
            247931959,
            238293187,
            236736845,
            236736791,
            234229021,
            228095666
        ]
    },
    {
        "id": 216792889,
        "title": "Improving the Convergence of Back-Propagation Learning with Second-Order Methods",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "Suzanna Becker",
            "Yann Lecun"
        ],
        "references": []
    },
    {
        "id": 236736791,
        "title": "A Stochastic Approximation Method",
        "abstract": "Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \\theta$ of the equation $M(x) = \\alpha$, where $\\alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\\cdots$ in such a way that $x_n$ will tend to $\\theta$ in probability.",
        "date": "September 1951",
        "authors": [
            "Herbert Robbins",
            "Sutton Monro"
        ],
        "references": []
    },
    {
        "id": 221497515,
        "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.",
        "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",
        "date": "January 2010",
        "authors": [
            "John C. Duchi",
            "Elad Hazan",
            "Yoram Singer"
        ],
        "references": [
            228699264,
            225220238,
            225168001,
            221620124,
            221619907,
            221497723,
            221497703,
            221497543,
            221361415,
            220320442
        ]
    },
    {
        "id": 234795504,
        "title": "Shared task: statistical machine translation between European languages",
        "abstract": "The ACL-2005 Workshop on Parallel Texts hosted a shared task on building statistical machine translation systems for four European language pairs: French-English, German-English, Spanish-English, and Finnish-English. Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis.",
        "date": "June 2005",
        "authors": [
            "Philipp Koehn",
            "Christof Monz"
        ],
        "references": [
            230875890,
            229023614,
            220874161,
            220816913,
            307174794,
            230875959,
            228379274,
            223689862,
            221013260,
            220839665
        ]
    },
    {
        "id": 220873989,
        "title": "A Program for Aligning Sentences in Bilingual Corpora",
        "abstract": "Researchers in both machine translation (e.g., Brown ., 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English). This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small trilingual sample of Swiss economic reports. A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI.",
        "date": "January 1991",
        "authors": [
            "William A. Gale",
            "Kenneth Ward Church"
        ],
        "references": [
            234815897,
            221510305,
            221101783
        ]
    },
    {
        "id": 220839665,
        "title": "Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models",
        "abstract": "We describe Pharaoh, a freely available decoder for phrase-based statistical machine translation models. The decoder is the implement at ion of an efficient dynamic programming search algorithm with lattice generation and XML markup for external components.",
        "date": "September 2004",
        "authors": [
            "Philipp Koehn"
        ],
        "references": [
            230876701,
            230875890,
            228972288,
            228972286,
            228959549,
            228948343,
            228876001,
            228762954,
            228606216,
            228604352
        ]
    },
    {
        "id": 2457211,
        "title": "Combining Labeled and Unlabeled Data with Co-Training",
        "abstract": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm 's predictions on new unlabeled examples are used to e...",
        "date": "October 2000",
        "authors": [
            "Avrim Blum",
            "Tom Mitchell"
        ],
        "references": [
            262168534,
            2457904,
            222480440,
            3027121,
            2906430
        ]
    },
    {
        "id": 303147113,
        "title": "Compositional Semantics With Lexicalized Tree-Adjoining Grammar (LTAG): How Much Underspecification is Necessary?",
        "abstract": "Let us consider a possible implication of the lexicalization of grammatical structures and the localization of dependencies (especially the predicate- argument relationships) that are central features of LTAG. Consider the elementary trees in the LTAG in Figure 1.. The tree corresponding to John likes peanuts passionately is derived by starting with the elementary tree for likes and then substituting the trees for John and peanuts at the respective nodes of the tree \u03b11 and adjoining the tree for passionately at the VP node of the tree \u03b11. The derivation tree in Figure 1. shows this derivation. If both substitution and adjoining are described as attachment of one tree to another tree, then the entire derivation consists of a set of attachments.",
        "date": "January 2001",
        "authors": [
            "Aravind K. Joshi",
            "K. Vijay-Shanker"
        ],
        "references": [
            234790657,
            220874982,
            2665531,
            2271138,
            319395206,
            319395098,
            319394931,
            307175241,
            300126299,
            245775820
        ]
    },
    {
        "id": 247442902,
        "title": "Introduction To Algorithms",
        "abstract": "",
        "date": "January 2001",
        "authors": [
            "Thomas H. Cormen",
            "Charles E. Leiserson",
            "Ronald Rivest",
            "Clifford Stein"
        ],
        "references": []
    },
    {
        "id": 263890871,
        "title": "Unsupervised language model adaptation",
        "abstract": "This paper investigates unsupervised language model adaptation, from ASR transcripts. N-gram counts from these transcripts can be used either to adapt an existing n-gram model or to build an n-gram model from scratch. Various experimental results are re-ported on a particular domain adaptation task, namely building a customer care application starting from a general voicemail tran-scription system. The experiments investigate the effectiveness of various adaptation strategies, including iterative adaptation and self-adaptation on the test data. They show an error rate reduction of 3.9% over the unadapted baseline performance, from 28% to 24.1%, using 17 hours of unsupervised adaptation material. This is 51% of the 7.7% adaptation gain obtained by supervised adap-tation. Self-adaptation on the test data resulted in a 1.3% improve-ment over the baseline.",
        "date": "May 2003",
        "authors": [
            "Michiel Bacchiani",
            "Brian Roark"
        ],
        "references": [
            3908393,
            3333325,
            308670916,
            303517922,
            257267653,
            243774218,
            229078317,
            3908400,
            3908356,
            2812975
        ]
    },
    {
        "id": 220475988,
        "title": "Automated Natural Spoken Dialog.",
        "abstract": "Traditional menu-driven speech recognition systems force users to learn the machine's jargon, but many people can't or won't navigate such highly structured interactions. AT&T's \"How May I Help You?\" technology shifts the burden to the machine by requiring it to adapt to human language and understand what people actually say rather than what a system designer expects them to say. For a given task, it is more crucial to recognize and understand some linguistic events than others. The authors have developed algorithms that automatically learn the salient words, phrases, and grammar fragments for a given task far more reliably than other methods.",
        "date": "April 2002",
        "authors": [
            "Allen L. Gorin",
            "Alicia Abella",
            "Tirso Alonso",
            "Giuseppe Riccardi"
        ],
        "references": [
            271452206,
            269079064,
            230876077,
            221607780,
            221479739,
            220419137,
            260692505,
            223836764,
            222573864,
            222499956
        ]
    },
    {
        "id": 220355374,
        "title": "Estimation of Probabilistic Context-Free Grammars.",
        "abstract": "The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. The condition for proper assignment is rather subtle. Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper. We show here that estimated production probabilities always yield proper distributions.",
        "date": "June 1998",
        "authors": [
            "Zhiyi Chi",
            "Stuart Geman"
        ],
        "references": [
            270313024,
            268974723,
            252797252,
            243765832,
            230875831,
            228057764,
            221995817,
            220566342,
            220049264,
            50335869
        ]
    },
    {
        "id": 4084664,
        "title": "Good-Turing estimation from word lattices for unsupervised language model adaptation",
        "abstract": "We present a comparison of using the weighted word lattice output of a recognizer versus its one-best transcription for unsupervised language model adaptation. We begin with a general analysis of how to smooth word probabilities when the sample is hidden, as is the case with recognizer lattices. For each smoothing technique for the known sample case, we show there is a natural generalization to the hidden case. In particular, we use this generalization with the well-known Good-Turing estimate on word lattices, and show results using Monte Carlo methods for building Katz backoff models. In our realistic adaptation task, with mismatched acoustic and language models, we find that Katz backoff models trained on word lattice samples provide a small, consistent benefit over those trained on one-best output, most notably when there is a limited amount of adaptation data (less than 100 hours). Thus, while the recognizer one-best transcription can provide an effective approximation for the purpose of language model adaptation under certain circumstances, the word lattice provides information that can be exploited for more robust language modeling.",
        "date": "January 2003",
        "authors": [
            "M. Riley",
            "B. Roark",
            "Richard Sproat"
        ],
        "references": [
            263890871,
            220685657,
            220475988,
            313201001,
            232128580,
            230876151,
            3908400,
            3618313,
            3178307
        ]
    },
    {
        "id": 3333325,
        "title": "Lee, C.: Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains. IEEE Trans. Speech Audio Process. 2, 291-298",
        "abstract": "In this paper, a framework for maximum a posteriori (MAP) estimation of hidden Markov models (HMM) is presented. Three key issues of MAP estimation, namely, the choice of prior distribution family, the specification of the parameters of prior densities, and the evaluation of the MAP estimates, are addressed. Using HMM's with Gaussian mixture state observation densities as an example, it is assumed that the prior densities for the HMM parameters can be adequately represented as a product of Dirichlet and normal-Wishart densities. The classical maximum likelihood estimation algorithms, namely, the forward-backward algorithm and the segmental k-means algorithm, are expanded, and MAP estimation formulas are developed. Prior density estimation issues are discussed for two classes of applications-parameter smoothing and model adaptation-and some experimental results are given illustrating the practical interest of this approach. Because of its adaptive nature, Bayesian learning is shown to serve as a unified approach for a wide range of speech recognition applications",
        "date": "May 1994",
        "authors": [
            "Jean-Luc Gauvain",
            "Chin-Hui Lee"
        ],
        "references": [
            264580081,
            313576717,
            288951061,
            285747845,
            270367182,
            269502081,
            264960853,
            261682013,
            248848344,
            248348842
        ]
    },
    {
        "id": 2955633,
        "title": "Automated natural spoken dialog",
        "abstract": "The next generation of voice-based interface technology will enable easy-to-use automation of new and existing communication services, making human-machine interaction more natural",
        "date": "May 2002",
        "authors": [
            "Allen L. Gorin",
            "A. Abella",
            "Giuseppe Riccardi",
            "J. H. Wright"
        ],
        "references": [
            271452206,
            269079064,
            230876077,
            221607780,
            221479739,
            220419137,
            260692505,
            223836764,
            222573864,
            222499956
        ]
    },
    {
        "id": 2927500,
        "title": "Example Selection for Bootstrapping Statistical Parsers",
        "abstract": "This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both a mostly-unsupervised approach, co-training, in which two parsers are iteratively re-trained on each other's output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser's output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility.",
        "date": "March 2004",
        "authors": [
            "Mark Steedman",
            "Rebecca Hwa",
            "Stephen Clark",
            "Miles Osborne"
        ],
        "references": [
            221617806,
            220017637,
            2810752,
            247645153,
            229078318,
            221345942,
            220355517,
            2757429,
            2663660,
            2576903
        ]
    },
    {
        "id": 2910947,
        "title": "Automatic Transcription Of Voicemail At Att",
        "abstract": "This paper reports on the automatic transcription accuracy of voicemail messages. It shows that vocal tract length normalization and adaptation using linear transformations, proven to improve accuracy on the Switchboard task, provide similar accuracy improvements on this task. Direct application of the normalization techniques is complicated by the fragmentation of the data. However, unsupervised clustering was found to be effective in ensuring robust estimation of normalization parameters. Variance adaptation resulted in larger accuracy improvements than adaptation of only mean parameters, probably due to a large variability in channel conditions. The use of semi-tied covariances provides additional gains over using speaker and channel normalization. The combined gain of using various compensation techniques improves the system word error rate from 34.9% for the baseline system to 28.7%.",
        "date": "March 2001",
        "authors": [
            "Michiel Bacchiani"
        ],
        "references": [
            221487512,
            257267653,
            243774218,
            243766521,
            229078317,
            3703425,
            3644304,
            3644274,
            3333632,
            2361214
        ]
    },
    {
        "id": 2876571,
        "title": "Supervised and unsupervised PCFG adaptation to novel domains",
        "abstract": "This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation. The MAP framework is general enough to include some previous model adaptation approaches, such as corpus mixing in Gildea (2001), for example. Other approaches falling within this framework are more effective.",
        "date": "March 2004",
        "authors": [
            "Brian Roark",
            "Michiel Bacchiani"
        ],
        "references": [
            263890871,
            308670916,
            303517922,
            262349927,
            257267653,
            243774218,
            238312866,
            231746427,
            229078317,
            220708227
        ]
    },
    {
        "id": 242394905,
        "title": "Basic Methods of Probabilistic Context Free Grammars",
        "abstract": "In automatic speech recognition, language models can be represented by Probabilistic Context Free Grammars (PCFGs). In this lecture we review some known algorithms which handle PCFGs; in particular an algorithm for the computation of the total probability that a PCFG generates a given sentence (Inside), an algorithm for finding the most probable parse tree (Viterbi), and an algorithm for the estimation of the probabilities of the rewriting rules of a PCFG given a corpus (Inside-Outside). Moreover, we introduce the Left-to-Right Inside algorithm, which computes the probability that successive applications of the grammar rewriting rules (beginning with the sentence start symbol s) produce a word string whose initial substring is a given one.",
        "date": "January 1992",
        "authors": [
            "Fred Jelinek",
            "John D. Lafferty",
            "Robert Mercer"
        ],
        "references": [
            224377724,
            246769833,
            234819176,
            230876033,
            230875831,
            222491221,
            222191456,
            221995817,
            215721461
        ]
    },
    {
        "id": 221482600,
        "title": "Estimation of the probability distributions of stochastic context-free grammars from the k-best derivations.",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "Joan-Andreu S\u00e1nchez",
            "Jos\u00e9-Miguel Bened\u00ed"
        ],
        "references": [
            220017637,
            2434567,
            239667395,
            221275429,
            220359909,
            220049264,
            3192745
        ]
    },
    {
        "id": 221478582,
        "title": "Learning of stochastic context-free grammars by means of estimation algorithms.",
        "abstract": "The learning of stochastic context-free grammars from bracketed corpora by means ofreestimation algorithms was explored in this work. The Comparisons of the Inside-Outsidealgorithm (IO) and it's bracketed version (lOb), the Viterbi-Score algorithm (VS) and thebracketed version of the Viterbi-Score (VSb) algorithms was also carried out. A completeexperiment with the Penn-Tree Bank was made in order to study the performance of theresulting model and the speed of convergence of the...",
        "date": "January 1999",
        "authors": [
            "Joan-Andreu S\u00e1nchez",
            "Jos\u00e9-Miguel Bened\u00ed"
        ],
        "references": [
            221482600,
            239667395,
            230876149,
            230875831,
            220359909,
            220049264,
            3192745,
            2272094
        ]
    },
    {
        "id": 31447209,
        "title": "Stochastic Context-Free Grammars for tRNA Modeling",
        "abstract": "Stochastic context-free grammars (SCFGs) are applied to the problems of folding, aligning and modeling families of tRNA sequences. SCFGs capture the sequences' common primary and secondary structure and generalize the hidden Markov models (HMMs) used in related work on protein and DNA. Results show that after having been trained on as few as 20 tRNA sequences from only two tRNA subfamilies (mitochondrial and cytoplasmic), the model can discern general tRNA from similar-length RNA sequences of other kinds, can find secondary structure of new tRNA sequences, and can produce multiple alignments of large sets of tRNA sequences. Our results suggest potential improvements in the alignments of the D- and T-domains in some mitochdondrial tRNAs that cannot be fit into the canonical secondary structure.",
        "date": "December 1994",
        "authors": [
            "Yasubumi Sakakibara",
            "Michael Brown",
            "Richard Hughey",
            "I. Saira Mian"
        ],
        "references": [
            258029806,
            225466160,
            37597294,
            278924639,
            267149006,
            238831754,
            228057764,
            222938261,
            222491221,
            220262017
        ]
    },
    {
        "id": 267149006,
        "title": "Syntactic Pattern Recognition and Applications",
        "abstract": "",
        "date": "January 1982",
        "authors": [
            "K.S. Fu"
        ],
        "references": []
    },
    {
        "id": 262493823,
        "title": "The Art of Computer Programming",
        "abstract": "",
        "date": "January 2001",
        "authors": [
            "D E Knuth"
        ],
        "references": []
    },
    {
        "id": 242361757,
        "title": "Introduction to Formal Language Theory",
        "abstract": "",
        "date": "January 1978",
        "authors": [
            "Michael A. Harrison"
        ],
        "references": []
    },
    {
        "id": 236411695,
        "title": "Syntactic Pattern Recognition. An Introduction",
        "abstract": "This book provides an introduction to basic concepts and techniques of syntactic pattern recognition. The presentation emphasizes fundamental and practical material rather than strictly theoretical topics, and numerous examples illustrate the principles. The subject is developed according to the following topics: introduction (background, patterns and pattern classes, approaches to pattern recognition, elements of a pattern recognition system, concluding remarks); elements of formal language theory (introduction; string grammars and languages; examples of pattern languages and grammars; equivalent context-free grammars; syntax-directed translations; deterministic, nondeterministic, and stochastic systems; concluding remarks); higher-dimensional grammars (introduction; tree grammars; web grammars; plex grammars; shape gammars; concluding remarks); recognition and translation of syntactic structures (introduction; string language recognizers; automata for simple syntax-directed translation; parsing in string languages; recognition of imperfect strings; tree automata; concluding remarks); stochastic grammars, languages, and recognizers (introduction; stochastic grammars and languages; consisting of stochastic context-free grammars; stochastic reocgnizers; stochastic syntax-directed translations; modified Cocke-Younger-Kasami parsing algorithm for stochastic errors of changed symbols; concluding remarks); and grammatical inference (introduction; inference of regular grammars; inference of context-free grammars; inference of tree grammars; inference of stochastic grammar; concluding remarks). 155 references, 93 figures, 4 tables. (RWR)",
        "date": "January 1978",
        "authors": [
            "R.C. Gonzalez",
            "M.G. Thomason"
        ],
        "references": []
    },
    {
        "id": 222994624,
        "title": "Applications of stochastic context-free grammars using the Inside-Outside algorithm",
        "abstract": "This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.",
        "date": "July 1991",
        "authors": [
            "K. Lari",
            "Steve Young"
        ],
        "references": [
            242394905,
            235197253,
            235038883,
            3548274,
            3177528,
            284498324,
            284194399,
            269021866,
            264584191,
            260673067
        ]
    },
    {
        "id": 301144179,
        "title": "Estimation Procedures for Language Context: Poor Estimates are Worse than None",
        "abstract": "It is difficult to estimate the probability of a word\u2019s context because of sparse data problems. If appropriate care is taken, we find that it is possible to make useful estimates of contextual probabilities that improve performance in a spelling correction application. In contrast, less careful estimates are found to be useless. Specifically, we will show that the Good-Turing method makes the use of contextual information practical for a spelling corrector, while attempts to use the maximum likelihood estimator (MLE) or expected likelihood estimator (ELE) fail. Spelling correction was selected as an application domain because it is analogous to many important recognition applications based on a noisy channel model (such as speech recognition), though somewhat simpler and therefore possibly more amenable to detailed statistical analysis.",
        "date": "January 1990",
        "authors": [
            "W. A. Gale",
            "Kenneth Church"
        ],
        "references": [
            234828877,
            221510305,
            313201001,
            272087448,
            242574596,
            232128580,
            50335996,
            38366977,
            30868825,
            3488972
        ]
    },
    {
        "id": 248593046,
        "title": "Combining Syntactic Knowledge and Visual Text Recognition: A Hidden Markov Model for Part of Speech Tagging In A Word Recognition Algorithm",
        "abstract": "The use of a hidden Markov model (HMM) for the assignment of part-of-speech (POS) tags to improve the performance of a text recognition algorithm is dis- cussed. Syntactic constraints are described by the tran- sition probabilities between POS tags. The confusion between the feature string for a word and the various tags is also described probabilislically. A modification of the Viterbi algorithm is also presented that finds a fixed number of sequences of tags for a given sentence that have the highest probabilities of occurrence, given the feature strings for the words. An experimental application of this approach is demonstrated with a word hypothesizalion algorithm that produces a number of guesses about the identity of each word in a running text. The use of first and second order transition probabili- lies is explored. Overall performance of between 65 and 80 percent reduction in the average number of words that can match a given image is achieved. A computational theory for word recognition has been proposed that overcomes some of the constraints of other methodologies (6). The design of this tech- nique is based on human performance in reading which suggests that the feature analysis of word images includes a wholistic analysis of word shape. Also, feature extraction from a word image is only; one part of a complex process of developing an understanding of a text. Furthermore, the model suggests that to achieve high levels of performance in text recognition for a range of input qualities it may be necessary to understand the text as well. One part of the understanding process that underlies word recognition is an analysis of the syn- tax of the text. This paper discusses aspects of a Markov model for POS tagging where the probability of observing any POS tag is dependent on the tag of the previous word (8, 9). This model is applied to text recognition by first using a word recognition algorithm to supply a number of alternatives for the identity of each word. The tags of the alternatives for the words in a sentence are then input to a modilied Viterbi algorithm that determines sequences of syntactic classes that include each word. An alternative for a word decision is out- put only if its part of speech tag is included in at least one of these sequences. The Markov model improves word recognition performance if the number of altema- lives for a word are reduced without removing the correct choice. The rest of this paper briefly introduces the com- putational model for word recognition. This is followed by a description of how a Markov model for language syntax is incorporated in the model. The modified Viterbi algorithm proposed in this paper is then described. The performance of this technique in reduc- ing the number of alternatives for words in a sample of text is then discussed. The effect of employing the first and second order Markov assumptions and different methods of estimating the probabilities are explored.",
        "date": "January 1992",
        "authors": [
            "Jonathan J. Hull"
        ],
        "references": [
            265194012,
            234823075,
            3513839,
            3282124,
            238735047,
            221102572,
            220049445,
            220049367,
            200003659,
            51597333
        ]
    },
    {
        "id": 230875889,
        "title": "An Estimate of an Upper Bound for the Entropy of English",
        "abstract": "We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds.",
        "date": "January 1992",
        "authors": [
            "Peter F Brown",
            "Stephen Andrew Della Pietra",
            "Vincent Della J Pietra",
            "Jenifer C Lai"
        ],
        "references": [
            242070491,
            239059509,
            238735047,
            234776814,
            230876473,
            230876153,
            230876151,
            224773133,
            38361178,
            3083078
        ]
    },
    {
        "id": 224377724,
        "title": "A Maximum Likelihood Approach to Continuous Speech Recognition",
        "abstract": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.",
        "date": "April 1983",
        "authors": [
            "Lalit R. Bahl",
            "Frederick Jelinek",
            "Robert Mercer"
        ],
        "references": [
            313220763,
            256121821,
            256121599,
            242506135,
            242406000,
            242396627,
            236157558,
            235890211,
            234803144,
            234544416
        ]
    },
    {
        "id": 221102042,
        "title": "A Spelling Correction Program Based on a Noisy Channel Model.",
        "abstract": "This paper describes a new program, correct, which takes words rejected by the Unix\u00ae spell program, proposes a list of candidate corrections, and sorts them by probability. The probability scores are the novel contribution of this work. Probabilities are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in the speech recognition literature (Jelinek, 1985), one can often recover the intended correction, c, from a typo, t, by finding the correction c that maximizes Pr(c) Pr(t/c). The first factor, Pr(c), is a prior model of word probabilities; the second factor, Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (e.g., insertions, delections, substitutions and reversals). Both sets of probabilities were trained on data collected from the Associated Press (AP) newswire. This text is ideally suited for this purpose since it contains a large number of typos (about two thousand per month).",
        "date": "January 1990",
        "authors": [
            "Mark D. Kernighan",
            "Kenneth Ward Church",
            "William A. Gale"
        ],
        "references": [
            313201001,
            237109467,
            232128580,
            230876148,
            224732903
        ]
    },
    {
        "id": 303517837,
        "title": "Numerical recipes in C",
        "abstract": "",
        "date": "January 1992",
        "authors": [
            "William Press",
            "S A Teukolsky",
            "William Vetterling",
            "Brian Flannery"
        ],
        "references": []
    },
    {
        "id": 220874539,
        "title": "Estimators for Stochastic \"Unification-Based\" Grammars*",
        "abstract": "Log-linear models provide a statistically sound framework for Stochastic \"Unification-Based\" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar.",
        "date": "January 1999",
        "authors": [
            "Mark Johnson",
            "Stuart Geman",
            "Stephen Canon",
            "Zhiyi Chi"
        ],
        "references": [
            243772178,
            2332340,
            312613930,
            243769716,
            242706958,
            235559029,
            230876149,
            220482192,
            216301244,
            200033867
        ]
    },
    {
        "id": 51991698,
        "title": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation",
        "abstract": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.",
        "date": "August 2002",
        "authors": [
            "Robert Malouf"
        ],
        "references": [
            251251118,
            243772178,
            220874539,
            285906939,
            247042546,
            244958007,
            244446733,
            243768111,
            230873264,
            228057950
        ]
    },
    {
        "id": 3491928,
        "title": "The N-best Algorithm: An Efficient and Exact Procedure for Finding the N Most Likely Sentence Hypotheses",
        "abstract": "A search algorithm that provides a simple, clean, and efficient interface between the speech and natural language components of a spoken language system is introduced. The N -best algorithm is a time-synchronous Viterbi-style beam search procedure that is guaranteed to find the N most likely whole sentence alternatives that are within a given beam of the most likely sentence. The computation is linear with the length of the utterance, and faster than linear in N . When used together with a first-order statistical grammar, the correct sentence is usually within the first few sentence choices. The output of the algorithm, which is an ordered set of sentence hypotheses with acoustic and language model scores can easily be processed by natural language knowledge sources without the huge expansion of the search space that would be needed to include all possible knowledge sources in a top-down search. In experiments using a first-order statistical language model, the average rank of the correct answer was 1.8 and was within the first 24 choices 99% of the time",
        "date": "May 1990",
        "authors": [
            "Richard Schwartz",
            "Y.-L. Chow"
        ],
        "references": [
            234787342,
            224737907,
            3175688,
            224736184,
            224657044,
            221491931
        ]
    },
    {
        "id": 2921800,
        "title": "In Accurate Unlexicalized Parsing",
        "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.",
        "date": "June 2003",
        "authors": [
            "Dan Klein",
            "Christopher D. Manning"
        ],
        "references": [
            220873033,
            284689173,
            270860605,
            242430108,
            230876256,
            230875831,
            220355517,
            3046036,
            2904422,
            2803418
        ]
    },
    {
        "id": 2839498,
        "title": "Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques",
        "abstract": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score.",
        "date": "June 2002",
        "authors": [
            "Stefan Riezler",
            "Tracy Holloway King",
            "Ronald M Kaplan",
            "Richard Crouch"
        ],
        "references": [
            220874539,
            220778236,
            220355535,
            2848688,
            2683860,
            262349927,
            260187426,
            243768111,
            221619104,
            2906848
        ]
    },
    {
        "id": 2561857,
        "title": "An Efficient Implementation of a New DOP Model",
        "abstract": "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.",
        "date": "May 2003",
        "authors": [
            "Rens Bod"
        ],
        "references": [
            221014258,
            262349927,
            255673306,
            243713955,
            243448719,
            242509122,
            242396917,
            221605419,
            221102362,
            220875022
        ]
    },
    {
        "id": 259810695,
        "title": "A Transformational Approach to English Syntax: Root, Structure-Preserving, and Local Transformations",
        "abstract": "",
        "date": "January 1976",
        "authors": [
            "Joseph Emonds"
        ],
        "references": []
    },
    {
        "id": 228609663,
        "title": "Projection, Heads, and Optimality",
        "abstract": "Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry. This article argues that inversion of the subject and auxiliary in English matrix questions and elsewhere is the effect of a violable optimality-theoretic constraint that requires head positions to be filled. When no other auxiliary is available, do-support occurs to satisfy this constraint, resulting in the presence of an expletive verb. When a higher-ranked constraint prohibits inversion, no inversion or do-support is found. The argument is then extended to cases where the complementizer that is obligatory, which are shown to offer best satisfaction of the proposed set of violable constraints.",
        "date": "October 1997",
        "authors": [
            "Jane Grimshaw"
        ],
        "references": [
            261985820,
            306123749,
            289212741,
            270439329,
            268378820,
            259578945,
            256279376,
            248230205,
            247677324,
            247434479
        ]
    },
    {
        "id": 220874688,
        "title": "Constituent Parsing with Incremental Sigmoid Belief Networks.",
        "abstract": "We introduce a framework for syntactic parsing with latent variables based on a form of dynamic Sigmoid Belief Networks called Incremental Sigmoid Belief Networks. We demonstrate that a previous feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model. By construct- ing a more accurate but still tractable ap- proximation, we significantly improve pars- ing accuracy, suggesting that ISBNs provide a good idealization for parsing. This gener- ative model of parsing achieves state-of-the- art results on WSJ text and 8% error reduc- tion over the baseline neural network parser.",
        "date": "January 2007",
        "authors": [
            "Ivan Titov",
            "James Henderson"
        ],
        "references": [
            226435002,
            220875081,
            220874045,
            262349927,
            244150117,
            239582331,
            224773133,
            223927652,
            220875180,
            220873590
        ]
    },
    {
        "id": 220817154,
        "title": "Towards Robust Semantic Role Labeling",
        "abstract": "Most semantic role labeling (SRL) research has been focused on training and evaluating on the same corpus. This strategy, although appropriate for initiating research, can lead to overtraining to the particular corpus. This article describes the operation of assert, a state-of-the art SRL system, and analyzes the robustness of the system when trained on one genre of data and used to label a different genre. As a starting point, results are first presented for training and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data. Experiments are then presented to evaluate the portability of the system to another source of data. These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and argument identification transfer relatively well to a new corpus, argument classification does not. An analysis of the reasons for this is presented and these generally point to the nature of the more lexical/semantic features dominating the classification task where more general structural features are dominant in the argument identification task.",
        "date": "January 2007",
        "authors": [
            "Sameer S. Pradhan",
            "Wayne Ward",
            "James H. Martin"
        ],
        "references": [
            269087887,
            262408350,
            244957564,
            230876718,
            271431029,
            243771370,
            230876712,
            230876711,
            230876696,
            230876695
        ]
    },
    {
        "id": 224640972,
        "title": "Reranking for Sentence Boundary Detection in Conversational Speech",
        "abstract": "We present a reranking approach to sentence-like unit (SU) boundary detection, one of the EARS metadata extraction tasks. Techniques for generating relatively small n-best lists with high oracle accuracy are presented. For each candidate, features are derived from a range of information sources, including the output of a number of parsers. Our approach yields significant improvements over the best performing system from the NIST RT-04F community evaluation",
        "date": "June 2006",
        "authors": [
            "B. Roark",
            "Yang Liu",
            "Mary Harper",
            "R. Stewart"
        ],
        "references": [
            228992017,
            228822214,
            228704033,
            221489176,
            220874364,
            255644540,
            225879575,
            222513968,
            221993393,
            220875180
        ]
    },
    {
        "id": 228649189,
        "title": "Re-ranking algorithms for name tagging",
        "abstract": "Integrating information from different stages of an NLP processing pipeline can yield significant error reduction. We dem-onstrate how re-ranking can improve name tagging in a Chinese information extrac-tion system by incorporating information from relation extraction, event extraction, and coreference. We evaluate three state-of-the-art re-ranking algorithms (MaxEnt-Rank, SVMRank, and p-Norm Push Rank-ing), and show the benefit of multi-stage re-ranking for cross-sentence and cross-document inference.",
        "date": "July 2006",
        "authors": [
            "Heng Ji",
            "Cynthia Rudin",
            "Ralph Grishman"
        ],
        "references": [
            220873474,
            2635195,
            2614456,
            262349927,
            262333164,
            239006410,
            228597503,
            221497518,
            221346436,
            221102546
        ]
    },
    {
        "id": 228057950,
        "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
        "abstract": "We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
        "date": "January 2001",
        "authors": [
            "John Lafferty",
            "Andrew Mccallum",
            "Fernando Pereira"
        ],
        "references": [
            3192689,
            2754791,
            262349927,
            233971696,
            224890468,
            224619498,
            222675710,
            220688767,
            216301243,
            201841001
        ]
    },
    {
        "id": 225070191,
        "title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting",
        "abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone-Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in n. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.",
        "date": "December 1999",
        "authors": [
            "Yoav Freund",
            "Robert E Schapire"
        ],
        "references": [
            221620492,
            221619244,
            221590259,
            220430859,
            2798688,
            2385290,
            272832134,
            244498683,
            243773325,
            242580265
        ]
    },
    {
        "id": 221346436,
        "title": "An Efficient Boosting Algorithm for Combining Preferences.",
        "abstract": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations.",
        "date": "January 1998",
        "authors": [
            "Yoav Freund",
            "Raj D. Iyer",
            "Robert E. Schapire",
            "Yoram Singer"
        ],
        "references": [
            237374298,
            262207386,
            222443115,
            222190436,
            221515677,
            221300740,
            220688577,
            220688491,
            23738783,
            3671963
        ]
    },
    {
        "id": 221102892,
        "title": "Building a Large-Scale Annotated Chinese Corpus.",
        "abstract": "In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.",
        "date": "January 2002",
        "authors": [
            "Nianwen Xue",
            "Fu-Dong Chiou",
            "Martha Stone Palmer"
        ],
        "references": [
            2539378
        ]
    },
    {
        "id": 220355542,
        "title": "Discriminative Reranking for Natural Language Parsing",
        "abstract": "This paper considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. We describe and compare two approaches to the problem: one based on Markov Random Fields, the other based on boosting approaches to reranking problems. The methods were applied to reranking output of the parser of Collins (1999) on the Wall Street Journal corpus, with a 13% relative decrease in error rate. 1. Introduction Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as newswire text. Many of these methods fall into the general category of history-based models, where a parse tree is represented as a derivation (sequence of decisions) and the proba...",
        "date": "March 2005",
        "authors": [
            "Michael J. Collins",
            "Terry Koo"
        ],
        "references": [
            228776646,
            228771954,
            220874539,
            220816667,
            220270192,
            220017637,
            51991698,
            3192689,
            2877976,
            2839498
        ]
    },
    {
        "id": 220816667,
        "title": "Inducing History Representations for Broad Coverage Statistical Parsing.",
        "abstract": "We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.",
        "date": "January 2003",
        "authors": [
            "James Henderson"
        ],
        "references": [
            221153125,
            220017637,
            2624365,
            262349927,
            246304479,
            220875022,
            220355517,
            220343911,
            4355809,
            2803418
        ]
    },
    {
        "id": 220873944,
        "title": "Parsing the WSJ using CCG and log-linear models",
        "abstract": "This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estima- tion. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non- standard derivations, with normal-form models. The performances of the two models are com- parable and the results are competitive with ex- isting wide-coverage CCG parsers.",
        "date": "January 2004",
        "authors": [
            "Stephen Clark",
            "James R. Curran"
        ],
        "references": [
            2919753,
            2902392,
            2841185,
            2839498,
            2539300,
            2539140,
            2480097,
            285906939,
            238606254,
            230873264
        ]
    },
    {
        "id": 220488267,
        "title": "Efficient Algorithms for Parsing the DOP Model",
        "abstract": "Excellent results have been reported for Data- Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expen- sive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second prob- lem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is compara- ble to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.",
        "date": "April 1996",
        "authors": [
            "Joshua Goodman"
        ],
        "references": [
            246778246,
            230875831,
            2433793,
            2294814
        ]
    },
    {
        "id": 220355517,
        "title": "Head-Driven Statistical Models for Natural Language Parsing",
        "abstract": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",
        "date": "December 2003",
        "authors": [
            "Michael Collins"
        ],
        "references": [
            220873033,
            220817598,
            220816736,
            220708270,
            220685657,
            220017637,
            30377805,
            2925361,
            2683860,
            2635195
        ]
    },
    {
        "id": 2929293,
        "title": "Recovering Latent Information in Treebanks",
        "abstract": "Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-specific rules to augment the training data with extra information. For example, head-finding rules are used to augment node labels with lexical heads. In this paper, we provide machinery to reduce the amount of human e#ort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by di#erent models; second, we report on an experiment to see whether we can use ExpectationMaximization to automatically fine-tune a set of hand-written rules to a particular corpus.",
        "date": "March 2004",
        "authors": [
            "David Chiang",
            "Dan Bikel"
        ],
        "references": [
            228593466,
            220874982,
            220873033,
            220017637,
            2635195,
            2620648,
            262243284,
            234803744,
            230875831,
            223151844
        ]
    },
    {
        "id": 2922698,
        "title": "Nondeterministic LTAG Derivation Tree Extraction",
        "abstract": "In this paper we introduce a naive algorithm for nondeterminisctic LTAG derivation tree extraction from the Penn Treebank and the Proposition Bank. This algorithm is used in the EM models of LTAG Treebank Induction reported in (Shen and Joshi, 2004). Given the trees in the Penn Treebank with PropBank tags, this algorithm generates shared structures that allow efficient dynamic programming in the EM models.",
        "date": "July 2004",
        "authors": [
            "Libin Shen"
        ],
        "references": [
            243766594,
            220873033,
            220017637,
            2401842,
            312607081,
            228569842,
            221995817,
            215721461,
            34517750,
            2929609
        ]
    },
    {
        "id": 221013249,
        "title": "The infinite PCFG using hierarchical Dirichlet processes",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Percy Liang",
            "Slav Petrov",
            "Michael Jordan",
            "Dan Klein"
        ],
        "references": [
            254212736,
            229100417,
            228850244,
            221997021,
            284477399,
            265328171,
            261742334,
            225517426,
            224881743,
            222626991
        ]
    },
    {
        "id": 221012623,
        "title": "Max-Margin Parsing",
        "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying sup- port vector machines. Our formulation uses a factor- ization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candi- dates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic com- plexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimen- tal evidence of the model's improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
        "date": "January 2004",
        "authors": [
            "Ben Taskar",
            "Dan Klein",
            "Mike Collins",
            "Daphne Koller"
        ],
        "references": [
            220874539,
            220817222,
            2921800,
            262349927,
            248590871,
            228540070,
            220873944,
            220355542,
            220355517,
            2929609
        ]
    },
    {
        "id": 220874765,
        "title": "Lexicalization in Crosslinguistic Probabilistic Parsing: The Case of French.",
        "abstract": "This paper presents the first probabilistic parsing results for French, using the re- cently released French Treebank. We start with an unlexicalized PCFG as a base- line model, which is enriched to the level of Collins' Model 2 by adding lexical- ization and subcategorization. The lexi- calized sister-head model and a bigram model are also tested, to deal with the flat- ness of the French Treebank. The bigram model achieves the best performance: 81% constituency F-score and 84% de- pendency accuracy. All lexicalized mod- els outperform the unlexicalized baseline, consistent with probabilistic parsing re- sults for English, but contrary to results for German, where lexicalization has only a limited effect on parsing performance.",
        "date": "January 2005",
        "authors": [
            "Abhishek Arun",
            "Frank Keller"
        ],
        "references": [
            220873033,
            220017637,
            2921800,
            2912527,
            234829591,
            228768528,
            228703966,
            221012579,
            2702732,
            2564663
        ]
    },
    {
        "id": 220873596,
        "title": "Efficient, Feature-based, Conditional Random Field Parsing.",
        "abstract": "Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14 % relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length 40, our system achieves an F-score of 89.0%, a 36 % relative reduction in error over a generative baseline. 1",
        "date": "January 2008",
        "authors": [
            "Jenny Rose Finkel",
            "Alex Kleeman",
            "Christopher D. Manning"
        ],
        "references": [
            234798411,
            221344720,
            220874045,
            220873863,
            262349927,
            226174686,
            221619007,
            221618383,
            220875180,
            220873590
        ]
    },
    {
        "id": 2476043,
        "title": "An Annotation Scheme for Free Word Order Languages",
        "abstract": "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal ,ssump- tions about the interrelation of the particu- lar representational strata.",
        "date": "May 2002",
        "authors": [
            "Wojciech Skut",
            "Brigitte Krenn",
            "Thorsten Brants",
            "Hans Uszkoreit"
        ],
        "references": [
            230876180,
            221102855,
            2479713,
            324385343,
            245889388,
            244955917,
            243782998,
            240461688,
            2612964,
            2434077
        ]
    },
    {
        "id": 228058014,
        "title": "Pattern Classification",
        "abstract": "",
        "date": "January 2001",
        "authors": [
            "Richard O Duda",
            "Peter E Hart",
            "David G.Stork"
        ],
        "references": []
    },
    {
        "id": 220874121,
        "title": "A Generative Constituent-Context Model for Improved Grammar Induction",
        "abstract": "We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.",
        "date": "January 2002",
        "authors": [
            "Dan Klein",
            "Christopher D. Manning"
        ],
        "references": [
            327221114,
            289827888,
            270860605,
            246981102,
            239535201,
            239059847,
            230875831,
            229078258,
            225517426,
            222491221
        ]
    },
    {
        "id": 220320285,
        "title": "The Entire Regularization Path for the Support Vector Machine",
        "abstract": "The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions. \u00a9 2004 Trevor Hastie, Saharon Rosset, Robert Tibshirani and Ji Zhu.",
        "date": "October 2004",
        "authors": [
            "Trevor Hastie",
            "Saharon Rosset",
            "Robert Tibshirani",
            "Ji Zhu"
        ],
        "references": [
            225734295,
            288023219,
            287823327,
            278651717,
            262333164,
            248728006,
            243767165,
            239006410,
            236736850,
            228527066
        ]
    },
    {
        "id": 220814356,
        "title": "A Two-Stage Method for Active Learning of Statistical Grammars.",
        "abstract": "Active learning reduces the amount of manually an- notated sentences necessary when training state-of- the-art statistical parsers. One popular method, un- certainty sampling, selects sentences for which the parser exhibits low certainty. However, this method does not quantify confidence about the current sta- tistical model itself. In particular, we should be less confident about selection decisions based on low frequency events. We present a novel two- stage method which first targets sentences which cannot be reliably selected using uncertainty sam- pling, and then applies standard uncertainty sam- pling to the remaining sentences. An evaluation shows that this method performs better than pure uncertainty sampling, and better than an ensemble method based on bagged ensemble members only.",
        "date": "January 2005",
        "authors": [
            "Markus Becker",
            "Miles Osborne"
        ],
        "references": [
            220017637,
            51909346,
            2537464,
            2490883,
            238198632,
            228737468,
            2866071,
            2757429,
            2565928,
            2553568
        ]
    },
    {
        "id": 2477043,
        "title": "Bootstrapping Statistical Parsers from Small Datasets",
        "abstract": "We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses fom the target domain are used.",
        "date": "April 2003",
        "authors": [
            "Mark Steedman",
            "Miles Osborne",
            "Anoop Sarkar",
            "Stephen Clark"
        ],
        "references": [
            221617806,
            2539754,
            2481707,
            2367274,
            2364251
        ]
    },
    {
        "id": 230876696,
        "title": "Rerankinng and Self-Training for Parser Adaptation",
        "abstract": "Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typi- cally) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such wor- ries have merit. The standard \"Charniak parser\" checks in at a labeled precision- recall f -measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In par- ticular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (Mc- Closky et al., 2006) raise this to 87.8% (an error reduction of 28%) again with- out any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.",
        "date": "July 2006",
        "authors": [
            "David McClosky",
            "Eugene Charniak",
            "Mark Johnson"
        ],
        "references": [
            2477043,
            220875180
        ]
    },
    {
        "id": 228796527,
        "title": "Corrected co-training for statistical parsers",
        "abstract": "Corrected co-training (Pierce & Cardie, 2001) and the closely related co-testing (Muslea et al., 2000) are active learning methods which exploit redundant views to reduce the cost of manually creating labeled training data. We extend these methods to statistical parsing algorithms for nat-ural language. Because creating complex parse structures by hand is significantly more time-consuming than selecting labels from a small set, it may be easier for the human to correct the learner's partially accurate output rather than generate the complex label from scratch. The goal of our work is to minimize the number of corrections that the annotator must make. To reduce the human effort in correcting machine parsed sentences, we propose a novel approach, which we call one-sided corrected co-training and show that this method requires only a third as many manual annotation decisions as corrected co-training/co-testing to achieve the same im-provement in performance.",
        "date": "January 2003",
        "authors": [
            "Rebecca Hwa",
            "Miles Osborne",
            "Anoop Sarkar",
            "Mark Steedman"
        ],
        "references": [
            221345252,
            220817659,
            220813973,
            220017637,
            2810752,
            2537464,
            2477043,
            2452577,
            312607081,
            221345942
        ]
    },
    {
        "id": 2481913,
        "title": "The Structure of Shared Forests in Ambiguous Parsing",
        "abstract": "The Context-Free Backbone of some natural language analyzers produces all possible CF parses as some kind of shared forest, from which a singie tree is to be chosen by a disam- blguation process that may be Based on the finer features of the language. We study the structure of these forests with respect to optimality of sharing, and in relation with the parsing schema used to produce them. In addition to a theo- retical and experimental framework for studying these issues, the main results presented are: - sophist/cat/on in chart parsing schemata (e.g. use of look-ahead) may reduce time and space efficiency instead of improving it, - there is a shared forest structure w/th at most cubic size for any CF grammar, - when O(n z) complexity is reqnired, the shape of a shared forest is dependent on the parsing schema used.",
        "date": "May 2002",
        "authors": [
            "Sylvie Billot",
            "Bernard Lang"
        ],
        "references": [
            305263025,
            243508968,
            242585487,
            242506574,
            242430394,
            242327474,
            238690909,
            238685673,
            234816323,
            230876474
        ]
    },
    {
        "id": 246304479,
        "title": "Generative versus discriminative models for statistical left-corner parsing",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "James Henderson"
        ],
        "references": []
    },
    {
        "id": 220875022,
        "title": "New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",
        "abstract": "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all subtrees\" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.",
        "date": "January 2002",
        "authors": [
            "Michael Collins",
            "Nigel Duffy"
        ],
        "references": [
            221619881,
            220874539,
            220017637,
            37705036,
            2809303,
            2799191,
            2371925,
            262349927,
            255673306,
            248857140
        ]
    },
    {
        "id": 215721451,
        "title": "Neural Networks For Pattern Recognition",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "Ch. M. Bishop"
        ],
        "references": [
            220267826,
            245386787,
            220343922,
            3192856,
            2423042
        ]
    },
    {
        "id": 4355809,
        "title": "Deterministic left corner parsing",
        "abstract": "Left corner parsing refers to a class of parsing procedures in which the productions are recognized in a particular order which is different than both bottom up and top down. Each production is recognized after its left descendant but before its other descendants. Procedures in this class have occurred frequently in the compiler literature. In this paper a class of grammars, called LC (k) grammars, is defined and shown to be exactly those grammars for which a certain canonical pushdown machine that does left corner parsing is deterministic. A subset of LC(k) grammars, called strong LC(k) grammars, is defined and shown to be those grammars that can be deterministically left corner parsed by a simplified canonical machine. It is shown that when a particular grammatical rewriting procedure is applied to a grammar, the resulting grammar is (strong) LL(k) if and only if the original grammar is (strong) LC(k). This implies that the class of LC(k) languages is identical to the class of LL(k) languages. The syntax directed translations on LC(k) grammars that can be performed by the canonical pushdown machine are also discussed.",
        "date": "November 1970",
        "authors": [
            "Daniel J. Rosenkrantz",
            "P. M. Lewis"
        ],
        "references": [
            265444013,
            243508968,
            222443906,
            221591155,
            221499187,
            221330421,
            220432174,
            220425813,
            220247800,
            31135527
        ]
    },
    {
        "id": 221299854,
        "title": "Using WordNet to Disambiguate Word Senses for Text Retrieval.",
        "abstract": "This paper describes an automatic indexing procedure that uses the \u201cIS-A\u201d relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches.",
        "date": "January 1993",
        "authors": [
            "Ellen M. Voorhees"
        ],
        "references": [
            246620241,
            244467801,
            242376187,
            234819712,
            221037651
        ]
    },
    {
        "id": 2633937,
        "title": "Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections",
        "abstract": "Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval. We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm. 1 Introduction Document clustering has been extensively investigated as a methodology for improving document search and retrieval (see [15] for an excellent review). The general assumption is that mut...",
        "date": "May 1996",
        "authors": [
            "Douglass R. Cutting",
            "David Ron Karger",
            "Jan O. Pedersen",
            "John W. Tukey"
        ],
        "references": [
            242421516,
            242413746,
            236025139,
            229619310,
            223483101,
            222229774,
            221301238,
            221300662,
            220695912,
            220688577
        ]
    },
    {
        "id": 238984455,
        "title": "A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks",
        "abstract": "Representing and manipulating context information is one of the hardest problems in natural language processing. This paper proposes a method for representing some context information so that the correct meaning for a word in a sentence can be selected. The approach is primarily based on work by Waltz and Pollack (1985, 1984), who emphasized neutrally plausible systems. By contrast this paper focuses on computationally feasible methods applicable to full-scale natural language processing systems. There are two key elements: a collection of context vectors defined for every word used by a natural language processing system, and a context algorithm that computes a dynamic context vector at any position in a body of text. Once the dynamic context vector has been computed it is easy to choose among competing meanings for a word. This choice of definitions is essentially a neural network computation, and neural network learning algorithms should be able to improve such choices. Although context vectors do not represent all context information, their use should improve those full-scale systems that have avoided context as being too difficult to deal with. Good candidates for full-scale context vector implementations are machine translation systems and Japanese word processors. A main goal of this paper is to encourage such large-scale implementations and tests of context vector approaches. A variety of interesting directions for research in natural language processing and machine learning will be possible once a full set of context vectors has been created. In particular the development of more powerful context algorithms will be an important topic for future research.",
        "date": "September 1991",
        "authors": [
            "Steve Gallant"
        ],
        "references": [
            236136564,
            230875810,
            5569039,
            297221645,
            285609512,
            243764382,
            234808418,
            233142172,
            232524536,
            224483164
        ]
    },
    {
        "id": 232878134,
        "title": "Contextual Correlates of Semantic Similarity",
        "abstract": "The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
        "date": "January 1991",
        "authors": [
            "George A. Miller",
            "Walter G. Charles"
        ],
        "references": [
            9870377,
            321547750,
            304115919,
            270387211,
            247514692,
            246645735,
            245803876,
            243729251,
            243494067,
            240359088
        ]
    },
    {
        "id": 230854746,
        "title": "Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone",
        "abstract": "These ideas may provide one of the more meaningful sessions that you attend because we are going to be dealing with that thing of IDENTITY. The identity of the independent technical writer. Some of you will want to nod off and then there will be some ...",
        "date": "January 1986",
        "authors": [
            "Michael Lesk"
        ],
        "references": [
            242439023
        ]
    },
    {
        "id": 229593960,
        "title": "Word-Word Associations in Document Retrieval Systems",
        "abstract": "The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments. Associated pairs of words usually reflect localized word meanings, and true synonyms cannot readily be found from first or second order relationships in our document collections. There is little overlap between word relationships found through associations and those used in thesaurus construction, and the effects of word associations and a thesaurus in retrieval are independent. The use of associations in retrieval experiments improves not only recall, by permitting new matches between requests and documents, but also precision, by reinforcing existing matches. In our experiments, the precision effect is responsible for most of the improvement possible with associations. A properly constructed thesaurus, however, offers better performance than statistical association methods.",
        "date": "January 1969",
        "authors": [
            "M. E. Lesk"
        ],
        "references": [
            234771070,
            229673691,
            220430917
        ]
    },
    {
        "id": 228057706,
        "title": "Indexing By Latent Semantic Analysis",
        "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.",
        "date": "September 1990",
        "authors": [
            "Scott Deerwester",
            "Susan T. Dumais",
            "George Furnas",
            "Thomas K. Landauer"
        ],
        "references": [
            220466601,
            24062669,
            305261066,
            301690162,
            292020034,
            285537712,
            284686548,
            284667915,
            256182235,
            255565525
        ]
    },
    {
        "id": 223483101,
        "title": "Recent Trends in Hierarchic Document Clustering: A Critical Review",
        "abstract": "This article reviews recent research into the use of hierarchic agglomerative clustering methods for document retrieval. After an introduction to the calculation of interdocument similarities and to clustering methods that are appropriate for document clustering, the article discusses algorithms that can be used to allow the implementation of these methods on databases of nontrivial size. The validation of document hierarchies is described using tests based on the theory of random graphs and on empirical characteristics of document collections that are to be clustered. A range of search strategies is available for retrieval from document hierarchies and the results are presented of a series of research projects that have used these strategies to search the clusters resulting from several different types of hierarchic agglomerative clustering method. It is suggested that the complete linkage method is probably the most effective method in terms of retrieval performance; however, it is also difficult to implement in an efficient manner. Other applications of document clustering techniques are discussed briefly; experimental evidence suggests that nearest neighbor clusters, possibly represented as a network model, provide a reasonably efficient and effective means of including interdocument similarity information in document retrieval systems.",
        "date": "December 1988",
        "authors": [
            "Peter Willett"
        ],
        "references": [
            247533402,
            238655641,
            314794884,
            314784203,
            313719655,
            313024175,
            312986441,
            301222420,
            289099346,
            284689212
        ]
    },
    {
        "id": 222393384,
        "title": "Experiment on linguistically based term associations",
        "abstract": "A description of the hyperterm system REALIST (REtrieval Aids by LInguistics and STatistics) and in more detail a description of its semantic component is given. We call a hyperterm system a system that contains different kinds of term relations. The semantic component of REALIST generates semantic term relations such as synonyms. It takes as input a free-text database and generates as output term pairs that are semantically related with respect to their meanings in the database. This is done in two steps. In the first step an automatic syntactic analysis provides linguistical knowledge about the terms of the database. In the second step this knowledge is compared by statistical similarity computation. Various experiments with different similarity measures are described. These experiments are not standard recall and precision examinations, but direct evaluations of the term pairs. Beyond the new linguistic term association method and its good results, another important point of this paper is to show the value of direct term pair evaluation.",
        "date": "December 1992",
        "authors": [
            "Gerda Ruge"
        ],
        "references": [
            221300134,
            243554681,
            235150926,
            229593960,
            227660782,
            223062787,
            221562526,
            220231088,
            220230630,
            220113507
        ]
    },
    {
        "id": 222281706,
        "title": "A cooccurrence-based thesaurus and two applications to Information Retrieval",
        "abstract": "This paper presents a new method for computing a thesaurus from a text corpus. Each word is represented as a vector in a multi-dimensional space that captures cooccurrence information. Words are defined to be similar if they have similar cooccurrence patterns. Two different methods for using these thesaurus vectors in information retrieval are shown to significantly improve performance over the Tipster reference corpus as compared to a term vector space baseline.",
        "date": "January 1994",
        "authors": [
            "Hinrich Sch\u00fctze",
            "Jan O. Pedersen"
        ],
        "references": [
            262166369,
            21347546,
            291807076,
            286371547,
            271543691,
            247645830,
            238984455,
            234781964,
            228057706,
            223332191
        ]
    },
    {
        "id": 235039252,
        "title": "Measures of Syntactic Complexity",
        "abstract": "",
        "date": "August 1963",
        "authors": [
            "Y. BarHillel",
            "Asa Kasher",
            "Eli Shamir"
        ],
        "references": []
    },
    {
        "id": 9517409,
        "title": "Movement-Produced Stimulation in the Development of Visually Guided Behavior",
        "abstract": "Full and exact adaptation to sensory rearrangement in adult human Ss requires movement-produced sensory feedback. Riesen's work suggested that this factor also operates in the development of higher mammals but he proposed that sensory-sensory associations are the proposed that sensory-sensory associations are the prerequisite. To test these alternatives, visual stimulation of the active member (A) of each of 10 pairs of neonatal kittens was allowed to vary with its locomotor movements while equivalent stimulation of the second member (P) resulted from passive motion. Subsequent tests of visually guided paw placement, discrimination on a visual cliff, and the blink response were normal for A but failing in P. When other alternative explanations are excluded, this result extends the conclusions of studies of adult rearrangement to neonatal development. (18 ref.)",
        "date": "November 1963",
        "authors": [
            "Richard m Held",
            "Alan. Hein"
        ],
        "references": [
            289510916,
            285133744,
            284647839,
            254732858,
            242554268,
            234755556,
            232532633,
            232446490,
            232134485,
            10290074
        ]
    },
    {
        "id": 324640748,
        "title": "Applikativnaja porozdajuscaja model' i iscislenie transformacij v russkom jazyke",
        "abstract": "",
        "date": "July 1964",
        "authors": [
            "Barbara Hall",
            "S. K. Saumjan",
            "P. A. Soboleva"
        ],
        "references": []
    },
    {
        "id": 324304653,
        "title": "The Principles of Semantics",
        "abstract": "",
        "date": "April 1953",
        "authors": [
            "Austin Gill",
            "Stephen Ullmann"
        ],
        "references": []
    },
    {
        "id": 320181926,
        "title": "Word and Object",
        "abstract": "",
        "date": "January 1960",
        "authors": [
            "W.V. Quine"
        ],
        "references": []
    },
    {
        "id": 313154879,
        "title": "The misbehavior of organisms.",
        "abstract": "",
        "date": "January 1961",
        "authors": [
            "K. Breland",
            "M. Breland"
        ],
        "references": []
    },
    {
        "id": 303837236,
        "title": "Negation in English",
        "abstract": "",
        "date": "January 1964",
        "authors": [
            "E. Klima"
        ],
        "references": []
    },
    {
        "id": 285058764,
        "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex",
        "abstract": "",
        "date": "January 1962",
        "authors": [
            "DH Hubel",
            "TN Wiesel"
        ],
        "references": []
    },
    {
        "id": 284399676,
        "title": "The Position of Embedding Transformations in a Grammar",
        "abstract": "",
        "date": "January 1963",
        "authors": [
            "Charles J. Fillmore"
        ],
        "references": []
    },
    {
        "id": 319393405,
        "title": "An Introduction to Kolmogorov Complexity and Its Applications",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "Ming Li",
            "Paul M. B. Vit\u00e1nyi"
        ],
        "references": []
    },
    {
        "id": 220874758,
        "title": "The Intersection of Finite State Automata and Definite Clause Grammars.",
        "abstract": "Bernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers. In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable). Furthermore we discuss approaches to cope with the problem.",
        "date": "April 1995",
        "authors": [
            "Gertjan van Noord"
        ],
        "references": [
            228569838,
            220898271,
            2481913,
            2472596,
            240083487,
            234778865,
            228057764,
            222464606,
            221591692,
            220874195
        ]
    },
    {
        "id": 220816736,
        "title": "Decision Tree Parsing using a Hidden Derivation Model.",
        "abstract": "Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years.",
        "date": "January 1994",
        "authors": [
            "Frederick Jelinek",
            "John D. Lafferty",
            "David M. Magerman",
            "Robert Mercer"
        ],
        "references": [
            234825459,
            234799945,
            220875062,
            310606656,
            288345724,
            242396917,
            240310918,
            235961479,
            234831828,
            234819176
        ]
    },
    {
        "id": 38358970,
        "title": "A Universal Prior for Integers and Estimation by Minimum Description Length",
        "abstract": "An earlier introduced estimation principle, which calls for minimization of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated real-valued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision.",
        "date": "November 1982",
        "authors": [
            "Jorma Rissanen"
        ],
        "references": [
            256720488,
            243683368,
            238651290,
            236177887,
            226994378,
            222812561,
            38358303,
            38358297,
            31504475,
            3479364
        ]
    },
    {
        "id": 2607825,
        "title": "A Corpus-based Probabilistic Grammar with Only Two Non-terminals",
        "abstract": "The availability of large, syntactically-bracketed corpora such as the Penn Tree Bank affords us the opportunity to automatically build or train broad-coverage grammars, and in particular to train probabilistic grammars. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. To make maximal use of context, we have automatically constructed, from the Penn Tree Bank version 2, a grammar in which the symbols S and NP are the only real nonterminals, and the other non-terminals or grammatical nodes are in effect embedded into the right-hand-sides of the S and NP rules. For example, one of the rules extracted from the tree bank would be S -? NP VBX JJ CC VBX NP [1] (where NP is a non-terminal and the other symbols are terminals -- part-of-speech tags of the Tree Bank). The most common structure in the Tree Bank associated with...",
        "date": "December 1999",
        "authors": [
            "Satoshi Sekine",
            "Ralph Grishman"
        ],
        "references": [
            234825459,
            220817598,
            30379057,
            229423221,
            220875171,
            2757051,
            2513987,
            2479574
        ]
    },
    {
        "id": 2475419,
        "title": "A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis",
        "abstract": "We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of LexicalFunctional Grammar (LFG). We start by summarizing the original DOP model for tree represen- tations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.",
        "date": "May 2002",
        "authors": [
            "Rens Bod",
            "Ronald M Kaplan"
        ],
        "references": [
            230876175,
            221101811,
            290163364,
            243646226,
            243646220,
            243448719,
            239031201,
            238727771,
            221605419,
            220488267
        ]
    },
    {
        "id": 274364636,
        "title": "Maximum-Entropy and Bayesian Methods in Science and Engineering",
        "abstract": "Maximum entropy is presented as a universal method of finding a \u201cbest\u201d positive distribution constrained by incomplete data. The generalised entropy \u2211(f - m - f log(f/m))) is the only form which selects acceptable distributions f in particular cases. It holds even if f is not normalised, so that maximum entropy applies directly to physical distributions other than probabilities. Furthermore, maximum entropy should also be used to select \u201cbest\u201d parameters if the underlying model m has such freedom.",
        "date": "January 1991",
        "authors": [
            "Eddie Shoesmith",
            "Gary J. Erickson",
            "C. Ray Smith"
        ],
        "references": [
            258941379,
            220024164,
            220022167,
            38366114
        ]
    },
    {
        "id": 220312496,
        "title": "Grammatical inference by Hill Climbing",
        "abstract": "A cost function is developed, based on information-theoretic concepts, that measures the complexity of a stochastic context-free grammar, as well as the discrepancy between its language and a given stochastic language sample. This function is used to guide a search procedure that finds simple grammars whose languages are good fits to a sample. Reasonable results have been obtained in a variety of cases, including parenthesis and addition strings, Basic English (the first 25 sentences in English Through Pictures) and chain-encoded chromosome boundaries.",
        "date": "December 1976",
        "authors": [
            "Craig M. Cook",
            "Azriel Rosenfeld",
            "Alan Aronson"
        ],
        "references": [
            235182427,
            235026880,
            229124689,
            35846898,
            34616409,
            34126135,
            33711622,
            31787546
        ]
    },
    {
        "id": 220049445,
        "title": "An Introduction to Hidden Markov Models",
        "abstract": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.",
        "date": "February 1986",
        "authors": [
            "Lawrence Rabiner",
            "B.H. Juang"
        ],
        "references": [
            316801187,
            313220763,
            264569795,
            250754760,
            244958264,
            244418125,
            243619226,
            238986049,
            238287992,
            235890146
        ]
    },
    {
        "id": 38364607,
        "title": "A Maximization Technique Occurring in Statistical Analysis of Probabilistic Functions of Markov Chains",
        "abstract": "",
        "date": "February 1970",
        "authors": [
            "Leonard E. Baum",
            "Ted Petrie",
            "George Soules",
            "Norman Weiss"
        ],
        "references": []
    },
    {
        "id": 35846898,
        "title": "A study of grammatical inference /",
        "abstract": "Thesis (Ph. D.)--Stanford University, 1969. Bibliography: p. 161-166.",
        "date": "January 1969",
        "authors": [
            "J. J. (James J.) Horning"
        ],
        "references": []
    },
    {
        "id": 236157548,
        "title": "Factorial Hidden Markov Models",
        "abstract": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.",
        "date": "January 1995",
        "authors": [
            "Zoubin Ghahramani",
            "Michael Jordan"
        ],
        "references": [
            312978177,
            312913810,
            309630116,
            284531151,
            277292010,
            273692724,
            271512969,
            263072663,
            248192679,
            246366900
        ]
    },
    {
        "id": 230876175,
        "title": "Lexical-Functional Grammar: A Formal System for Grammatical Representation",
        "abstract": "",
        "date": "January 1982",
        "authors": [
            "Ronald M Kaplan",
            "Joan Bresnan"
        ],
        "references": [
            334364347,
            261985605,
            301690165,
            291095294,
            290736814,
            284037639,
            281263511,
            248150963,
            248030084,
            247017654
        ]
    },
    {
        "id": 220817598,
        "title": "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars.",
        "abstract": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).",
        "date": "January 1991",
        "authors": [
            "Ezra Black",
            "Steven P. Abney",
            "D. Flickenger",
            "Claudia Gdaniec"
        ],
        "references": []
    },
    {
        "id": 2912527,
        "title": "Probabilistic Parsing for German using Sister-Head Dependencies",
        "abstract": "We present a probabilistic parsing model for German trained on the Negra treebank. We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German. Learning curves show that this effect is not due to lack of training data. We propose an alternative model that uses sister-head dependencies instead of head-head dependencies. This model outperforms the baseline, achieving a labeled precision and recall of up to 74%. This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra. 1",
        "date": "June 2003",
        "authors": [
            "Amit Dubey",
            "Frank Keller"
        ],
        "references": [
            220017637,
            2868328,
            2558980,
            269428831,
            230876653,
            220690966,
            37705055,
            2803418,
            2637672,
            2564663
        ]
    },
    {
        "id": 2522353,
        "title": "Factorial Hidden Markov Models",
        "abstract": "We present a framework for learning in hidden Markov models with distributed state representations. Within this framework, we derive a learning algorithm based on the Expectation--Maximization (EM) procedure for maximum likelihood estimation. Analogous to the standard Baum-Welch update rules, the M-step of our algorithm is exact and can be solved analytically.However, due to the combinatorial nature of the hidden state representation, the exact E-step is intractable. A simple and tractable mean field approximation is derived. Empirical results on a set of problems suggest that both the mean field approximation and Gibbs sampling are viable alternatives to the computationally expensive exact algorithm.",
        "date": "August 2002",
        "authors": [
            "Zoubin Ghahramani",
            "Michael Jordan"
        ],
        "references": [
            14896130,
            2754791,
            2263315,
            239039429,
            223927652,
            221619832,
            220049445,
            2439047
        ]
    },
    {
        "id": 2445064,
        "title": "Valence Induction with a Head-Lexicalized PCFG",
        "abstract": "Introduction This chapter explains the theory underlying the lexicon learning system which was implemented at the University of Stuttgart, and reviews the experiment with the induction of a German lexicon which were performed in the Sparkle project. 1 The problems which motivate our work relate to the complexity and size of the lexicon; to the massive ambiguity in grammatical analysis which is observed in naturally occurring language, particularly written language; and to the scale of the texts which we would like to analyze computationally. We believe that in order to address these problems, computational linguistic technologies should be based on a combination of symbolic and numerical modeling, should embody approximations together with exact theories, and should constructed in a way which makes it possible to learn much of the information which is deployed from large samples of language use. In the research described in this chapter, probability theory an",
        "date": "June 1998",
        "authors": [
            "Glenn Carroll",
            "Mats Rooth",
            "Detlef Prescher",
            "Marc Light"
        ],
        "references": [
            271015410,
            248480554,
            243048495,
            230876357,
            230875878,
            230875831,
            221995817,
            215721461,
            44364210,
            3178307
        ]
    },
    {
        "id": 226171158,
        "title": "The Induction of Dynamical Recognizers",
        "abstract": "A higher order recurrent neural network architecture learns to recognize and generate languages after being trained on categorized exemplars. Studying these networks from the perspective of dynamical systems yields two interesting discoveries: First, a longitudinal examination of the learning process illustrates a new form of mechanical inference: Induction by phase transition. A small weight adjustment causes a bifurcation in the limit behavior of the network. This phase transition corresponds to the onset of the network''s capacity for generalizing to arbitrary-length strings. Second, a study of the automata resulting from the acquisition of previously published training sets indicates that while the architecture is not guaranteed to find a minimal finite automaton consistent with the given exemplars, which is an NP-Hard problem, the architecture does appear capable of generating non-regular languages by exploiting fractal and chaotic dynamics. I end the paper with a hypothesis relating linguistic generative capacity to the behavioral regimes of non-linear dynamical systems.",
        "date": "January 1991",
        "authors": [
            "Jordan Pollack"
        ],
        "references": [
            299483779,
            246799152,
            300065692,
            285365234,
            279233482,
            279166427,
            265670591,
            246899051,
            245770488,
            244446619
        ]
    },
    {
        "id": 239062408,
        "title": "The Cascade-Correlation Learning Algorithm",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "Scott E. Fahlman",
            "Christian Lebiere"
        ],
        "references": []
    },
    {
        "id": 238833309,
        "title": "Learning State Space Trajectories in Recurrent Neural Networks",
        "abstract": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.",
        "date": "June 1989",
        "authors": [
            "Barak A. Pearlmutter"
        ],
        "references": [
            252260043,
            270447299,
            267137555,
            257633383,
            257392775,
            256919078,
            256594502,
            256353255,
            243732587,
            242909887
        ]
    },
    {
        "id": 220500076,
        "title": "Induction of Finite-State Languages Using Second-Order Recurrent Networks",
        "abstract": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length.",
        "date": "May 1992",
        "authors": [
            "Raymond L. Watrous",
            "Gary Kuhn"
        ],
        "references": [
            317083799,
            243698906,
            241350552,
            239594484,
            222449846,
            220270149,
            3527788,
            2775093
        ]
    },
    {
        "id": 223416710,
        "title": "Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability",
        "abstract": "Many neural net learning algorithms aim at finding \u201csimple\u201d nets to explain training data. The expectation is that the \u201csimpler\u201d the networks, the better the generalization on test data (\u2192 Occam's razor). Previous implementations, however, use measures for \u201csimplicity\u201d that lack the power, universality and elegance of those based on Kolmogorov complexity and Solomonoff's algorithmic probability. Likewise, most previous approaches (especially those of the \u201cBayesian\u201d kind) suffer from the problem of choosing appropriate priors. This paper addresses both issues. It first reviews some basic concepts of algorithmic complexity theory relevant to machine learing, and how the Solomonoff-Levin distribution (or universal prior) deals with the prior problem. The universal prior leads to a probabilistic method for finding \u201calgorithmically simple\u201d problem solutions with high generalization capability. The method is based on Levin complexity (a time-bounded generalization of Kolmogorov complexity) and inspired by Levin's optimal universal search algorithm. For a given problem, solution candidates are computed by efficient \u201cself-sizing\u201d programs that influence their own runtime and storage size. The probabilistic search algorithm finds the \u201cgood\u201d programs (the ones quickly computing algorithmically probable solutions fitting the training data). Simulations focus on the task of discovering \u201calgorithmically simple\u201d neural networks with low Kolmogorov complexity and high generalization capability. It is demonstrated that the method, at least with certain toy problems where it is computationally feasible, can lead to generalization results unmatchable by previous neural network algorithms. Much remains to be done, however, to make large scale applications and \u201cincremental learning\u201d feasible. \u00a9 1997 Elsevier Science Ltd.",
        "date": "July 1997",
        "authors": [
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            319393405,
            284090758,
            279233597,
            252995638,
            247048942,
            322622397,
            321559016,
            319770146,
            316805467,
            312662323
        ]
    },
    {
        "id": 220500234,
        "title": "Learning Complex, Extended Sequences Using the Principle of History Compression",
        "abstract": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to \u201cdivide and conquer\u201d by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets.",
        "date": "March 1992",
        "authors": [
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            273515529,
            243683010,
            230784756,
            230587667,
            224929634,
            220340310,
            10698901,
            5598777,
            3303030,
            3302210
        ]
    },
    {
        "id": 220500087,
        "title": "First-Order Recurrent Neural Networks and Deterministic Finite State Automata",
        "abstract": "We examine the correspondence between first-order recurrent neural networks and deterministic finite state automata. We begin with the problem of inducing deterministic finite state automata from finite training sets, that include both positive and negative examples, an NP-hard problem (Angluin and Smith 1983). We use a neural network architecture with two recurrent layers, which we argue can approximate any discrete-time, time-invariant dynamic system, with computation of the full gradient during learning. The networks are trained to classify strings as belonging or not belonging to the grammar. The training sets used contain only short strings, and the sets are constructed in a way that does not require a priori knowledge of the grammar. After training, the networks are tested using various test sets with strings of length up to 1000, and are often able to correctly classify all the test strings. These results are comparable to those obtained with second-order networks (Giles et al. 1992; Watrous and Kuhn 1992a; Zeng et al. 1993). We observe that the networks emulate finite state automata, confirming the results of other authors, and we use a vector quantization algorithm to extract deterministic finite state automata after training and during testing of the networks, obtaining a table listing the start state, accept states, reject states, all transitions from the states, as well as some useful statistics. We examine the correspondence between finite state automata and neural networks in detail, showing two major stages in the learning process. To this end, we use a graphics module, which graphically depicts the states of the network during the learning and testing phases. We examine the networks' performance when tested on strings much longer than those in the training set, noting a measure based on clustering that is correlated to the stability of the networks. Finally, we observe that with sufficiently long training times, neural networks can become true finite state automata, due to the attractor structure of their dynamics.",
        "date": "November 1994",
        "authors": [
            "Peter Manolios",
            "Robert Fanelli"
        ],
        "references": [
            268036430,
            243698906,
            221620263,
            220500076,
            313667012,
            243743707,
            239594484,
            229091480,
            224740344,
            222797300
        ]
    },
    {
        "id": 220359636,
        "title": "Experimental Comparison of the Effect of Order in Recurrent Neural Networks.",
        "abstract": "There has been much interest in increasing the computational power of neural networks. In addition there has been much interest in \u201cdesigning\u201d neural networks better suited to particular problems. Increasing the \u201corder\u201d of the connectivity of a neural network permits both. Though order has played a significant role in feedforward neural networks, its role in dynamically driven recurrent networks is still being understood. This work explores the effect of order in learning grammars. We present an experimental comparison of first order and second order recurrent neural networks, as applied to the task of grammatical inference. We show that for the small grammars studied these two neural net architectures have comparable learning and generalization power, and that both are reasonably capable of extracting the correct finite state automata for the language in question. However, for a larger randomly-generated ten-state grammar, second order networks significantly outperformed the first order networks, both in convergence time and generalization capability. We show that these networks learn faster the more neurons they have (our experiments used up to 10 hidden neurons), but that the solutions found by smaller networks are usually of better quality (in terms of generalization performance after training). Second order nets have the advantage that they converge more quickly to a solution and can find it more reliably than first order nets, but that the second order solutions tend to be of poorer quality than those of the first order if both architectures are trained to the same error tolerance. Despite this, second order nets can more successfully extract finite state machines using heuristic clustering techniques applied to the internal state representations. We speculate that this may be due to restrictions on the ability of first order architecture to fully make use of its internal state representation power and that this may have implications for the performance of the two architectures when scaled up to larger problems.",
        "date": "August 1993",
        "authors": [
            "Clifford Miller",
            "C. Lee Giles"
        ],
        "references": []
    },
    {
        "id": 243743707,
        "title": "Learning Internal Representations by Error Propagation",
        "abstract": "",
        "date": "July 1986",
        "authors": [
            "D. E. Rumelhart",
            "G. H. Hinton",
            "Williams RJ"
        ],
        "references": []
    },
    {
        "id": 242353012,
        "title": "NETtalk: a Parallel Network that Learns to Read Aloud",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "T. J. Sejnowski",
            "C. R. Rosenberg"
        ],
        "references": []
    },
    {
        "id": 239059398,
        "title": "Leaning internal representations by back-propagating errors",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "D. E. Rumelhart",
            "G. E. Hinton",
            "R. J. Williams"
        ],
        "references": []
    },
    {
        "id": 230876307,
        "title": "The case for interactionism in language processing",
        "abstract": "",
        "date": "January 1987",
        "authors": [
            "James L Mcclelland"
        ],
        "references": [
            16000512
        ]
    },
    {
        "id": 222438973,
        "title": "Implicit Learning of Artifical Grammars",
        "abstract": "Two experiments were carried out to investigate the process by which Ss respond to the statistical nature of the stimulus array, a process defined as \u201cimplicit learning\u201d. An artificial grammar was used to generate the stimuli. Experiment I showed that Ss learned to become increasingly sensitive to the grammatical structure of the stimuli, but little was revealed about the nature of such learning. Experiment II showed that information gathered about the grammar in a memorization task could be extended to a recognition task with new stimuli. Various analyses of the data strongly implied that Ss were learning to respond to the general grammatical nature of the stimuli, rather than learning to respond according to specific coding systems imposed upon the stimuli. It was argued that this \u201cimplicit\u201d learning is similar in nature to the \u201cdifferentiation\u201d process of perceptual learning espoused by Gibson and Gibson (1955).",
        "date": "December 1967",
        "authors": [
            "Arthur Reber"
        ],
        "references": [
            240148362,
            232567647,
            230876324,
            222365967,
            49552016,
            9955653,
            9552084,
            9455791,
            9225786
        ]
    },
    {
        "id": 16609657,
        "title": "Neurons With Graded Response Have Collective Computational Properties Like Those of Two-State Neurons",
        "abstract": "A model for a large network of \"neurons\" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.",
        "date": "June 1984",
        "authors": [
            "John J Hopfield"
        ],
        "references": [
            16246447,
            6026283,
            240302952,
            234929061,
            234014356,
            226859579,
            23055649,
            18426219
        ]
    },
    {
        "id": 345402887,
        "title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "David E. Rumelhart",
            "James L Mcclelland"
        ],
        "references": []
    },
    {
        "id": 243243435,
        "title": "A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition",
        "abstract": "A natural, collective neural model for Content Addressable Memory (CAM) and pattern recognition is described. The model uses nonsymmetrical, bounded synaptic connection matrices and continuous valued neurons. The problem of specifying a synaptic connection matrix suitable for CAM is formulated as an optimization problem, and recent techniques of Hopfield are used to perform the optimization. This treatment naturally leads to two interacting neural nets. The first net is a symmetrically connected net (master net) containing information about the desired fixed points or memory vectors. The second net is, in general, a nonsymmetric net (slave net), whose synapse values are determined by the master net, and is the net that actually performs the CAM task. The two nets acting together are an example of neural self-organization. Many advantages of this master/slave approach are described, one of which is that nonsymmetric synaptic matrices offer a greater potential for relating formal neural modeling to neurophysiology. In addition, it seems that this approach offers advantages in application to pattern recognition problems due to the new ability to sculpt basins of attraction. The simple structure of the master net connections indicates that this approach presents no additional problems in reduction to hardware when compared to single net implementations.",
        "date": "October 1986",
        "authors": [
            "Alan Lapedes",
            "Robert M. Farber"
        ],
        "references": [
            22654874,
            18399991,
            16609657,
            16246447,
            222290338,
            18834037
        ]
    },
    {
        "id": 3115638,
        "title": "Characteristics of Random Nets of Analog Neuron-Like Elements",
        "abstract": "The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net\u00c2\u00bfin contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.",
        "date": "December 1972",
        "authors": [
            "Shun-Ichi Amarimber"
        ],
        "references": [
            305815156,
            304541567,
            285069626,
            275807826,
            266534838,
            253865471,
            247588557,
            235246944,
            234997859,
            225990580
        ]
    },
    {
        "id": 319770414,
        "title": "Identity Mappings in Deep Residual Networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10/100, and a 200-layer ResNet on ImageNet.",
        "date": "March 2016",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "references": [
            284579051,
            284476548,
            269935397,
            265908778,
            228102719,
            13853244,
            319770191,
            306218037,
            301874967,
            286512696
        ]
    },
    {
        "id": 319770216,
        "title": "Deconstructing the Ladder Network Architecture",
        "abstract": "Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of combinator function in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization due to the injection of noise in each layer. Furthermore, we present a new type of combinator functions that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively.",
        "date": "January 2016",
        "authors": [
            "Mohammad Pezeshki",
            "Linxi Fan",
            "Philemon Brakel",
            "Aaron Courville"
        ],
        "references": []
    },
    {
        "id": 319770191,
        "title": "FitNets: Hints for Thin Deep Nets",
        "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
        "date": "December 2015",
        "authors": [
            "Adriana Romero",
            "Nicolas Ballas",
            "Samira Ebrahimi Kahou",
            "Antoine Chassang"
        ],
        "references": []
    },
    {
        "id": 309076254,
        "title": "Gradientbased learning applied to document recognition",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "L. Yann",
            "L. Bottou",
            "Y. Bengio",
            "Patrick Haffner"
        ],
        "references": []
    },
    {
        "id": 306281834,
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "abstract": "Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error.",
        "date": "June 2016",
        "authors": [
            "Christian Szegedy",
            "Vincent Vanhoucke",
            "Sergey Ioffe",
            "Jon Shlens"
        ],
        "references": [
            305196650,
            275279789,
            270454670,
            265908778,
            259212328,
            233730646,
            319770430,
            319770272,
            319770198,
            319770183
        ]
    },
    {
        "id": 308262743,
        "title": "What You Get Is What You See: A Visual Markup Decompiler",
        "abstract": "Building on recent advances in image caption generation and optical character recognition (OCR), we present a general-purpose, deep learning-based system to decompile an image into presentational markup. While this task is a well-studied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domain-specific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples.",
        "date": "September 2016",
        "authors": [
            "Yuntian Deng",
            "Anssi Kanervisto",
            "Alexander M. Rush"
        ],
        "references": [
            319770160,
            312216100,
            309551291,
            307747289,
            305334305,
            301898469,
            319770411,
            311611182,
            308813785,
            301848381
        ]
    },
    {
        "id": 305334401,
        "title": "Hierarchical Attention Networks for Document Classification",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Zichao Yang",
            "Diyi Yang",
            "Chris Dyer",
            "Xiaodong He"
        ],
        "references": [
            301408881,
            281607724,
            273067823,
            272091377,
            319769995,
            301446024,
            301445948,
            287607356,
            285021947,
            284039049
        ]
    },
    {
        "id": 220874004,
        "title": "Moses: Open Source Toolkit for Statistical Machine Translation.",
        "abstract": "We describe an open-source toolkit for sta- tistical machine translation whose novel contributions are (a) support for linguisti- cally motivated factors, (b) confusion net- work decoding, and (c) efficient data for- mats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.",
        "date": "June 2007",
        "authors": [
            "Philipp Koehn",
            "Hieu Hoang",
            "Alexandra Birch",
            "Chris Callison-Burch"
        ],
        "references": [
            221013253,
            220816913,
            220816875,
            2876877,
            240911908,
            220839665,
            4208266,
            2884718,
            2863295,
            2602251
        ]
    },
    {
        "id": 308646556,
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
        "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
        "date": "September 2016",
        "authors": [
            "Yonghui Wu",
            "Mike Schuster",
            "Zhifeng Chen",
            "Quoc V. Le"
        ],
        "references": [
            306093534,
            306093072,
            303657108,
            301404440,
            287853408,
            272195143,
            270877878,
            269280447,
            266225209,
            265252627
        ]
    },
    {
        "id": 306093856,
        "title": "Generating Sentences from a Continuous Space",
        "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
        "date": "January 2016",
        "authors": [
            "Samuel R. Bowman",
            "Luke Vilnis",
            "Oriol Vinyals",
            "Andrew Dai"
        ],
        "references": [
            319770438,
            319770160,
            279310212,
            275280239,
            265252627,
            263048923,
            263012109,
            228971629,
            228452494,
            224246503
        ]
    },
    {
        "id": 284788001,
        "title": "rnn : Recurrent Library for Torch",
        "abstract": "The rnn package provides components for implementing a wide range of Recurrent Neural Networks. It is built withing the framework of the Torch distribution for use with the nn package. The components have evolved from 3 iterations, each adding to the flexibility and capability of the package. All component modules inherit either the AbstractRecurrent or AbstractSequencer classes. Strong unit testing, continued backwards compatibility and access to supporting material are the principles followed during its development. The package is compared against existing implementations of two published papers.",
        "date": "November 2015",
        "authors": [
            "Nicholas L\u00e9onard",
            "Sagar Waghmare",
            "Yang Wang"
        ],
        "references": [
            300873377,
            273640320,
            228102719,
            220017637,
            13853244,
            2903062,
            319770411,
            319770184,
            312607081,
            303432735
        ]
    },
    {
        "id": 312759848,
        "title": "DSSD : Deconvolutional Single Shot Detector",
        "abstract": "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \\times 513$ input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.",
        "date": "January 2017",
        "authors": [
            "Cheng-Yang Fu",
            "Wei Liu",
            "Ananth Ranga",
            "Ambrish Tyagi"
        ],
        "references": [
            308278279,
            305638531,
            319770820,
            319770430,
            319770291,
            319769911,
            308278045,
            308277376,
            308277265,
            303822227
        ]
    },
    {
        "id": 319770416,
        "title": "DRAW: A Recurrent Neural Network For Image Generation",
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",
        "date": "February 2015",
        "authors": [
            "Karol Gregor",
            "Ivo Danihelka",
            "Alex Graves",
            "Daan Wierstra"
        ],
        "references": []
    },
    {
        "id": 319769911,
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. The code will be released.",
        "date": "January 2016",
        "authors": [
            "Shaoqing Ren",
            "Kaiming He",
            "Ross Girshick",
            "Jian Sun"
        ],
        "references": []
    },
    {
        "id": 313760952,
        "title": "Visual Discovery at Pinterest",
        "abstract": "Over the past three years Pinterest has experimented with several visual search and recommendation services, including Related Pins (2014), Similar Looks (2015), Flashlight (2016) and Lens (2017). This paper presents an overview of our visual discovery engine powering these services, and shares the rationales behind our technical and product decisions such as the use of object detection and interactive user interfaces. We conclude that this visual discovery engine significantly improves engagement in both search and recommendation tasks.",
        "date": "February 2017",
        "authors": [
            "Andrew Zhai",
            "Dmitry Kislyuk",
            "Yushi Jing",
            "Michael Feng"
        ],
        "references": []
    },
    {
        "id": 312430297,
        "title": "Fast Single Shot Detection and Pose Estimation",
        "abstract": "For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or tracking parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for slidingwindow detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose estimation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the offset for its location, and the approximate pose are all estimated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose estimation (42.4% 8 View mAVP on Pascal 3D+ [21] ) and significantly faster (46 frames per second (FPS) on a TITAN X GPU). This approach to detection and rough pose estimation is fast and accurate enough to be widely applied as a pre-processing step for tasks including high-accuracy pose estimation, object tracking and localization, and vSLAM.",
        "date": "October 2016",
        "authors": [
            "Patrick Poirson",
            "Phil Ammirato",
            "Cheng-Yang Fu",
            "Wei Liu"
        ],
        "references": [
            308278279,
            301197096,
            277023129,
            265295439,
            260106931,
            224635428,
            221363626,
            221111526,
            221110795,
            220660293
        ]
    },
    {
        "id": 308884050,
        "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions",
        "abstract": "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.",
        "date": "January 2016",
        "authors": [
            "Marcin Junczys-Dowmunt",
            "Tomasz Dwojak",
            "Hieu Hoang"
        ],
        "references": [
            306094414,
            281427452,
            265252627,
            257943380,
            256847566,
            308646556,
            306093962,
            306093632,
            270878335,
            267627635
        ]
    },
    {
        "id": 308807302,
        "title": "Vocabulary Selection Strategies for Neural Machine Translation",
        "abstract": "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.",
        "date": "September 2016",
        "authors": [
            "Gurvan L'Hostis",
            "David Grangier",
            "Michael Auli"
        ],
        "references": [
            308109831,
            307955489,
            306094217,
            306093962,
            306093774,
            306093632,
            306093359,
            303969947,
            302879092,
            301448990
        ]
    },
    {
        "id": 319769995,
        "title": "End-To-End Memory Networks",
        "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",
        "date": "March 2015",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Jason Weston",
            "Rob Fergus"
        ],
        "references": []
    },
    {
        "id": 319769905,
        "title": "Sequence Level Training with Recurrent Neural Networks",
        "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.",
        "date": "January 2016",
        "authors": [
            "Marc'Aurelio Ranzato",
            "Sumit Chopra",
            "Michael Auli",
            "Wojciech Zaremba"
        ],
        "references": []
    },
    {
        "id": 310626854,
        "title": "Convolutional Neural Network Language Models",
        "abstract": "",
        "date": "November 2016",
        "authors": [
            "Quan Ngoc Pham",
            "Germ\u00e1n Kruszewski",
            "Gemma Boleda"
        ],
        "references": [
            301448983,
            301405212,
            273471942,
            273145632,
            270877878,
            266201822,
            228102719,
            220874004,
            220733157,
            220320709
        ]
    },
    {
        "id": 306094124,
        "title": "MetaMind Neural Machine Translation System for WMT 2016",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "James Bradbury",
            "Richard Socher"
        ],
        "references": [
            284097353,
            281427452,
            269416998,
            269280447,
            265385879,
            265252627,
            220874004,
            319770465,
            306092988,
            289758666
        ]
    },
    {
        "id": 306093962,
        "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
        "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English<->Czech, English<->German, English<->Romanian and English<->Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems.",
        "date": "June 2016",
        "authors": [
            "Rico Sennrich",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "references": [
            308849574,
            307587408,
            306094280,
            306093806,
            305334571,
            281427452,
            265252627,
            259334958,
            306093632,
            306092988
        ]
    },
    {
        "id": 306093909,
        "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
        "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.",
        "date": "March 2016",
        "authors": [
            "Junyoung Chung",
            "Kyunghyun Cho",
            "Y. Bengio"
        ],
        "references": [
            302569301,
            301448995,
            301445891,
            284097353,
            281607724,
            281427452,
            280912217,
            274380525,
            272195367,
            269280447
        ]
    },
    {
        "id": 329977566,
        "title": "T ree T alk : Composition and Compression of Trees for Image Descriptions",
        "abstract": "We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation, where the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.",
        "date": "December 2014",
        "authors": [
            "Polina Kuznetsova",
            "Vicente Ordonez",
            "Tamara L. Berg",
            "Choi Yejin"
        ],
        "references": [
            267748907,
            265295439,
            259882036,
            224354405,
            221303952,
            221101254,
            221012945,
            221012584,
            2588204,
            319770820
        ]
    },
    {
        "id": 312714920,
        "title": "Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections",
        "abstract": "This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of images that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.",
        "date": "September 2014",
        "authors": [
            "Yunchao Gong",
            "Liwei Wang",
            "Micah Hodosh",
            "Julia Hockenmaier"
        ],
        "references": [
            261263530,
            261168668,
            234131208,
            224577638,
            224164326,
            221362912,
            221361988,
            221361415,
            221346269,
            221303952
        ]
    },
    {
        "id": 308804607,
        "title": "CIDEr: Consensus-based image description evaluation",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Ramakrishna Vedantam",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "references": [
            319770160,
            308034527,
            307747289,
            270878844,
            268056398,
            238347229,
            238123710,
            227943302,
            221361415,
            221303985
        ]
    },
    {
        "id": 303721259,
        "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
        "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
        "date": "December 2014",
        "authors": [
            "Peter Young",
            "Alice Lai",
            "Micah Hodosh",
            "Julia Hockenmaier"
        ],
        "references": [
            268056398,
            262203675,
            221101775,
            221101532,
            220947158,
            220597358,
            200179363,
            200042392,
            2931224,
            2477223
        ]
    },
    {
        "id": 290345348,
        "title": "Image description using visual dependency representations",
        "abstract": "Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.",
        "date": "January 2013",
        "authors": [
            "D. Elliott",
            "F. Keller"
        ],
        "references": [
            221012945,
            262174543,
            228917508
        ]
    },
    {
        "id": 272194743,
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
        "date": "February 2015",
        "authors": [
            "Sergey Ioffe",
            "Christian Szegedy"
        ],
        "references": [
            267515250,
            266225209,
            234841757,
            234140324,
            233730646,
            231556969,
            230710850,
            221362377,
            220320677,
            215616968
        ]
    },
    {
        "id": 306093072,
        "title": "Pointing the Unknown Words",
        "abstract": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional count based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.",
        "date": "March 2016",
        "authors": [
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Ramesh Nallapati",
            "Bowen Zhou"
        ],
        "references": [
            265252627,
            262877889,
            319770348,
            306093632,
            303256841,
            285896121,
            281487270,
            278048272,
            277959429,
            267627635
        ]
    },
    {
        "id": 301404465,
        "title": "HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Carlos A. Colmenares",
            "Marina Litvak",
            "Amin Mantrach",
            "Fabrizio Silvestri"
        ],
        "references": [
            239061212,
            232556850,
            221101279,
            220875167,
            270877548,
            254000268,
            228057706,
            224890821,
            222635695,
            222397975
        ]
    },
    {
        "id": 279068977,
        "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset",
        "abstract": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which will be released to public soon. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.",
        "date": "June 2015",
        "authors": [
            "Baotian Hu",
            "Qingcai Chen",
            "Fangze Zhu"
        ],
        "references": [
            301404465,
            277722623,
            273471942,
            273388012,
            319770465,
            301231651,
            279068643,
            277722709,
            267627635,
            265554383
        ]
    },
    {
        "id": 306093832,
        "title": "Neural Summarization by Extracting Sentences and Words",
        "abstract": "Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.",
        "date": "March 2016",
        "authors": [
            "Jianpeng Cheng",
            "Mirella Lapata"
        ],
        "references": [
            306093072,
            272194092,
            265252627,
            259239818,
            257882504,
            224890667,
            221300393,
            221012766,
            220874485,
            220543248
        ]
    },
    {
        "id": 305334286,
        "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Sumit Chopra",
            "Michael Auli",
            "Alexander M. Rush"
        ],
        "references": [
            265252627,
            233922524,
            221101254,
            221013043,
            220817352,
            13853244,
            2919603,
            2539652,
            301445791,
            288644117
        ]
    },
    {
        "id": 288644117,
        "title": "Overcoming the lack of parallel data in sentence compression",
        "abstract": "A major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data. We address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained. In our corpus, the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence supervised systems which require a structural alignment between the input and output can be successfully trained. We also extend an existing unsupervised compression method with a learning module. The new system uses structured prediction to learn from lexical, syntactic and other features. An evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality. Also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline.",
        "date": "January 2013",
        "authors": [
            "Katja Filippova",
            "Y. Altun"
        ],
        "references": [
            253504417,
            233530390,
            230881862,
            221013235,
            220874485,
            220874480,
            220873293,
            220817352,
            2906946,
            292795276
        ]
    },
    {
        "id": 281487270,
        "title": "A Neural Attention Model for Abstractive Sentence Summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.",
        "date": "September 2015",
        "authors": [
            "Alexander M. Rush",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "references": [
            272091377,
            265252627,
            262877889,
            233922524,
            228102719,
            221101254,
            221013043,
            220874004,
            2919603,
            2906946
        ]
    },
    {
        "id": 278048272,
        "title": "Teaching Machines to Read and Comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
        "date": "June 2015",
        "authors": [
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Edward Grefenstette",
            "Lasse Espeholt"
        ],
        "references": [
            266201822,
            265252627,
            221012766,
            220874485,
            13853244,
            319770465,
            319769995,
            286965813,
            272422583,
            270877627
        ]
    },
    {
        "id": 277722709,
        "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents",
        "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\\footnote{Code for the three models described in this paper can be found at www.stanford.edu/~jiweil/ .",
        "date": "June 2015",
        "authors": [
            "Jiwei Li",
            "Minh-Thang Luong",
            "Dan Jurafsky"
        ],
        "references": [
            262245231,
            229060015,
            220873494,
            49244371,
            13853244,
            319770465,
            301408874,
            284568640,
            270878232,
            267627635
        ]
    },
    {
        "id": 319769811,
        "title": "Fast Algorithms for Convolutional Neural Networks",
        "abstract": "We derive a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. Specifically we derive algorithms for network layers with 3x3 kernels, which are the preferred kernel size for image recognition tasks. The best of our algorithms reduces arithmetic complexity up to 4X compared with direct convolution, while using small block sizes with limited transform overhead and high computational intensity. By comparison, FFT based convolution requires larger block sizes and significantly greater transform overhead to achieve an equal complexity reduction. We measure the accuracy of our algorithms to be sufficient for deep learning and inference with fp32 or fp16 data. Also, we demonstrate the practical application of our approach with a simple CPU implementation of our slowest algorithm using the Intel Math Kernel Library, and report VGG network inference results that are 2.6X as fast as Caffe with an effective utilization of 109%. We believe these are the highest utilization convnet inference results to date, and that they can be improved significantly with more implementation effort. We also believe that the new algorithms lend themselves equally well to GPU and FPGA implementations for both training and inference.",
        "date": "September 2015",
        "authors": [
            "Andrew Lavin",
            "Scott Gray"
        ],
        "references": [
            272195143,
            290224579,
            281895294,
            272194743,
            269994818,
            269932963,
            265535122,
            265385906,
            261880160,
            260470750
        ]
    },
    {
        "id": 311610318,
        "title": "Fast Algorithms for Convolutional Neural Networks",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Andrew Lavin",
            "Scott Gray"
        ],
        "references": [
            272195143,
            319770156,
            281895294,
            272194743,
            269994818,
            269932963,
            265535122,
            265385906,
            261880160,
            260470750
        ]
    },
    {
        "id": 311474948,
        "title": "LINQits: big data on little clients",
        "abstract": "We present LINQits, a flexible hardware template that can be mapped onto programmable logic or ASICs in a heterogeneous system-on-chip for a mobile device or server. Unlike fixed-function accelerators, LINQits accelerates a domain-specific query language called LINQ. LINQits does not provide coverage for all possible applications---however, existing applications (re-)written with LINQ in mind benefit extensively from hardware acceleration. Furthermore, the LINQits framework offers a graceful and transparent migration path from software to hardware. LINQits is prototyped on a 2W heterogeneous SoC called the ZYNQ processor, which combines dual ARM A9 processors with an FPGA on a single die in 28nm silicon technology. Our physical measurements show that LINQits improves energy efficiency by 8.9 to 30.6 times and performance by 10.7 to 38.1 times compared to optimized, multithreaded C programs running on conventional ARM A9 processors.",
        "date": "July 2013",
        "authors": [
            "Eric S. Chung",
            "John D. Davis",
            "Jaewon Lee"
        ],
        "references": [
            275575114,
            262424453,
            254021755,
            254005676,
            244302390,
            236345466,
            233932468,
            221643784,
            221351758,
            221225081
        ]
    },
    {
        "id": 308575069,
        "title": "Incremental, iterative data processing with timely dataflow",
        "abstract": "We describe the timely dataflow model for distributed computation and its implementation in the Naiad system. The model supports stateful iterative and incremental computations. It enables both low-latency stream processing and high-throughput batch processing, using a new approach to coordination that combines asynchronous and fine-grained synchronous execution. We describe two of the programming frameworks built on Naiad: GraphLINQ for parallel graph processing, and differential dataflow for nested iterative and incremental computations. We show that a generalpurpose system can achieve performance that matches, and sometimes exceeds, that of specialized systems.",
        "date": "September 2016",
        "authors": [
            "Derek G. Murray",
            "Frank McSherry",
            "Michael Isard",
            "Rebecca Isaacs"
        ],
        "references": [
            262233351,
            221351758,
            220910113,
            220851822,
            220538711,
            3297049,
            312775005,
            312455488,
            306548354,
            306209651
        ]
    },
    {
        "id": 307799813,
        "title": "Toward accelerating deep learning at scale using specialized hardware in the datacenter",
        "abstract": "",
        "date": "August 2015",
        "authors": [
            "Kalin Ovtcharov",
            "Olatunji Ruwase",
            "Joo-Young Kim",
            "Jeremy Fowers"
        ],
        "references": []
    },
    {
        "id": 306209959,
        "title": "Project adam: Building an efficient and scalable deep learning training system",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "T. Chilimbi",
            "Y. Suzue",
            "J. Apacible",
            "K. Kalyanaraman"
        ],
        "references": [
            261845797,
            254005623
        ]
    },
    {
        "id": 308034527,
        "title": "Long-term recurrent convolutional networks for visual recognition and description",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Jeff Donahue",
            "Lisa Anne Hendricks",
            "Sergio Guadarrama",
            "Marcus Rohrbach"
        ],
        "references": [
            319770438,
            319770160,
            329977566,
            329975395,
            319770465,
            319770291,
            319770249,
            319770183,
            313060232,
            312727767
        ]
    },
    {
        "id": 287850221,
        "title": "An efficient approach for assessing hyperparameter importance",
        "abstract": "The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that-even in very high- dimensional cases-most performance variation is attributable to just a few hyperparameters. Copyright \u00a9 (2014) by the International Machine Learning Society (IMLS) All rights reserved.",
        "date": "January 2014",
        "authors": [
            "F. Hutter",
            "Holger H Hoos",
            "Kevin Leyton-Brown"
        ],
        "references": [
            268364029,
            257459290,
            304786585,
            304781977,
            299673542,
            288906520,
            286988090,
            280046680,
            262425589,
            262395872
        ]
    },
    {
        "id": 317646849,
        "title": "Dictionary of Linguistics and Phonetics",
        "abstract": "",
        "date": "March 1998",
        "authors": [
            "Frances Ingemann",
            "David Crystal"
        ],
        "references": []
    },
    {
        "id": 287741874,
        "title": "TTS synthesis with bidirectional LSTM based Recurrent Neural Networks",
        "abstract": "Feed-forward, Deep neural networks (DNN)-based text-to-speech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems [1, 4]. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed.",
        "date": "January 2014",
        "authors": [
            "Y. Fan",
            "Yuang Qian",
            "Feng-Long Xie",
            "Frank K. Soong"
        ],
        "references": [
            261230379,
            243785932,
            3316656,
            271437175,
            261309981,
            260692539
        ]
    },
    {
        "id": 287059155,
        "title": "Fast and Robust Training of Recurrent Neural Networks for Offline Handwriting Recognition",
        "abstract": "In this paper we demonstrate a modified topology for long short-term memory recurrent neural networks that controls the shape of the squashing functions in gating units. We further propose an efficient training framework based on a mini-batch training on sequence level combined with a sequence chunking approach. The framework is evaluated on publicly available data sets containing English and French handwriting by utilizing a GPU based implementation. Speedups of more than 3x are achieved in training recurrent neural network models which outperform state of the art recognition results.",
        "date": "December 2014",
        "authors": [
            "Patrick Doetsch",
            "Michal Kozielski",
            "Hermann Ney"
        ],
        "references": [
            290925439,
            258713227,
            251735770,
            233730646,
            221472497,
            221126516,
            220861391,
            215616968,
            45660308,
            13853244
        ]
    },
    {
        "id": 285008105,
        "title": "Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies",
        "abstract": "",
        "date": "September 2009",
        "authors": [
            "J. Kolen",
            "S. Kremer"
        ],
        "references": []
    },
    {
        "id": 284867766,
        "title": "Distance measures for speech recognition, psychological and instrumental",
        "abstract": "",
        "date": "January 1976",
        "authors": [
            "P. Mermelstein"
        ],
        "references": []
    },
    {
        "id": 279815520,
        "title": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition",
        "abstract": "by Andrew K. Halberstadt.",
        "date": "August 2013",
        "authors": [
            "Halberstadt, Andrew K. (Andrew King),"
        ],
        "references": []
    },
    {
        "id": 279714069,
        "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
        "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters.",
        "date": "January 2014",
        "authors": [
            "H. Sak",
            "Andrew Senior",
            "F. Beaufays"
        ],
        "references": [
            224929675,
            224246411,
            12292425,
            3303030,
            288619064,
            261309981,
            222549933
        ]
    },
    {
        "id": 287781150,
        "title": "A dataset for research on short-text conversation",
        "abstract": "Natural language conversation is widely regarded as a highly difficult problem, which is usually attacked with either rule-based or learning-based models. In this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo (a popular Chinese mi-croblog service), which will be soon released to public. This dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. This dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset.",
        "date": "January 2013",
        "authors": [
            "Hao Wang",
            "Z. Lu",
            "H. Li",
            "Enhong Chen"
        ],
        "references": [
            234817432,
            230875890,
            228939660,
            220604729,
            220254272,
            262235199,
            222702919,
            221160077,
            221013184,
            200045867
        ]
    },
    {
        "id": 265209669,
        "title": "An Information Retrieval Approach to Short Text Conversation",
        "abstract": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \"intelligently\", when combined with a huge repository of conversation data from social media.",
        "date": "August 2014",
        "authors": [
            "Zongcheng Ji",
            "Zhengdong Lu",
            "Hang Li"
        ],
        "references": [
            287781150,
            264825261,
            234817432,
            228939660,
            326111275,
            288316493,
            270411066,
            262235199,
            246049244,
            243767009
        ]
    },
    {
        "id": 311469848,
        "title": "Recurrent neural network based language model",
        "abstract": "",
        "date": "January 2010",
        "authors": [
            "T. Mikolov",
            "Martin Karafi\u00e1t",
            "Lukas Burget",
            "J. Cernock\u00fd"
        ],
        "references": []
    },
    {
        "id": 270878844,
        "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
        "abstract": "This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14).",
        "date": "June 2014",
        "authors": [
            "Michael Denkowski",
            "Alon Lavie"
        ],
        "references": [
            254823190,
            252666388,
            307175243,
            270878886,
            270878171,
            262346958,
            262169914,
            242500664,
            239066288,
            228440799
        ]
    },
    {
        "id": 308871635,
        "title": "Mind's eye: A recurrent visual representation for image caption generation",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Xinlei Chen",
            "C. Lawrence Zitnick"
        ],
        "references": [
            319770438,
            307747289,
            272194766,
            268525230,
            264979485,
            241637478,
            234131319,
            221346269,
            221303952,
            221012945
        ]
    },
    {
        "id": 269935079,
        "title": "Adam: A Method for Stochastic Optimization",
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also ap- propriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.",
        "date": "December 2014",
        "authors": [
            "Diederik Kingma",
            "Jimmy Ba"
        ],
        "references": [
            258816388,
            236735729,
            228102719,
            220873867,
            220320677,
            319770229,
            319770184,
            267960550,
            260637318,
            255173850
        ]
    },
    {
        "id": 230690781,
        "title": "Object-Class Segmentation using Deep Convolutional Neural Networks",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "Hannes Schulz",
            "Sven Behnke"
        ],
        "references": [
            222823987,
            221618859,
            221362771,
            221080312,
            48199502,
            4082304,
            224716242,
            224135920,
            222817667,
            221364896
        ]
    },
    {
        "id": 221363369,
        "title": "Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts",
        "abstract": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of ob- ject categories. Inspired by the principles of efficient index- ing (bottom-up), robust matching (top-down), and ideas of compositionality, our approach learns a hierarchy of spa- tially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent fea- tures, we learn the statistically most significant composi- tions (parts composed of parts), which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally adjusting to the visual data. Lower layers are learned in a category-independent way to obtain com- plex, yet sharable visual building blocks, which is a crucial step towards a scalable representation. Higher layers of the hierarchy, on the other hand, are constructed by using spe- cific categories, achieving a category representation with a small number of highly generalizable parts that gained their structural flexibility through composition within the hierar- chy. Built in this way, new categories can be efficiently and continuously added to the system by adding a small number of parts only in the higher layers. The approach is demon- strated on a large collection of images and a variety of ob- ject categories. Detection results confirm the effectiveness and robustness of the learned parts.",
        "date": "June 2007",
        "authors": [
            "Sanja Fidler",
            "Ales Leonardis"
        ],
        "references": [
            242663551,
            220875359,
            220499751,
            243716653,
            232472619,
            227025232,
            221362706,
            221304728,
            220691569,
            220480308
        ]
    },
    {
        "id": 319770820,
        "title": "Histograms of Oriented Gradients for Human Detection",
        "abstract": "We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.",
        "date": "July 2005",
        "authors": [
            "Navneet Dalal",
            "Bill Triggs"
        ],
        "references": [
            246033882,
            227191648,
            222505795,
            221304792,
            220660132,
            215721498,
            22310482,
            7528498,
            4092547,
            3906077
        ]
    },
    {
        "id": 237417270,
        "title": "Discriminatively Trained Mixtures of Deformable Part Models",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "Pedro Felzenszwalb",
            "Ross Girshick",
            "David Allen Mcallester",
            "Deva Ramanan"
        ],
        "references": []
    },
    {
        "id": 221364242,
        "title": "Latent Hierarchical Structural Learning for Object Detection",
        "abstract": "We present a latent hierarchical structural learning method for object detection. An object is represented by a mixture of hierarchical tree models where the nodes rep- resent object parts. The nodes can move spatially to al- low both local and global shape deformations. The models can be trained discriminatively using latent structural SVM learning, where the latent variables are the node positions and the mixture component. But current learning methods are slow, due to the large number of parameters and latent variables, and have been restricted to hierarchies with two layers. In this paper we describe an incremental concave- convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently. We show that iCCCP leads to a simple training algorithm which avoids complex multi-stage layer-wise training, careful part selection, and achievesgoodperformance without requiring elaborateini- tialization. We perform object detection using our learnt models and obtain performance comparable with state-of- the-art methods when evaluatedon challengingpublic PAS- CAL datasets. We demonstratetheadvantagesofthree layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes.",
        "date": "June 2010",
        "authors": [
            "Long (Leo) Zhu",
            "Yuanhao Chen",
            "Alan L. Yuille",
            "William T. Freeman"
        ],
        "references": [
            244437609,
            224579210,
            224135930,
            221618673,
            221366761,
            221346169,
            221111859,
            220660094,
            215990336,
            41781863
        ]
    },
    {
        "id": 281327886,
        "title": "Histograms of Oriented Gradients for Human Detection",
        "abstract": "",
        "date": "June 2005",
        "authors": [
            "Navneet Dalal",
            "Bill Triggs"
        ],
        "references": [
            246033882,
            227191648,
            222505795,
            221304792,
            221111492,
            220660132,
            215721498,
            22310482,
            7528498,
            4092547
        ]
    },
    {
        "id": 262311378,
        "title": "KV-Cache: A Scalable High-Performance Web-Object Cache for Manycore",
        "abstract": "Latency and cost of Internet-based services are driving the proliferation of web-object caching. Memcached, the most broadly deployed web-object caching solution, is a key infrastructure component for many companies that offer services via the Web, such as Amazon, Facebook, Linked In, Twitter, Wikipedia, and YouTube. Its aim is to reduce service latency and improve processing capability on back-end data servers by caching immutable data closer to the client machines. Caching of key-value pairs is performed solely in memory. In this paper, we present a novel design for a high-performance web-object caching solution, KV-Cache, that is Memcache-protocol compliant. Our solution, based on TU Dresden's Fiasco. OC micro kernel operating system, offers scalability and performance that significantly exceeds that of its Linux-based counterpart. KV-Cache's highly optimized architecture benefits from truly \"absolute\" zero copy by eliminating any software memory copying at the kernel level or in the network stack, and only performing direct memory access (DMA) for each transmit and receive path. We report experimental results for the current prototype running on an Intel E5-based 32-core server platform. Our results show that KV-Cache offers significant performance advantages over optimized Memcached on Linux for commodity x86 server hardware.",
        "date": "December 2013",
        "authors": [
            "Daniel Waddington",
            "Juan A. Colmenares",
            "Jilong Kuang",
            "Fengguang Song"
        ],
        "references": [
            254029356,
            233765173,
            224262990,
            278914802,
            265109543,
            262411304,
            262317728,
            262159720,
            258847821,
            252067257
        ]
    },
    {
        "id": 262274001,
        "title": "NUMA-aware shared-memory collective communication for MPI",
        "abstract": "As the number of cores per node keeps increasing, it becomes increasingly important for MPI to leverage shared memory for intranode communication. This paper investigates the design and optimizations of MPI collectives for clusters of NUMA nodes. We develop performance models for collective communication using shared memory, and we develop several algorithms for various collectives. Experiments are conducted on both Xeon X5650 and Opteron 6100 InfiniBand clusters. The measurements agree with the model and indicate that different algorithms dominate for short vectors and long vectors. We compare our shared-memory allreduce with several traditional MPI implementations -- Open MPI, MPICH2, and MVAPICH2 -- that utilize system shared memory to facilitate interprocess communication. On a 16-node Xeon cluster and 8-node Opteron cluster, our implementation achieves on average 2.5X and 2.3X speedup over MVAPICH2, respectively. Our techniques enable an efficient implementation of collective operations on future multi- and manycore systems.",
        "date": "June 2013",
        "authors": [
            "Shigang Li",
            "Torsten Hoefler",
            "Marc Snir"
        ],
        "references": [
            228460998,
            221643558,
            221597341,
            305257851,
            262295686,
            248008104,
            245586762,
            242414880,
            234799330,
            221597419
        ]
    },
    {
        "id": 262159992,
        "title": "Traffic Management: A Holistic Approach to Memory Placement on NUMA Systems",
        "abstract": "NUMA systems are characterized by Non-Uniform Memory Access times, where accessing data in a remote node takes longer than a local access. NUMA hardware has been built since the late 80's, and the operating systems designed for it were optimized for access locality. They co-located memory pages with the threads that accessed them, so as to avoid the cost of remote accesses. Contrary to older systems, modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main concern for performance, as we discovered in this work. Instead, congestion on memory controllers and interconnects, caused by memory traffic from data-intensive applications, hurts performance a lot more. Because of that, memory placement algorithms must be redesigned to target traffic congestion. This requires an arsenal of techniques that go beyond optimizing locality. In this paper we describe Carrefour, an algorithm that addresses this goal. We implemented Carrefour in Linux and obtained performance improvements of up to 3.6 relative to the default kernel, as well as significant improvements compared to NUMA-aware patchsets available for Linux. Carrefour never hurts performance by more than 4% when memory placement cannot be improved. We present the design of Carrefour, the challenges of implementing it on modern hardware, and draw insights about hardware support that would help optimize system software on future NUMA systems.",
        "date": "April 2013",
        "authors": [
            "Mohammad Dashti",
            "Alexandra Fedorova",
            "Justin Funston",
            "Fabien Gaud"
        ],
        "references": [
            268223458,
            262329928,
            225769358,
            279843864,
            269135988,
            264159793,
            241623777,
            241623334,
            234826842,
            228743091
        ]
    },
    {
        "id": 258521061,
        "title": "NUMA-aware graph mining techniques for performance and energy efficiency",
        "abstract": "We investigate dynamic methods to improve the power and performance profiles of large irregular applications on modern multi-core systems. In this context, we study a large sparse graph application, Betweenness Centrality, and focus on memory behavior as core count scales. We introduce new techniques to efficiently map the computational demands onto non-uniform memory architectures (NUMA). Our dynamic design adapts to hardware topology and dramatically improves both energy and performance. These gains are more significant at higher core counts. We implement a scheme for adaptive data layout, which reorganizes the graph after observing parallel access patterns, and a dynamic task scheduler that encourages shared data between neighboring cores. We measure performance and energy consumption on a modern multi-core machine and observe that mean execution time is reduced by 51.2% and energy is reduced by 52.4%.",
        "date": "November 2012",
        "authors": [
            "Michael Frasca",
            "Kamesh Madduri",
            "Padma Raghavan"
        ],
        "references": [
            266467762,
            260686305,
            238753649,
            297378090,
            278718850,
            278712204,
            277370968,
            276959721,
            267974342,
            236377984
        ]
    },
    {
        "id": 224132799,
        "title": "Memphis: Finding and fixing NUMA-related performance problems on multi-core platforms",
        "abstract": "Until recently, most high-end scientific applications have been immune to performance problems caused by Non-Uniform Memory Access (NUMA). However, current trends in micro-processor design are pushing NUMA to smaller and smaller scales. This paper examines the current state of NUMA and makes several contributions. First, we summarize the performance problems that NUMA can present for multi-threaded applications and describe methods of addressing them. Second, we demonstrate that NUMA can indeed be a significant problem for scientific applications, showing that it can mean the difference between an application scaling perfectly and failing to scale at all. Third, we describe, in increasing order of usefulness, three methods of using hardware performance counters to aid in finding NUMA-related problems. Finally, we introduce Memphis, a data-centric toolset that uses Instruction Based Sampling to help pinpoint problematic memory accesses, and demonstrate how we used it to improve the performance of several production-level codes - HYCOM, XGC1 and CAM - by 13%, 23% and 24% respectively.",
        "date": "April 2010",
        "authors": [
            "Collin McCurdy",
            "Jeffrey Vetter"
        ],
        "references": [
            260053924,
            247884921,
            231080526,
            277286175,
            275483573,
            236221589,
            234217433,
            232636936,
            228743091,
            228367858
        ]
    },
    {
        "id": 286643249,
        "title": "Real-world concurrency",
        "abstract": "Some of the significant issues associated with the introduction of concurrent systems are discussed. The relationship of software development with concurrency depends on the location of its physical execution and where it is in the stack of abstraction and the business model that surrounds it. There are three fundamental ways to improve the performance of concurrent execution. These three fundamental ways of improving performance, include reducing unit of work, allow the system to continue doing work for a long period of time, and increase throughput. Concurrent execution can be used to perform useful work, while the operation is pending for long-running operations. Multiple concurrent executions of sequential logic can also be employed to accommodate simultaneous work, without using parallel logic to make a single operation faster.",
        "date": "November 2008",
        "authors": [
            "B. Cantrill",
            "J. Bonwick"
        ],
        "references": [
            220771058,
            270956407,
            242787082,
            220423924,
            220310240,
            220309900,
            213881289,
            200031673
        ]
    },
    {
        "id": 268461720,
        "title": "Traffic management",
        "abstract": "NUMA systems are characterized by Non-Uniform Memory Access times, where accessing data in a remote node takes longer than a local access. NUMA hardware has been built since the late 80's, and the operating systems designed for it were optimized for access locality. They co-located memory pages with the threads that accessed them, so as to avoid the cost of remote accesses. Contrary to older systems, modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main concern for performance, as we discovered in this work. Instead, congestion on memory controllers and interconnects, caused by memory traffic from data-intensive applications, hurts performance a lot more. Because of that, memory placement algorithms must be redesigned to target traffic congestion. This requires an arsenal of techniques that go beyond optimizing locality. In this paper we describe Carrefour, an algorithm that addresses this goal. We implemented Carrefour in Linux and obtained performance improvements of up to 3.6 relative to the default kernel, as well as significant improvements compared to NUMA-aware patchsets available for Linux. Carrefour never hurts performance by more than 4% when memory placement cannot be improved. We present the design of Carrefour, the challenges of implementing it on modern hardware, and draw insights about hardware support that would help optimize system software on future NUMA systems.",
        "date": "April 2013",
        "authors": [
            "Mohammad Dashti",
            "Alexandra Fedorova",
            "Justin Funston",
            "Fabien Gaud"
        ],
        "references": [
            262329928,
            221351802,
            221351728,
            220910230,
            220885003,
            220851700,
            44795231,
            3299627,
            2709106,
            279843864
        ]
    },
    {
        "id": 265478227,
        "title": "LAPACK users\u2019 guide",
        "abstract": "",
        "date": "January 1992",
        "authors": [
            "E. Anderson",
            "Zhengwu Bai",
            "C. Bischof"
        ],
        "references": []
    },
    {
        "id": 261020725,
        "title": "Optimizing Google's warehouse scale computers: The NUMA experience",
        "abstract": "Due to the complexity and the massive scale of modern warehouse scale computers (WSCs), it is challenging to quantify the performance impact of individual microarchitectural properties and the potential optimization benefits in the production environment. As a result of these challenges, there is currently a lack of understanding of the microarchitecture-workload interaction, leaving potentially significant performance on the table. This paper argues for a two-phase performance analysis methodology for optimizing WSCs that combines both an in-production investigation and an experimental load-testing study. To demonstrate the effectiveness of this two-phase approach, and to illustrate the challenges, methodologies and opportunities in optimizing modern WSCs, this paper investigates the impact of non-uniform memory access (NUMA) for several Google's key web-service workloads in large-scale production WSCs. Leveraging a newly-designed metric and continuous large-scale profiling in live datacenters, our production analysis demonstrates that NUMA has a significant impact (10-20%) on two important web-services: Gmail backend and web-search frontend. Our carefully designed load-test further reveals surprising tradeoffs between optimizing for NUMA performance and reducing cache contention.",
        "date": "February 2013",
        "authors": [
            "Lingjia Tang",
            "Jason Mars",
            "Xiao Zhang",
            "R. Hagmann"
        ],
        "references": [
            242455237,
            241623902,
            241623119,
            232634394,
            224168227,
            221005338,
            220909945,
            220884565,
            220799093,
            220771980
        ]
    },
    {
        "id": 256327878,
        "title": "ScaLAPACK user's guide",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "L S Blackford",
            "Jinhee Choi",
            "A Cleary",
            "E D'Azeuedo"
        ],
        "references": []
    },
    {
        "id": 255564380,
        "title": "Scalable stacking and learning for building deep architectures",
        "abstract": "Deep Neural Networks (DNNs) have shown remarkable success in pattern recognition tasks. However, parallelizing DNN training across computers has been dif?cult. We present the Deep Stack- ing Network (DSN), which overcomes the problem of paralleliz- ing learning algorithms for deep architectures. The DSN provides a method of stacking simple processing modules in buiding deep architectures, with a convex learning problem in each module. Ad- ditional ?ne tuning further improves the DSN, while introducing mi- nor non-convexity. Full learning in the DSN is batch-mode, making it amenable to parallel training over many machines and thus be scal- able over the potentially huge size of the training data. Experimental results on both the MNIST (image) and TIMIT (speech) classi?ca- tion tasks demonstrate that the DSN learning algorithm developed in this work is not only parallelizable in implementation but it also attains higher classi?cation accuracy than the DNN.",
        "date": "March 2012",
        "authors": [
            "li Deng",
            "Dong Yu",
            "John C. Platt"
        ],
        "references": [
            239784731,
            228533673,
            228518002,
            222467943,
            309349955,
            309076254,
            224226885,
            224174152,
            221491878,
            221487297
        ]
    },
    {
        "id": 240308775,
        "title": "Representation Learning: A Review and New Perspectives",
        "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
        "date": "August 2013",
        "authors": [
            "Y. Bengio",
            "Aaron Courville",
            "Pascal Vincent"
        ],
        "references": [
            319770626,
            319770387,
            319770305,
            319770183,
            319770174,
            311469666,
            310752071,
            309186321,
            306218037,
            305386599
        ]
    },
    {
        "id": 221653840,
        "title": "Model compression",
        "abstract": "Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classiers. Unfortunately, the space required to store this many clas- siers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea- ring aids). We present a method for \\compressing\" large, complex ensembles into smaller, faster models, usually with- out signican t loss in performance.",
        "date": "August 2006",
        "authors": [
            "Cristian Bucila",
            "Rich Caruana",
            "Alexandru Niculescu-Mizil"
        ],
        "references": [
            222467943,
            221346504,
            221345642,
            30876208,
            274189441,
            257826403,
            248192679,
            215721461,
            44366367,
            37685291
        ]
    },
    {
        "id": 233806999,
        "title": "Adaptive Mixture of Local Expert",
        "abstract": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.",
        "date": "February 1991",
        "authors": [
            "Robert Jacobs",
            "Michael Jordan",
            "Steven J. Nowlan",
            "Geoffrey E. Hinton"
        ],
        "references": [
            258402689,
            257631592,
            284697644,
            273922377,
            271636317,
            270447132,
            269301070,
            260732198,
            257705745,
            257690533
        ]
    },
    {
        "id": 239571798,
        "title": "Information processing in dynamical systems: Foundations of harmony theory",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "Paul Smolensky"
        ],
        "references": []
    },
    {
        "id": 224737607,
        "title": "Maximum mutual information estimation of hidden Markov parameters for speech recognition",
        "abstract": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented, comparing this method with maximum-likelihood estimation. In the example given, estimating parameters by maximizing mutual information resulted in the training script having a probability 10**1 **8 **9 times greater than when parameters were estimated by maximum likelihood estimation. Moreover, training by maximizing mutual information resulted in 18% fewer recognition errors.",
        "date": "May 1986",
        "authors": [
            "Lalit R. Bahl",
            "Peter F. Brown",
            "Peter V. de Souza",
            "Robert Mercer"
        ],
        "references": [
            224377724,
            224738936,
            220049264
        ]
    },
    {
        "id": 224327435,
        "title": "Learning in Sequential Pattern Recognition",
        "abstract": "In this article, we studied the objective functions of MMI, MCE, and MPE/MWE for discriminative learning in sequential pattern recognition. We presented an approach that unifies the objective functions of MMI, MCE, and MPE/MWE in a common rational-function form of (25). The exact structure of the rational-function form for each discriminative criterion was derived and studied. While the rational-function form of MMI has been known in the past, we provided the theoretical proof that the similar rational-function form exists for the objective functions of MCE and MPE/MWE. Moreover, we showed that the rational function forms for objective functions of MMI, MCE, and MPE/MWE differ in the constant weighting factors CDT (s1 . . . sR) and these weighting factors depend only on the labeled sequence s1 . . . sR, and are independent of the parameter set - to be optimized. The derived rational-function form for MMI, MCE, and MPE/MWE allows the GT/EBW-based parameter optimization framework to be applied directly in discriminative learning. In the past, lack of the appropriate rational-function form was a difficulty for MCE and MPE/MWE, because without this form, the GT/EBW-based parameter optimization framework cannot be directly applied. Based on the unified rational-function form, in a tutorial style, we derived the GT/EBW-based parameter optimization formulas for both discrete HMMs and CDHMMs in discriminative learning using MMI, MCE, and MPE/MWE criteria. The unifying review provided in this article has been based upon a large number of earlier contributions that have been cited and discussed throughout the article. Here we provide a brief summary of such background work. Extension to large-scale speech recognition tasks was accomplished in the work of [59] and [60]. The dissertation of [47] further improved the MMI criterion to that of MPE/MWE. In a parallel vein, the work of [20] provided an alternative approach to that of [41], with an attempt to more rigorously provide - - a CDHMM model re-estimation formula that gives positive growth of the MMI objective function. A crucial error of this attempt was corrected in [2] for establishing an existence proof of such positive growth. The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discriminative learning paradigm.",
        "date": "October 2008",
        "authors": [
            "Xiaodong He",
            "li Deng",
            "Wu Chou"
        ],
        "references": [
            345149044,
            309698218,
            281114166,
            279827221,
            279365765,
            269337645,
            267776060,
            247618866,
            245938900,
            244418125
        ]
    },
    {
        "id": 221620570,
        "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.",
        "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1",
        "date": "January 2010",
        "authors": [
            "George E. Dahl",
            "Marc'Aurelio Ranzato",
            "Abdel-rahman Mohamed",
            "Geoffrey E. Hinton"
        ],
        "references": [
            228871422,
            239666609,
            238727850,
            229091480,
            224640903,
            224371235,
            224096916,
            223927652,
            221620477,
            221489416
        ]
    },
    {
        "id": 221618539,
        "title": "Optimal Brain Damage",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "Yann Lecun",
            "John Denker",
            "Sara A Solla"
        ],
        "references": [
            265896254,
            221619244,
            288373415,
            262360039,
            243772923,
            243765702,
            238246057,
            233784971,
            230675486,
            221618281
        ]
    },
    {
        "id": 11294744,
        "title": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent",
        "abstract": "We propose a generic method for iteratively approximating various second-order gradient steps - Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient - in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.",
        "date": "August 2002",
        "authors": [
            "Nicol N. Schraudolph"
        ],
        "references": [
            258286832,
            239665351,
            234247633,
            221618988,
            313758754,
            312230773,
            256651264,
            235409844,
            221678287,
            221619224
        ]
    },
    {
        "id": 329743813,
        "title": "MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Ariel Gordon",
            "Elad Eban",
            "Ofir Nachum",
            "Bo Chen"
        ],
        "references": [
            322057654,
            319235652,
            318337409,
            317300069,
            316184205,
            310610589,
            306187717,
            284219622,
            281144459,
            265896254
        ]
    },
    {
        "id": 322518045,
        "title": "Not All Ops Are Created Equal!",
        "abstract": "Efficient and compact neural network models are essential for enabling the deployment on mobile and embedded devices. In this work, we point out that typical design metrics for gauging the efficiency of neural network architectures -- total number of operations and parameters -- are not sufficient. These metrics may not accurately correlate with the actual deployment metrics such as energy and memory footprint. We show that throughput and energy varies by up to 5X across different neural network operation types on an off-the-shelf Arm Cortex-M7 microcontroller. Furthermore, we show that the memory required for activation data also need to be considered, apart from the model parameters, for network architecture exploration studies.",
        "date": "January 2018",
        "authors": [
            "Liangzhen Lai",
            "Naveen Suda",
            "Vikas Chandra"
        ],
        "references": [
            319770123,
            316184205,
            309738510,
            301878495,
            321665311,
            321180613,
            318205093,
            309738632,
            308277088,
            267960550
        ]
    },
    {
        "id": 320964885,
        "title": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Jonathan Huang",
            "Vivek Rathod",
            "Chen Sun",
            "Menglong Zhu"
        ],
        "references": [
            312759848,
            310769756,
            308320592,
            307302911,
            305196650,
            289917319,
            286513835,
            269116491,
            266225209,
            265295439
        ]
    },
    {
        "id": 319769991,
        "title": "Trust Region Policy Optimization",
        "abstract": "We describe a iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
        "date": "February 2015",
        "authors": [
            "John Schulman",
            "Sergey Levine",
            "Philipp Moritz",
            "Michael Jordan"
        ],
        "references": []
    },
    {
        "id": 317399572,
        "title": "Inductive Representation Learning on Large Graphs",
        "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
        "date": "June 2017",
        "authors": [
            "William L. Hamilton",
            "Rex Ying",
            "Jure Leskovec"
        ],
        "references": [
            318487927,
            313628491,
            311411614,
            345728985,
            312927331,
            310824980,
            308277201,
            307991731,
            306093715,
            305997704
        ]
    },
    {
        "id": 277959210,
        "title": "Deeply Learning the Messages in Message Passing Inference",
        "abstract": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.",
        "date": "June 2015",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "Ian Reid",
            "Anton van den Hengel"
        ],
        "references": [
            319769910,
            273388101,
            319770268,
            319770168,
            311610893,
            308849673,
            308820111,
            301921832,
            276923091,
            273157570
        ]
    },
    {
        "id": 322383418,
        "title": "Adaptive Graph Convolutional Neural Networks",
        "abstract": "Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current filters in graph CNNs are built for fixed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efficiently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.",
        "date": "January 2018",
        "authors": [
            "Ruoyu Li",
            "Sheng Wang",
            "Feiyun Zhu",
            "Junzhou Huang"
        ],
        "references": [
            320828461,
            314182452,
            313481320,
            311411614,
            309854696,
            319770469,
            315881954,
            308277448,
            307991731,
            304641430
        ]
    },
    {
        "id": 339561958,
        "title": "Enforcing Geometric Constraints of Virtual Normal for Depth Prediction",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Wei Yin",
            "Yifan Liu",
            "Chunhua Shen",
            "Youliang Yan"
        ],
        "references": [
            349160653,
            339561869,
            331591265,
            339555412,
            338506505,
            338506305,
            335144529,
            333691188,
            333366615,
            330813545
        ]
    },
    {
        "id": 335056781,
        "title": "CornerNet: Detecting Objects as Paired Keypoints",
        "abstract": "We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.",
        "date": "March 2020",
        "authors": [
            "Hei Law",
            "Jia Deng"
        ],
        "references": [
            329747829,
            329745476,
            329740973,
            329740280,
            329488196,
            328142654,
            326566753,
            322060561,
            322059652,
            322059369
        ]
    },
    {
        "id": 338509384,
        "title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Sun ke",
            "Bin Xiao",
            "Dong Liu",
            "Jingdong Wang"
        ],
        "references": [
            329747444,
            329743885,
            326579996,
            326202450,
            325789041,
            324584089,
            324055335,
            321241826,
            321161014,
            320967386
        ]
    },
    {
        "id": 338506619,
        "title": "Feature Selective Anchor-Free Module for Single-Shot Object Detection",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Chenchen Zhu",
            "Yihui He",
            "Marios Savvides"
        ],
        "references": [
            335239949,
            335056781,
            324744842,
            321241658,
            321180719,
            320797032,
            319035683,
            317101239,
            312759848,
            308278279
        ]
    },
    {
        "id": 338506505,
        "title": "Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Zhi Tian",
            "He Tong",
            "Chunhua Shen",
            "Youliang Yan"
        ],
        "references": [
            329743873,
            319770123,
            308278291,
            308262491,
            303448993,
            302305068,
            301880609,
            283471087,
            278413297,
            264975444
        ]
    },
    {
        "id": 338506305,
        "title": "Structured Knowledge Distillation for Semantic Segmentation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Yifan Liu",
            "Ke Chen",
            "Chris Liu",
            "Zengchang Qin"
        ],
        "references": [
            329442528,
            323867606,
            319770123,
            318810091,
            311066880,
            310953387,
            309935608,
            308324937,
            305196650,
            303840621
        ]
    },
    {
        "id": 324387691,
        "title": "YOLOv3: An Incremental Improvement",
        "abstract": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/",
        "date": "April 2018",
        "authors": [
            "Joseph Redmon",
            "Ali Farhadi"
        ],
        "references": [
            311769895,
            308278279,
            220659463,
            326566753,
            320967941,
            320964885,
            320964510,
            319769911,
            311609041,
            308863239
        ]
    },
    {
        "id": 322059369,
        "title": "Focal Loss for Dense Object Detection",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Tsung-Yi Lin",
            "Priya Goyal",
            "Ross Girshick",
            "Kaiming He"
        ],
        "references": [
            312759848,
            311769895,
            308278279,
            286513835,
            262270555,
            259441043,
            259212328,
            221259850,
            220659463,
            3940582
        ]
    },
    {
        "id": 320968449,
        "title": "EAST: An Efficient and Accurate Scene Text Detector",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Xinyu Zhou",
            "Cong Yao",
            "He Wen",
            "Yuzhi Wang"
        ],
        "references": [
            308834858,
            308277110,
            307302911,
            305857552,
            304590104,
            303488577,
            301879841,
            301879695,
            301856311,
            286945604
        ]
    },
    {
        "id": 320968452,
        "title": "Learning Non-maximum Suppression",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Jan Hosang",
            "Rodrigo Benenson",
            "Bernt Schiele"
        ],
        "references": [
            308278279,
            286513835,
            283356705,
            282844271,
            261116324,
            259441043,
            253329335,
            233529294,
            224181528,
            220660094
        ]
    },
    {
        "id": 319770184,
        "title": "Speech Recognition With Deep Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $backslash$emphdeep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
        "date": "January 2013",
        "authors": [
            "Alex Graves",
            "Abdel-rahman Mohamed",
            "Geoffrey Hinton"
        ],
        "references": []
    },
    {
        "id": 309192180,
        "title": "Amortised MAP Inference for Image Super-resolution",
        "abstract": "Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR.",
        "date": "October 2016",
        "authors": [
            "Casper S\u00f8nderby",
            "Jose Caballero",
            "Lucas Theis",
            "Wenzhe Shi"
        ],
        "references": [
            313910272,
            308262491,
            308152499,
            320968363,
            319770355,
            319770229,
            319770144,
            311610454,
            309009016,
            307925703
        ]
    },
    {
        "id": 320920335,
        "title": "Non-Autoregressive Neural Machine Translation",
        "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
        "date": "November 2017",
        "authors": [
            "Jiatao Gu",
            "James Bradbury",
            "Caiming Xiong",
            "Victor O. K. Li"
        ],
        "references": [
            265252627,
            230875890,
            2588204,
            319770465,
            318741705,
            312417327,
            302879092,
            281145060,
            44629251,
            2426555
        ]
    },
    {
        "id": 312550770,
        "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications",
        "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.",
        "date": "January 2017",
        "authors": [
            "Tim Salimans",
            "Andrej Karpathy",
            "Xi Chen",
            "Diederik P. Kingma"
        ],
        "references": [
            308026508,
            267627490,
            230616821,
            221362297,
            319770416,
            319770229,
            319770134,
            309573096,
            305193694,
            304018350
        ]
    },
    {
        "id": 308278061,
        "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
        "date": "October 2016",
        "authors": [
            "Justin Johnson",
            "Alexandre Alahi",
            "Li Fei-Fei"
        ],
        "references": [
            313910272,
            307560165,
            304409176,
            301842292,
            281312423,
            279068412,
            278693824,
            275523282,
            270454670,
            269280482
        ]
    },
    {
        "id": 319770331,
        "title": "ParseNet: Looking Wider to See Better",
        "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .",
        "date": "June 2016",
        "authors": [
            "Wei Liu",
            "Andrew Rabinovich",
            "Alexander C Berg"
        ],
        "references": []
    },
    {
        "id": 311610461,
        "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Di Lin",
            "Jifeng Dai",
            "Jiaya Jia",
            "Kaiming He"
        ],
        "references": [
            281670742,
            265295439,
            264975444,
            256663526,
            226923261,
            225496431,
            220659463,
            220184077,
            220183779,
            319770420
        ]
    },
    {
        "id": 311610151,
        "title": "Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Shu Liu",
            "Xiaojuan Qi",
            "Jianping Shi",
            "Hong Zhang"
        ],
        "references": [
            303912984,
            281670744,
            281670742,
            275670269,
            273158406,
            272422948,
            269116202,
            268689640,
            264979485,
            263736545
        ]
    },
    {
        "id": 305638304,
        "title": "Is Faster R-CNN Doing Well for Pedestrian Detection?",
        "abstract": "Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN [1, 2] have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN [2] for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but effective baseline for pedestrian detection, using an RPN followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting competitive accuracy and good speed. Code will be made publicly available.",
        "date": "July 2016",
        "authors": [
            "Liliang Zhang",
            "Liang Lin",
            "Xiaodan Liang",
            "Kaiming He"
        ],
        "references": [
            308806299,
            319770820,
            319770430,
            319770291,
            319770168,
            319770161,
            311610683,
            311609097,
            308858813,
            308835152
        ]
    },
    {
        "id": 311611062,
        "title": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Sean Bell",
            "C. Lawrence Zitnick",
            "Kavita Bala",
            "Ross Girshick"
        ],
        "references": [
            319770352,
            301879841,
            278413297,
            275897265,
            273158406,
            272521784,
            268689640,
            265385879,
            264979485,
            262270555
        ]
    },
    {
        "id": 311609522,
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Joseph Redmon",
            "Santosh Divvala",
            "Ross Girshick",
            "Ali Farhadi"
        ],
        "references": [
            280608596,
            275836193,
            265967416,
            265295439,
            262270555,
            259441043,
            259212328,
            228102719,
            220660094,
            41781863
        ]
    },
    {
        "id": 308808582,
        "title": "Hypercolumns for object segmentation and fine-grained localization",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Bharath Hariharan",
            "Pablo Arbelaez",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "references": [
            263048918,
            262974624,
            261493845,
            260350533,
            240308781,
            221305034,
            221304497,
            220320803,
            319770362,
            319770183
        ]
    },
    {
        "id": 267958868,
        "title": "Sparselet Models for Efficient Multiclass Object Detection",
        "abstract": "We develop an intermediate representation for deformable part models and show that this representation has favorable performance characteristics for multi-class problems when the number of classes is high. Our model uses sparse coding of part filters to represent each filter as a sparse linear combination of shared dictionary elements. This leads to a universal set of parts that are shared among all object classes. Re-construction of the original part filter responses via sparse matrix-vector product reduces computation relative to conventional part filter convo-lutions. Our model is well suited to a parallel implementation, and we report a new GPU DPM implementation that takes advantage of sparse coding of part filters. The speed-up offered by our intermediate repre-sentation and parallel computation enable real-time DPM detection of 20 different object classes on a laptop computer.",
        "date": "January 2012",
        "authors": [
            "Hyun Oh Song",
            "Stefan Zickler",
            "Tim Althoff",
            "Ross Girshick"
        ],
        "references": [
            277292831,
            225444067,
            224254937,
            319770820,
            319770349,
            281327886,
            248516115,
            224254751,
            221618910,
            221617868
        ]
    },
    {
        "id": 244706524,
        "title": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine",
        "abstract": "Many object detection systems are constrained by the time required to convolve a target image with a bank of fil-ters that code for different aspects of an object's appear-ance, such as the presence of component parts. We ex-ploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter re-sponses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approx-imately 20,000 times\u2014 four orders of magnitude\u2014 when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and col-lecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.",
        "date": "June 2013",
        "authors": [
            "Thomas Dean",
            "Mark Ruzon",
            "Mark Segal",
            "Jonathon Shlens"
        ],
        "references": [
            277292831,
            267958868,
            244437609,
            224254937,
            221366761,
            221110937,
            220660094,
            220659463,
            215721846,
            319770829
        ]
    },
    {
        "id": 338503788,
        "title": "Libra R-CNN: Towards Balanced Learning for Object Detection",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Jiangmiao Pang",
            "Kai Chen",
            "Jianping Shi",
            "Huajun Feng"
        ],
        "references": [
            338513481,
            335056781,
            338512599,
            333359371,
            329748363,
            329747447,
            329745798,
            329745476,
            329740280,
            326566753
        ]
    },
    {
        "id": 334773154,
        "title": "ScratchDet: Training Single-Shot Object Detectors From Scratch",
        "abstract": "Current state-of-the-art object objectors are fine-tuned from the off-the-shelf networks pretrained on large-scale classification dataset ImageNet, which incurs some additional problems: 1) The classification and detection have different degrees of sensitivity to translation, resulting in the learning objective bias; 2) The architecture is limited by the classification network, leading to the inconvenience of modification. To cope with these problems, training detectors from scratch is a feasible solution. However, the detectors trained from scratch generally perform worse than the pre-trained ones, even suffer from the convergence issue in training. In this paper, we explore to train object detectors from scratch robustly. By analysing the previous work on optimization landscape, we find that one of the overlooked points in current trained-from-scratch detector is the BatchNorm. Resorting to the stable and predictable gradient brought by BatchNorm, detectors can be trained from scratch stably while keeping the favourable performance independent to the network architecture. Taking this advantage, we are able to explore various types of networks for object detection, without suffering from the poor convergence. By extensive experiments and analyses on downsampling factor, we propose the Root-ResNet backbone network, which makes full use of the information from original images. Our ScratchDet achieves the state-of-the-art accuracy on PASCAL VOC 2007, 2012 and MS COCO among all the train-from-scratch detectors and even performs better than several one-stage pretrained methods. Codes will be made publicly available at https: //github.com/KimSoybean/ScratchDet.",
        "date": "July 2019",
        "authors": [
            "Rui Zhu",
            "Shifeng Zhang",
            "Xiaobo Wang",
            "Longyin Wen"
        ],
        "references": [
            331209117,
            321511551,
            321487807,
            321369549,
            333359371,
            329488196,
            324584281,
            323867513,
            322059652,
            322059369
        ]
    },
    {
        "id": 329743392,
        "title": "Single-Shot Object Detection with Enriched Semantics",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Zhishuai Zhang",
            "Siyuan Qiao",
            "Cihang Xie",
            "Wei Shen"
        ],
        "references": [
            319770123,
            329740202,
            329488196,
            322060257,
            320964522,
            319770430,
            319770291,
            319770284,
            319770183,
            319769911
        ]
    },
    {
        "id": 324166929,
        "title": "Multi-scale Location-aware Kernel Representation for Object Detection",
        "abstract": "Although Faster R-CNN and its variants have shown promising performance in object detection, they only exploit simple first-order representation of object proposals for final classification and regression. Recent classification methods demonstrate that the integration of high-order statistics into deep convolutional neural networks can achieve impressive improvement, but their goal is to model whole images by discarding location information so that they cannot be directly adopted to object detection. In this paper, we make an attempt to exploit high-order statistics in object detection, aiming at generating more discriminative representations for proposals to enhance the performance of detectors. To this end, we propose a novel Multi-scale Location-aware Kernel Representation (MLKP) to capture high-order statistics of deep features in proposals. Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation.Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at: https://github.com/Hwang64/MLKP.",
        "date": "April 2018",
        "authors": [
            "Hao Wang",
            "Qilong Wang",
            "Mingqi Gao",
            "Peihua Li"
        ],
        "references": [
            315570363,
            322058926,
            322058505,
            320967941,
            320966973,
            320964841,
            320964522,
            320964510,
            319770284,
            319769911
        ]
    },
    {
        "id": 323867543,
        "title": "Learning Region Features for Object Detection",
        "abstract": "While most steps in the modern object detection methods are learnable, the region feature extraction step remains largely hand-crafted, featured by RoI pooling methods. This work proposes a general viewpoint that unifies existing region feature extraction methods and a novel method that is end-to-end learnable. The proposed method removes most heuristic choices and outperforms its RoI pooling counterparts. It moves further towards fully learnable object detection.",
        "date": "March 2018",
        "authors": [
            "Jiayuan Gu",
            "Han Hu",
            "Liwei Wang",
            "Yichen Wei"
        ],
        "references": [
            321417928,
            315463609,
            286513835,
            326566753,
            321794741,
            321180477,
            320968452,
            311573567,
            303409473,
            287250476
        ]
    },
    {
        "id": 321241658,
        "title": "An Analysis of Scale Invariance in Object Detection - SNIP",
        "abstract": "An analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented. Scale specific and scale invariant design of detectors are compared by training them with different configurations of input data. To examine if upsampling images is necessary for detecting small objects, we evaluate the performance of different network architectures for classifying small objects on ImageNet. Based on this analysis, we propose a deep end-to-end trainable Image Pyramid Network for object detection which operates on the same image scales during training and inference. Since small and large objects are difficult to recognize at smaller and larger scales respectively, we present a novel training scheme called Scale Normalization for Image Pyramids (SNIP) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. On the COCO dataset, our single model performance is 45.7% and an ensemble of 3 networks obtains an mAP of 48.3%. We use ImageNet-1000 pre-trained models and only train with bounding box supervision. Our submission won the Best Student Entry in the COCO 2017 challenge. Code will be made available at http://bit.ly/2yXVg4c.",
        "date": "November 2017",
        "authors": [
            "Bharat Singh",
            "Larry S. Davis"
        ],
        "references": [
            311769895,
            326566753,
            322060558,
            322060396,
            322059602,
            322058119,
            319770183,
            319769911,
            319501512,
            316240097
        ]
    },
    {
        "id": 321180719,
        "title": "Single-Shot Refinement Neural Network for Object Detection",
        "abstract": "For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet.",
        "date": "June 2018",
        "authors": [
            "Shifeng Zhang",
            "Longyin Wen",
            "Xiao Bian",
            "Zhen Lei"
        ],
        "references": [
            321369549,
            320495167,
            319164062,
            326566753,
            322060558,
            320967941,
            320964522,
            319770420,
            319770331,
            319770284
        ]
    },
    {
        "id": 320797032,
        "title": "Improving Object Localization with Fitness NMS and Bounded IoU Loss",
        "abstract": "We demonstrate that modern detection methods are designed to identify only a sufficently accurate bounding box, rather than the best available one. To address this issue we propose a simple and fast modification to the existing methods called Fitness NMS. This method is tested with the DeNet model and obtains a significantly improved MAP at greater localization accuracies without a loss in evaluation rate. We also derive a novel bounding box regression loss based on a set of IoU upper bounds and investigate simple RoI clustering schemes for improving evaluation rates for the DeNet wide model variants. Following these novelties we provide an analysis of input image dimensions with the DeNet model. We obtain a MAP[0.5:0.95] of 33.6%@79Hz and 41.8%@5Hz for MSCOCO and a Titan X (Maxwell). These results provide further evidence that DeNet and Fitness NMS methods can be scaled to provide highly competitive performance at a broad range of evaluation rates without requiring the manually specified bounding box anchors used in competing methods.",
        "date": "October 2017",
        "authors": [
            "Lachlan Tychsen-Smith",
            "Lars Petersson"
        ],
        "references": [
            316184748,
            315711972,
            326566753,
            322060558,
            322059369,
            322058378,
            322058119,
            319770430,
            319769911,
            316779992
        ]
    },
    {
        "id": 347508984,
        "title": "Contextual Priming for Object Detection",
        "abstract": "There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",
        "date": "July 2003",
        "authors": [
            "Antonio Torralba"
        ],
        "references": [
            243776886,
            236170014,
            232635556,
            232060860,
            227191648,
            225680308,
            224377747,
            221364881,
            22010827,
            19588504
        ]
    },
    {
        "id": 329741355,
        "title": "Dynamic-Structured Semantic Propagation Network",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Xiaodan Liang",
            "Eric Xing",
            "Hongfei Zhou"
        ],
        "references": [
            322749812,
            317558562,
            315666869,
            314453083,
            313481320,
            311222763,
            310610877,
            306357649,
            306094026,
            302305068
        ]
    },
    {
        "id": 329740332,
        "title": "Non-local Neural Networks",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Xiaolong Wang",
            "Ross Girshick",
            "Abhinav Gupta",
            "Kaiming He"
        ],
        "references": [
            320968277,
            320195152,
            319121504,
            317716400,
            317356778,
            317356629,
            317040382,
            316821219,
            311737185,
            311299547
        ]
    },
    {
        "id": 328128947,
        "title": "Multi-scale Context Intertwining for Semantic Segmentation: 15th European Conference, Munich, Germany, September 8\u201314, 2018, Proceedings, Part III",
        "abstract": "Accurate semantic image segmentation requires the joint consideration of local appearance, semantic information, and global scene context. In today\u2019s age of pre-trained deep networks and their powerful convolutional features, state-of-the-art semantic segmentation approaches differ mostly in how they choose to combine together these different kinds of information. In this work, we propose a novel scheme for aggregating features from different scales, which we refer to as Multi-Scale Context Intertwining (MSCI). In contrast to previous approaches, which typically propagate information between scales in a one-directional manner, we merge pairs of feature maps in a bidirectional and recurrent fashion, via connections between two LSTM chains. By training the parameters of the LSTM units on the segmentation task, the above approach learns how to extract powerful and effective features for pixel-level semantic segmentation, which are then combined hierarchically. Furthermore, rather than using fixed information propagation routes, we subdivide images into super-pixels, and use the spatial relationship between them in order to perform image-adapted context aggregation. Our extensive evaluation on public benchmarks indicates that all of the aforementioned components of our approach increase the effectiveness of information propagation throughout the network, and significantly improve its eventual segmentation accuracy.",
        "date": "September 2018",
        "authors": [
            "Di Lin",
            "Ji Yuanfeng",
            "Dani Lischinski",
            "Daniel Cohen-Or"
        ],
        "references": [
            314115448,
            310610877,
            309288645,
            308311897,
            304409176,
            301880609,
            301877264,
            301876223,
            281670742,
            347509008
        ]
    },
    {
        "id": 324078024,
        "title": "Adaptive Affinity Field for Semantic Segmentation",
        "abstract": "Existing semantic segmentation methods mostly rely on per-pixel supervision, unable to capture structural regularity present in natural images. Instead of learning to enforce semantic labels on individual pixels, we propose to enforce affinity field patterns in individual pixel neighbourhoods, i.e., the semantic label patterns of whether neighbouring pixels are in the same segment should match between the prediction and the ground-truth. The affinity fields characterize geometric relationships within the image, such as \"motorcycles have round wheels\". We further develop a novel method for learning the optimal neighbourhood size for each semantic category, with an adversarial loss that optimizes over worst-case scenarios. Unlike the common Conditional Random Field (CRF) approaches, our adaptive affinity field (AAF) method has no extra parameters during inference, and is less sensitive to appearance changes in the image. Extensive evaluations on Cityscapes, PASCAL VOC 2012 and GTA5 datasets demonstrate the effectiveness and robustness of AAF in semantic segmentation.",
        "date": "March 2018",
        "authors": [
            "Tsung-Wei Ke",
            "Jyh-Jing Hwang",
            "Ziwei Liu",
            "Stella X. Yu"
        ],
        "references": [
            320195152,
            310953387,
            305984144,
            302305068,
            301880609,
            301877264,
            301836893,
            281670742,
            263811780,
            220659463
        ]
    },
    {
        "id": 324005873,
        "title": "Context Encoding for Semantic Segmentation",
        "abstract": "Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.",
        "date": "March 2018",
        "authors": [
            "Hang Zhang",
            "Kristin J. Dana",
            "Jianping Shi",
            "Zhongyue Zhang"
        ],
        "references": [
            322749812,
            320967995,
            315489372,
            311222763,
            308278291,
            303448993,
            302305068,
            264975444,
            224164326,
            221361415
        ]
    },
    {
        "id": 323846355,
        "title": "Dynamic-structured Semantic Propagation Network",
        "abstract": "Semantic concept hierarchy is still under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into dense prediction. This lack of modeling semantic correlations also makes prior works must tune highly-specified models for each task due to the label discrepancy across datasets. It severely limits the generalization capability of segmentation models for open set concept vocabulary and annotation utilization. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph by explicitly incorporating the semantic concept hierarchy into network construction. Each neuron represents the instantiated module for recognizing a specific type of entity such as a super-class (e.g. food) or a specific concept (e.g. pizza). During training, DSSPN performs the dynamic-structured neuron computation graph by only activating a sub-graph of neurons for each image in a principled way. A dense semantic-enhanced neural block is proposed to propagate the learned knowledge of all ancestor neurons into each fine-grained child neuron for feature evolving. Another merit of such semantic explainable structure is the ability of learning a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of our DSSPN over state-of-the-art segmentation models. Moreoever, we demonstrate a universal segmentation model that is jointly trained on diverse datasets can surpass the performance of the common fine-tuning scheme for exploiting multiple domain knowledge.",
        "date": "March 2018",
        "authors": [
            "Xiaodan Liang",
            "Hongfei Zhou",
            "Eric Xing"
        ],
        "references": [
            322749812,
            319770123,
            317558562,
            314453083,
            310610877,
            221344862,
            220659463,
            215616967,
            328955291,
            322060396
        ]
    },
    {
        "id": 320967995,
        "title": "Deep TEN: Texture Encoding Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Hang Zhang",
            "Jia Xue",
            "Kristin J. Dana"
        ],
        "references": [
            324400684,
            320967167,
            319770414,
            319770411,
            319770291,
            319770183,
            319770112,
            319769911,
            317297400,
            316240097
        ]
    },
    {
        "id": 329748363,
        "title": "Path Aggregation Network for Instance Segmentation",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Shu Liu",
            "Lu Qi",
            "Haifang Qin",
            "Jianping Shi"
        ],
        "references": [
            319770123,
            315881952,
            315463609,
            312759848,
            311769895,
            311223153,
            310953291,
            310953253,
            310769756,
            308277256
        ]
    },
    {
        "id": 329745796,
        "title": "MegDet: A Large Mini-Batch Object Detector",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Chao Peng",
            "Tete Xiao",
            "Zeming Li",
            "Yuning Jiang"
        ],
        "references": [
            317101160,
            316779920,
            315463609,
            311223153,
            302305068,
            286513835,
            266560893,
            262270555,
            259441043,
            221361415
        ]
    },
    {
        "id": 322060368,
        "title": "Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Xun Huang",
            "Serge Belongie"
        ],
        "references": [
            320971402,
            320968590,
            320967151,
            320966887,
            318830253,
            315489372,
            312170783,
            311612361,
            311459088,
            310769491
        ]
    },
    {
        "id": 322060088,
        "title": "Deep Dual Learning for Semantic Image Segmentation",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Ping Luo",
            "Guangrun Wang",
            "Liang Lin",
            "Xiaogang Wang"
        ],
        "references": [
            322749812,
            320966887,
            310610633,
            301877264,
            284579108,
            283040455,
            281670742,
            269116202,
            265908778,
            265295439
        ]
    },
    {
        "id": 320971174,
        "title": "Learning Object Interactions and Descriptions for Semantic Image Segmentation",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Guangrun Wang",
            "Ping Luo",
            "Liang Lin",
            "Xiaogang Wang"
        ],
        "references": [
            308806299,
            301877264,
            283040455,
            281670742,
            269116202,
            269041123,
            265603266,
            265295439,
            263237688,
            257882504
        ]
    },
    {
        "id": 325529707,
        "title": "SSNet: Scale Selection Network for Online 3D Action Prediction",
        "abstract": "In action prediction (early action recognition), the goal is to predict the class label of an ongoing action using its observed part so far. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the time axis. As there are significant temporal scale variations of the observed part of the ongoing action at different progress levels, we propose a novel window scale selection scheme to make our network focus on the performed part of the ongoing action and try to suppress the noise from the previous actions at each time step. Furthermore, an activation sharing scheme is proposed to deal with the overlapping computations among the adjacent steps, which allows our model to run more efficiently. The extensive experiments on two challenging datasets show the effectiveness of the proposed action prediction framework.",
        "date": "June 2018",
        "authors": [
            "Jun Liu",
            "Amir Shahroudy",
            "Gang Wang",
            "Ling-Yu Duan"
        ],
        "references": [
            329740252,
            323003597,
            322059078,
            321758880,
            322329195,
            322060616,
            322059593,
            322058130,
            321382985,
            320971075
        ]
    },
    {
        "id": 323433964,
        "title": "Deep Coupled ResNet for Low-Resolution Face Recognition",
        "abstract": "Face images captured by surveillance cameras are often of low resolution (LR), which adversely affects the performance of their matching with high-resolution (HR) gallery images. Existing methods including super resolution, coupled mappings, multidimensional scaling, and CNN yield only modest performance. In this paper, we propose the Deep Coupled ResNet (DCR) model. It consists of one trunk network and two branch networks. The trunk network, trained by face images of 3 significantly different resolutions, is used to extract discriminative features robust to the resolution change. Two branch networks, trained by HR images and images of the targeted LR, work as resolution-specific coupled mappings to transform HR and corresponding LR features to a space where their difference is minimized. Model parameters of branch networks are optimized using our proposed Coupled-Mapping (CM) loss function, which considers not only the discriminability of HR and LR features, but also the similarity between them. In order to deal with various possible resolutions of probe images, we train multiple pairs of small branch networks while using the same trunk network. Thorough evaluation on LFW and SCface databases shows that the proposed DCR model achieves consistently and considerably better performance than the state-of-the-arts.",
        "date": "February 2018",
        "authors": [
            "Ze Lu",
            "Xudong Jiang",
            "Alex ChiChung Kot"
        ],
        "references": [
            329740252,
            310953343,
            306927729,
            291229731,
            281487308,
            268988012,
            264979485,
            264786906,
            236124723,
            224695628
        ]
    },
    {
        "id": 322058130,
        "title": "An Empirical Study of Language CNN for Image Captioning",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Jiuxiang Gu",
            "Gang Wang",
            "Jianfei Cai",
            "Tsuhan Chen"
        ],
        "references": [
            319770160,
            317716159,
            311411280,
            308809117,
            308716468,
            308034527,
            307953213,
            307747289,
            305638191,
            305037719
        ]
    },
    {
        "id": 315178093,
        "title": "Holistically-Nested Edge Detection",
        "abstract": "We develop a new edge detection algorithm that addresses two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSDS500 dataset (ODS F-score of 0.790) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 s per image) that is orders of magnitude faster than some CNN-based edge detection algorithms developed before HED. We also observe encouraging results on other boundary detection benchmark datasets such as Multicue and PASCAL-Context.",
        "date": "December 2017",
        "authors": [
            "Saining Xie",
            "Z. W. Tu"
        ],
        "references": [
            347509008,
            319770430,
            319770291,
            319770168,
            318439989,
            314100361,
            313137413,
            311611359,
            311610691,
            311609706
        ]
    },
    {
        "id": 318439989,
        "title": "Pushing the Boundaries of Boundary Detection using Deep Learning",
        "abstract": "In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-ofthe-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.",
        "date": "May 2016",
        "authors": [
            "I Kokkinos"
        ],
        "references": []
    },
    {
        "id": 314258679,
        "title": "Layered Interpretation of Street View Images",
        "abstract": "",
        "date": "July 2015",
        "authors": [
            "Ming-Yu Liu",
            "Shuoxin Lin",
            "Srikumar Ramalingam",
            "Oncel Tuzel"
        ],
        "references": [
            264979485,
            260350533,
            256663526,
            319770430,
            308820676,
            300358285,
            287515053,
            262162932,
            258374356,
            257249132
        ]
    },
    {
        "id": 311610325,
        "title": "Semantic Segmentation with Boundary Neural Fields",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Gedas Bertasius",
            "Jianbo Shi",
            "Lorenzo Torresani"
        ],
        "references": [
            308820325,
            272194536,
            269116202,
            268689640,
            264160686,
            260350533,
            256663526,
            240308781,
            221363985,
            45821321
        ]
    },
    {
        "id": 311609706,
        "title": "Weakly Supervised Object Boundaries",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Anna Khoreva",
            "Rodrigo Benenson",
            "Mohamed Omran",
            "Matthias Hein"
        ],
        "references": [
            275527361,
            273158406,
            272521784,
            272422948,
            262270555,
            261121672,
            224377985,
            221110035,
            220184077,
            45821321
        ]
    },
    {
        "id": 311344466,
        "title": "Learning spatiotemporal features with 3d convolutional networks",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Du Tran",
            "L. Bourdev",
            "Rob Fergus",
            "L. Torresani"
        ],
        "references": []
    },
    {
        "id": 307664547,
        "title": "SUN RGB-D: A RGB-D scene understanding benchmark suite",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Shuran Song",
            "Samuel P. Lichtenberg",
            "Jianxiong Xiao"
        ],
        "references": [
            283659100,
            279839496,
            273394734,
            271551255,
            264975661,
            264160686,
            262414476,
            261392461,
            261353760,
            261353409
        ]
    },
    {
        "id": 335467236,
        "title": "Learning Compositional Neural Information Fusion for Human Parsing",
        "abstract": "This work proposes to combine neural networks with the compositional hierarchy of human bodies for efficient and complete human parsing. We formulate the approach as a neural information fusion framework. Our model assembles the information from three inference processes over the hierarchy: direct inference (directly predicting each part of a human body using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). The bottom-up and top-down inferences explicitly model the compositional and decompositional relations in human bodies, respectively. In addition, the fusion of multi-source information is conditioned on the inputs, i.e., by estimating and considering the confidence of the sources. The whole model is end-to-end differentiable, explicitly model-ing information flows and structures. Our approach is extensively evaluated on four popular datasets, outperform-ing the state-of-the-arts in all cases, with a fast processing speed of 23fps. Our code and results have been released to help ease future research in this direction.",
        "date": "October 2019",
        "authors": [
            "Wenguan Wang",
            "Zhijie Zhang",
            "Siyuan Qi",
            "Jianbing Shen"
        ],
        "references": [
            333931943,
            332752049,
            328375157,
            335551739,
            332813720,
            332442460,
            329748404,
            329740202,
            328372282,
            328370396
        ]
    },
    {
        "id": 328308268,
        "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation",
        "abstract": "",
        "date": "September 2018",
        "authors": [
            "Zhuotun Zhu",
            "Yingda Xia",
            "Wei Shen",
            "Elliot K Fishman"
        ],
        "references": [
            319642574,
            319631503,
            319461525,
            336461589,
            323194218,
            322060625,
            321160815,
            319770291,
            319770183,
            319770168
        ]
    },
    {
        "id": 347509008,
        "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild",
        "abstract": "",
        "date": "June 2014",
        "authors": [
            "Roozbeh Mottaghi",
            "Xianjie Chen",
            "Xiaobai Liu",
            "Nam-Gyu Cho"
        ],
        "references": []
    },
    {
        "id": 321124772,
        "title": "NISP: Pruning Networks using Neuron Importance Score Propagation",
        "abstract": "To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the \"final response layer\" (FRL), which is the second-to-last layer before classification, for a pruned network to retrain its predictive power. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and then fine-tuned to retain its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.",
        "date": "November 2017",
        "authors": [
            "Ruichi Yu",
            "Ang Li",
            "Chun-Fu Chen",
            "Jui-Hsin Lai"
        ],
        "references": [
            322057654,
            321095503,
            329748370,
            329747532,
            329743625,
            324997825,
            322058064,
            321665770,
            320964672,
            319770346
        ]
    },
    {
        "id": 329743447,
        "title": "NISP: Pruning Networks Using Neuron Importance Score Propagation",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Ruichi Yu",
            "Ang Li",
            "Chun-Fu Chen",
            "Jui-Hsin Lai"
        ],
        "references": [
            322329217,
            322057654,
            321095333,
            308844880,
            307536925,
            305196650,
            304409696,
            280329902,
            275669366,
            275279789
        ]
    },
    {
        "id": 320971540,
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Saining Xie",
            "Ross Girshick",
            "Piotr Dollar",
            "Z. W. Tu"
        ],
        "references": [
            308026508,
            305196650,
            303822179,
            303409658,
            303409435,
            300412490,
            265787949,
            265295439,
            264979485,
            261368736
        ]
    },
    {
        "id": 320965171,
        "title": "Spatially Adaptive Computation Time for Residual Networks",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Michael Figurnov",
            "Maxwell D. Collins",
            "Yukun Zhu",
            "Li Zhang"
        ],
        "references": [
            320386397,
            319770123,
            311842587,
            305196650,
            305006910,
            301879329,
            301876854,
            292074166,
            284788333,
            284219037
        ]
    },
    {
        "id": 319524838,
        "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks",
        "abstract": "Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with additional side branch classifiers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected less frequently, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.",
        "date": "September 2017",
        "authors": [
            "Surat Teerapittayanon",
            "Bradley McDanel",
            "H. T. Kung"
        ],
        "references": [
            305196650,
            301879329,
            284218785,
            221653840,
            50596071,
            2985446,
            319770334,
            319770272,
            319770183,
            319770111
        ]
    },
    {
        "id": 272027131,
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
        "date": "February 2015",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "references": [
            286966779,
            269935282,
            265908778,
            265787949,
            265295439,
            264979485,
            263237688,
            259441043,
            259440750,
            259367619
        ]
    },
    {
        "id": 313086331,
        "title": "A Taxonomy And Evaluation Of Dense Two-Frame Stereo Correspondence Algorithms",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "Daniel Scharstein",
            "R. Szeliski"
        ],
        "references": []
    },
    {
        "id": 312466400,
        "title": "Early versus late fusion in semantic video analysis",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "Cees Snoek",
            "Marcel Worring",
            "A.W. Smeulders"
        ],
        "references": []
    },
    {
        "id": 267473326,
        "title": "The 2005 PASCAL Visual Object Classes Challenge",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "M Everingham"
        ],
        "references": []
    },
    {
        "id": 248516115,
        "title": "The pascal visual object classes challenge 2006 results",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "M. Everingham",
            "A. Zisserman",
            "C. Williams",
            "Luc Van Gool"
        ],
        "references": []
    },
    {
        "id": 337211284,
        "title": "Searching for a Robust Neural Architecture in Four GPU Hours",
        "abstract": "Conventional neural architecture search (NAS) approaches are based on reinforcement learning or evolutionary strategy, which take more than 3000 GPU hours to find a good model on CIFAR-10. We propose an efficient NAS approach learning to search by gradient descent. Our approach represents the search space as a directed acyclic graph (DAG). This DAG contains billions of sub-graphs, each of which indicates a kind of neural architecture. To avoid traversing all the possibilities of the sub-graphs, we develop a differentiable sampler over the DAG. This sam-pler is learnable and optimized by the validation loss after training the sampled architecture. In this way, our approach can be trained in an end-to-end fashion by gradient descent, named Gradient-based search using Differ-entiable Architecture Sampler (GDAS). In experiments, we can finish one searching procedure in four GPU hours on CIFAR-10, and the discovered model obtains a test error of 2.82% with only 2.5M parameters, which is on par with the state-of-the-art. Code is publicly available on GitHub: https://github.com/D-X-Y/NAS-Projects.",
        "date": "July 2019",
        "authors": [
            "Xuanyi Dong",
            "Yi Yang"
        ],
        "references": [
            329750201,
            325673550,
            338512082,
            329745708,
            329745117,
            329740282,
            329740172,
            323302973,
            323118469,
            322950113
        ]
    },
    {
        "id": 329740382,
        "title": "Practical Block-Wise Neural Network Architecture Generation",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Zhao Zhong",
            "Junjie Yan",
            "Wei Wu",
            "Jing Shao"
        ],
        "references": [
            319770123,
            326202443,
            320971540,
            320968382,
            319770414,
            319770291,
            319770272,
            319770183,
            317194083,
            315783546
        ]
    },
    {
        "id": 341075580,
        "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in Less Than 4 Hours",
        "abstract": "Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the latency constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 h. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves \\(74.96\\%\\) top-1 accuracy on ImageNet with 79 ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar inference latency constraints (\\(\\le \\)80 ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000\\(\\times \\) faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.",
        "date": "April 2020",
        "authors": [
            "Dimitrios Stamoulis",
            "Ruizhou Ding",
            "Di Wang",
            "Dimitrios Lymberopoulos"
        ],
        "references": [
            333336705,
            328776128,
            319856813,
            338510249,
            338510035,
            338509389,
            329745708,
            329740282,
            329740172,
            324712190
        ]
    },
    {
        "id": 339554815,
        "title": "Exploring Randomly Wired Neural Networks for Image Recognition",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Saining Xie",
            "Alexander Kirillov",
            "Ross Girshick",
            "Kaiming He"
        ],
        "references": [
            319770123,
            306187421,
            305196650,
            303409435,
            265295439,
            259441043,
            228102719,
            221618539,
            49820957,
            23974889
        ]
    },
    {
        "id": 338509389,
        "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Bichen Wu",
            "Kurt Keutzer",
            "Xiaoliang Dai",
            "Peizhao Zhang"
        ],
        "references": [
            329747614,
            328110752,
            324435700,
            320564723,
            319276974,
            316184205,
            311430427,
            305196650,
            301878495,
            221361415
        ]
    },
    {
        "id": 329745117,
        "title": "Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Tom Veniat",
            "Ludovic Denoyer"
        ],
        "references": [
            320971183,
            320322028,
            317230530,
            316184205,
            315695750,
            315666667,
            314092304,
            313096253,
            305196650,
            303859203
        ]
    },
    {
        "id": 329740282,
        "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Mark Sandler",
            "Andrew Howard",
            "Menglong Zhu",
            "Andrey Zhmoginov"
        ],
        "references": [
            320075558,
            317300069,
            316184205,
            308981007,
            308278279,
            307536925,
            306227124,
            305196650,
            301839500,
            286513835
        ]
    },
    {
        "id": 259772760,
        "title": "A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms",
        "abstract": "Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.",
        "date": "October 2013",
        "authors": [
            "Jannik Fritsch",
            "T Kuehnl",
            "Andreas Geiger"
        ],
        "references": [
            261245980,
            260707296,
            261500275,
            261225188,
            261225183,
            261127956,
            261127946,
            261124172,
            261121682,
            260543487
        ]
    },
    {
        "id": 338681644,
        "title": "Neural Architecture Search: A Survey",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Thomas Elsken",
            "Jan Hendrik Metzen",
            "Frank Hutter"
        ],
        "references": []
    },
    {
        "id": 338509164,
        "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Chenxi Liu",
            "Liang-Chieh Chen",
            "Florian Schroff",
            "Hartwig Adam"
        ],
        "references": [
            329740382,
            328123706,
            327994946,
            325673550,
            322749812,
            320974965,
            319770123,
            316184205,
            314115448,
            311769895
        ]
    },
    {
        "id": 322058612,
        "title": "Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Yang Zhang",
            "Philip David",
            "Boqing Gong"
        ],
        "references": [
            322749812,
            320967151,
            319646698,
            327455026,
            320968206,
            320964974,
            319770420,
            319770291,
            319769909,
            319649103
        ]
    },
    {
        "id": 320163409,
        "title": "Unified Deep Supervised Domain Adaptation and Generalization",
        "abstract": "This work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high speed of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results.",
        "date": "September 2017",
        "authors": [
            "Saeid Motiian",
            "Marco Piccirilli",
            "Donald Adjeroh",
            "Gianfranco Doretto"
        ],
        "references": [
            309233234,
            327455026,
            320971949,
            320971009,
            319770357,
            319646709,
            310734327,
            309689916,
            309076254,
            308872817
        ]
    },
    {
        "id": 329748675,
        "title": "ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Yuhua Chen",
            "Wen Li",
            "Luc Van Gool"
        ],
        "references": [
            323655313,
            321180610,
            319695330,
            319406416,
            318813886,
            318559982,
            316505724,
            311610825,
            311610734,
            305984144
        ]
    },
    {
        "id": 322058653,
        "title": "Associative Domain Adaptation",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Philip Haeusser",
            "Thomas Frerix",
            "Alexander Mordvintsev",
            "Daniel Cremers"
        ],
        "references": [
            320967151,
            311715036,
            306376890,
            305196650,
            304409658,
            301836275,
            289917319,
            277333816,
            268079628,
            265295439
        ]
    },
    {
        "id": 322058594,
        "title": "Open Set Domain Adaptation",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Pau Panareda Busto",
            "Juergen Gall"
        ],
        "references": [
            305288028,
            301846400,
            300359063,
            266378568,
            265967143,
            262808250,
            261989633,
            260341021,
            259930829,
            258218677
        ]
    },
    {
        "id": 322058433,
        "title": "Deeper, Broader and Artier Domain Generalization",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Da Li",
            "Yongxin Yang",
            "Yi-Zhe Song",
            "Timothy Hospedales"
        ],
        "references": [
            306376890,
            305648390,
            304018243,
            303682017,
            303409567,
            290324486,
            281427666,
            269935399,
            268079628,
            254461838
        ]
    },
    {
        "id": 321417854,
        "title": "ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes",
        "abstract": "Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two components can be readily integrated into existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We achieve a new state-of-the-art of 39.4% mean IoU on the Cityscapes dataset by adapting from the GTAV dataset.",
        "date": "November 2017",
        "authors": [
            "Yuhua Chen",
            "Wen Li",
            "Luc Van Gool"
        ],
        "references": [
            322058612,
            319695330,
            319406416,
            318813886,
            318559982,
            316505724,
            311610734,
            305984144,
            302305068,
            301880609
        ]
    },
    {
        "id": 322059321,
        "title": "Adversarial Examples for Semantic Segmentation and Object Detection",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Cihang Xie",
            "Jianyu Wang",
            "Zhishuai Zhang",
            "Yuyin Zhou"
        ],
        "references": [
            329488196,
            322058778,
            320968636,
            319770430,
            319770378,
            319770357,
            319770332,
            319770291,
            319770183,
            319770131
        ]
    },
    {
        "id": 320891224,
        "title": "Mitigating adversarial effects through randomization",
        "abstract": "Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 110 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense.",
        "date": "November 2017",
        "authors": [
            "Cihang Xie",
            "Jianyu Wang",
            "Zhishuai Zhang",
            "Zhou Ren"
        ],
        "references": [
            329743392,
            322059321,
            319770123,
            319735957,
            318527873,
            321503486,
            320671312,
            319770378,
            319770168,
            319769911
        ]
    },
    {
        "id": 320321492,
        "title": "Standard detectors aren't (currently) fooled by physical adversarial stop signs",
        "abstract": "An adversarial example is an example that has been adjusted to produce the wrong label when presented to a system at test time. If adversarial examples existed that could fool a detector, they could be used to (for example) wreak havoc on roads populated with smart vehicles. Recently, we described our difficulties creating physical adversarial stop signs that fool a detector. More recently, Evtimov et al. produced a physical adversarial stop sign that fools a proxy model of a detector. In this paper, we show that these physical adversarial stop signs do not fool two standard detectors (YOLO and Faster RCNN) in standard configuration. Evtimov et al.'s construction relies on a crop of the image to the stop sign; this crop is then resized and presented to a classifier. We argue that the cropping and resizing procedure largely eliminates the effects of rescaling and of view angle. Whether an adversarial attack is robust under rescaling and change of view direction remains moot. We argue that attacking a classifier is very different from attacking a detector, and that the structure of detectors -- which must search for their own bounding box, and which cannot estimate that box very accurately -- makes it quite likely that adversarial patterns are strongly disrupted. Finally, an adversarial pattern on a physical object that could fool a detector would have to be adversarial in the face of a wide family of parametric distortions (scale; view angle; box shift inside the detector; illumination; and soon). Such a pattern would be of great theoretical and practical interest. There is currently no evidence that such patterns exist.",
        "date": "October 2017",
        "authors": [
            "Jiajun Lu",
            "Hussein Sibai",
            "Evan Fabry",
            "David Forsyth"
        ],
        "references": [
            318729572,
            318392300,
            313713721,
            309460742,
            319770378,
            319769911,
            318671233,
            315764926,
            311925600,
            311610675
        ]
    },
    {
        "id": 318729572,
        "title": "Robust Physical-World Attacks on Machine Learning Models",
        "abstract": "Deep neural network-based classifiers are known to be vulnerable to adversarial examples that can fool them into misclassifying their input through the addition of small-magnitude perturbations. However, recent studies have demonstrated that such adversarial examples are not very effective in the physical world--they either completely fail to cause misclassification or only work in restricted cases where a relatively complex image is perturbed and printed on paper. In this paper we propose a new attack algorithm--Robust Physical Perturbations (RP2)-- that generates perturbations by taking images under different conditions into account. Our algorithm can create spatially-constrained perturbations that mimic vandalism or art to reduce the likelihood of detection by a casual observer. We show that adversarial examples generated by RP2 achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions. We physically realized and evaluated two attacks, one that causes a Stop sign to be misclassified as a Speed Limit sign in 100% of the testing conditions, and one that causes a Right Turn sign to be misclassified as either a Stop or Added Lane sign in 100% of the testing conditions.",
        "date": "July 2017",
        "authors": [
            "Ivan Evtimov",
            "Kevin Eykholt",
            "Earlence Fernandes",
            "Tadayoshi Kohno"
        ],
        "references": [
            322059321,
            321994706,
            318392300,
            326854186,
            322058778,
            320968636,
            319770378,
            319770183,
            318671233,
            317919653
        ]
    },
    {
        "id": 322058778,
        "title": "Universal Adversarial Perturbations Against Semantic Image Segmentation",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Jan Hendrik Metzen",
            "Mummadi Chaithanya Kumar",
            "Thomas Brox",
            "Volker Fischer"
        ],
        "references": [
            322049853,
            320619799,
            315666528,
            314237740,
            313713721,
            309730122,
            309460742,
            309448167,
            306304648,
            305186613
        ]
    },
    {
        "id": 320055388,
        "title": "Can you fool AI with adversarial examples on a visual Turing test?",
        "abstract": "Deep learning has achieved impressive results in many areas of Computer Vision and Natural Language Pro- cessing. Among others, Visual Question Answering (VQA), also referred to a visual Turing test, is considered one of the most compelling problems, and recent deep learning models have reported significant progress in vision and language modeling. Although Artificial Intelligence (AI) is getting closer to passing the visual Turing test, at the same time the existence of adversarial examples to deep learning systems may hinder the practical application of such systems. In this work, we conduct the first extensive study on adversarial examples for VQA systems. In particular, we focus on generating targeted adversarial examples for a VQA system while the target is considered to be a question-answer pair. Our evaluation shows that the success rate of whether a targeted adversarial example can be generated is mostly dependent on the choice of the target question-answer pair, and less on the choice of images to which the question refers. We also report the language prior phenomenon of a VQA model, which can explain why targeted adversarial examples are hard to generate for some question-answer targets. We also demonstrate that a compositional VQA architecture is slightly more resilient to adversarial attacks than a non-compositional one. Our study sheds new light on how to build deep vision and language resilient models robust against adversarial examples.",
        "date": "September 2017",
        "authors": [
            "Xiaojun Xu",
            "Xinyun Chen",
            "Chang Liu",
            "Anna Rohrbach"
        ],
        "references": [
            322059321,
            316269908,
            315454828,
            313713721,
            309797568,
            309730122,
            307560165,
            303750057,
            301844872,
            284788388
        ]
    },
    {
        "id": 318671233,
        "title": "Synthesizing Robust Adversarial Examples",
        "abstract": "Neural networks are susceptible to adversarial examples: small, carefully-crafted perturbations can cause networks to misclassify inputs in arbitrarily chosen ways. However, some studies have showed that adversarial examples crafted following the usual methods are not tolerant to small transformations: for example, zooming in on an adversarial image can cause it to be classified correctly again. This raises the question of whether adversarial examples are a concern in practice, because many real-world systems capture images from multiple scales and perspectives. This paper shows that adversarial examples can be made robust to distributions of transformations. Our approach produces single images that are simultaneously adversarial under all transformations in a chosen distribution, showing that we cannot rely on transformations such as rescaling, translation, and rotation to protect against adversarial examples.",
        "date": "July 2017",
        "authors": [
            "Anish Athalye",
            "Logan Engstrom",
            "Andrew Ilyas",
            "Kevin Kwok"
        ],
        "references": [
            318392300,
            307560165,
            305186613,
            259440613,
            348496402,
            319770378,
            319770332,
            317919653,
            317673614,
            311610675
        ]
    },
    {
        "id": 320975841,
        "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
        "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",
        "date": "November 2017",
        "authors": [
            "Judy Hoffman",
            "Eric Tzeng",
            "Taesung Park",
            "Jun-Yan Zhu"
        ],
        "references": [
            322060135,
            322058612,
            320967151,
            327808957,
            322060428,
            322058208,
            320971438,
            320964974,
            320014396,
            319770355
        ]
    },
    {
        "id": 318868246,
        "title": "Joint Transmission Map Estimation and Dehazing using Deep Networks",
        "abstract": "Single image haze removal is an extremely challenging problem due to its inherent ill-posed nature. Several prior-based and learning-based methods have been proposed in the literature to solve this problem and they have achieved superior results. However, most of the existing methods assume constant atmospheric light model and tend to follow a two- step procedure involving prior-based methods for estimating transmission map followed by calculation of dehazed image using the closed form solution. In this paper, we relax the constant atmospheric light assumption and propose a novel unified single image dehazing network that jointly estimates the transmission map and performs dehazing. In other words, our new approach provides an end-to-end learning framework, where the inherent transmission map and dehazed result are learned directly from the loss function. Extensive experiments on synthetic and real datasets with challenging hazy images demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.",
        "date": "August 2017",
        "authors": [
            "He Zhang",
            "Vishwanath Sindagi",
            "Vishal M. Patel"
        ],
        "references": [
            317771076,
            315489372,
            320968363,
            320963612,
            319770411,
            319770355,
            319770344,
            319770291,
            314100361,
            312771295
        ]
    },
    {
        "id": 319770327,
        "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.",
        "date": "November 2016",
        "authors": [
            "Fisher Yu",
            "Vladlen Koltun"
        ],
        "references": []
    },
    {
        "id": 318696955,
        "title": "Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "Matthew Johnson-Roberson",
            "Charles Barto",
            "Rounak Mehta",
            "Sharath Nittur Sridhar"
        ],
        "references": [
            311610734,
            308804180,
            305984144,
            304417782,
            301880609,
            286134669,
            277023129,
            265295439,
            263610826,
            221620190
        ]
    },
    {
        "id": 313394663,
        "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
        "abstract": "Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
        "date": "February 2017",
        "authors": [
            "Guy Katz",
            "Clark Wayne Barrett",
            "David L. Dill",
            "Kyle Julian"
        ],
        "references": [
            316911609,
            315861235,
            309401703,
            345814185,
            319770387,
            319770378,
            319770183,
            313202057,
            311609991,
            310827644
        ]
    },
    {
        "id": 307560165,
        "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Anh Nguyen",
            "Jason Yosinski",
            "Jeff Clune"
        ],
        "references": [
            304707177,
            269935591,
            268079628,
            266164467,
            265295439,
            265022827,
            319770183,
            309076254,
            267960550,
            266476598
        ]
    },
    {
        "id": 303486514,
        "title": "Measuring Neural Net Robustness with Constraints",
        "abstract": "Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \"overfit\" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.",
        "date": "May 2016",
        "authors": [
            "Osbert Bastani",
            "Yani Ioannou",
            "Leonidas Lampropoulos",
            "Dimitrios Vytiniotis"
        ],
        "references": [
            307560165,
            301875216,
            284219659,
            284096769,
            283762349,
            283043571,
            272195158,
            269935591,
            309773102,
            301854845
        ]
    },
    {
        "id": 301879941,
        "title": "Improving the Robustness of Deep Neural Networks via Stability Training",
        "abstract": "In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.",
        "date": "April 2016",
        "authors": [
            "Stephan Zheng",
            "Yang Song",
            "Thomas Leung",
            "Ian Goodfellow"
        ],
        "references": [
            308161118,
            307560165,
            305196650,
            279632719,
            277022946,
            269935591,
            319770183,
            308820963,
            286794765,
            272194743
        ]
    },
    {
        "id": 301875216,
        "title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples",
        "abstract": "Advances in deep learning have led to the broad adoption of Deep Neural Networks (DNNs) to a range of important machine learning problems, e.g., guiding autonomous vehicles, speech recognition, malware detection. Yet, machine learning models, including DNNs, were shown to be vulnerable to adversarial samples-subtly (and often humanly indistinguishably) modified malicious inputs crafted to compromise the integrity of their outputs. Adversarial examples thus enable adversaries to manipulate system behaviors. Potential attacks include attempts to control the behavior of vehicles, have spam content identified as legitimate content, or have malware identified as legitimate software. Adversarial examples are known to transfer from one model to another, even if the second model has a different architecture or was trained on a different set. We introduce the first practical demonstration that this cross-model transfer phenomenon enables attackers to control a remotely hosted DNN with no access to the model, its parameters, or its training data. In our demonstration, we only assume that the adversary can observe outputs from the target DNN given inputs chosen by the adversary. We introduce the attack strategy of fitting a substitute model to the input-output pairs in this manner, then crafting adversarial examples based on this auxiliary model. We evaluate the approach on existing DNN datasets and real-world settings. In one experiment, we force a DNN supported by MetaMind (one of the online APIs for DNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude with experiments exploring why adversarial samples transfer between DNNs, and a discussion on the applicability of our attack when targeting machine learning algorithms distinct from DNNs.",
        "date": "February 2016",
        "authors": [
            "Nicolas Papernot",
            "Patrick McDaniel",
            "Ian Goodfellow",
            "Somesh Jha"
        ],
        "references": [
            308027534,
            306304648,
            305186613,
            302569301,
            319770378,
            319770272,
            319770183,
            319769909,
            303032467,
            289733708
        ]
    },
    {
        "id": 348496402,
        "title": "Adversarial Examples in the Physical World",
        "abstract": "",
        "date": "July 2018",
        "authors": [
            "Alexey Kurakin",
            "Ian Goodfellow",
            "Samy Bengio"
        ],
        "references": [
            265252627
        ]
    },
    {
        "id": 304226143,
        "title": "Concrete Problems in AI Safety",
        "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
        "date": "June 2016",
        "authors": [
            "Dario Amodei",
            "Chris Olah",
            "Jacob Steinhardt",
            "Paul Christiano"
        ],
        "references": [
            319770179,
            309120602,
            307560165,
            305881456,
            305823490,
            303682017,
            303311529,
            301881129,
            301880033,
            301877554
        ]
    },
    {
        "id": 303032467,
        "title": "The Limitations of Deep Learning in Adversarial Settings",
        "abstract": "",
        "date": "March 2016",
        "authors": [
            "Nicolas Papernot",
            "Patrick McDaniel",
            "Somesh Jha",
            "Matt Fredrikson"
        ],
        "references": [
            305196650,
            269935591,
            269722671,
            269280482,
            269099395,
            268079628,
            259440613,
            215991023,
            200744481,
            2985446
        ]
    },
    {
        "id": 291437554,
        "title": "Understanding Deep Convolutional Networks",
        "abstract": "Deep convolutional networks provide state-of-the-art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and nonlinearities. A mathematical framework is introduced to analyse their properties. Computations of invariants involve multiscale contractions with wavelets, the linearization of hierarchical symmetries and sparse separations. Applications are discussed. \u00a9 2016 The Author(s) Published by the Royal Society. All rights reserved.",
        "date": "January 2016",
        "authors": [
            "St\u00e9phane Georges Mallat"
        ],
        "references": [
            319770352,
            277411157,
            277334140,
            272195584,
            270287280,
            269040844,
            263130506,
            259440613,
            258566960,
            258566607
        ]
    },
    {
        "id": 320971795,
        "title": "Optical Flow Estimation Using a Spatial Pyramid Network",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Anurag Ranjan",
            "Michael J Black"
        ],
        "references": [
            308865856,
            308277922,
            319770420,
            319770168,
            314635674,
            313748243,
            311756751,
            311609419,
            311609041,
            310792158
        ]
    },
    {
        "id": 320967536,
        "title": "Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Joel Janai",
            "Fatma Guney",
            "Jonas Wulff",
            "Michael J Black"
        ],
        "references": [
            319770165,
            314365971,
            311610734,
            319770183,
            319770011,
            319769911,
            314666195,
            314635674,
            313556846,
            313437359
        ]
    },
    {
        "id": 320971948,
        "title": "End-to-End Learning of Driving Models from Large-Scale Video Datasets",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Huazhe Xu",
            "Yang Gao",
            "Fisher Yu",
            "Trevor Darrell"
        ],
        "references": [
            309126870,
            308034527,
            305779478,
            303698513,
            303544998,
            303449434,
            302305068,
            301648615,
            280631115,
            278413880
        ]
    },
    {
        "id": 320971048,
        "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Gernot Riegler",
            "Ali Osman Ulusoy",
            "Andreas Geiger"
        ],
        "references": [
            322749812,
            317193734,
            308818469,
            308349377,
            308277898,
            305440736,
            304409658,
            304226155,
            304225895,
            303993087
        ]
    },
    {
        "id": 320968379,
        "title": "Multi-view 3D Object Detection Network for Autonomous Driving",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Xiaozhi Chen",
            "Huimin Ma",
            "Ji Wan",
            "Bo Li"
        ],
        "references": [
            305893446,
            305638531,
            305440736,
            305011709,
            304552923,
            301877453,
            301818275,
            281650252,
            272521784,
            221110937
        ]
    },
    {
        "id": 320964395,
        "title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Eddy Ilg",
            "Nikolaus Mayer",
            "Tonmoy Saikia",
            "Margret Keuper"
        ],
        "references": [
            317193612,
            314365971,
            309663584,
            308865856,
            308277922,
            305683093,
            301874393,
            301873695,
            301839167,
            286301890
        ]
    },
    {
        "id": 309460742,
        "title": "Universal adversarial perturbations",
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
        "date": "October 2016",
        "authors": [
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Alhussein Fawzi",
            "Omar Fawzi",
            "Pascal Frossard"
        ],
        "references": [
            317191738,
            309388209,
            307560165,
            319770378,
            319770183,
            319770131,
            311757216,
            311610675,
            311609041,
            309773102
        ]
    },
    {
        "id": 309192145,
        "title": "Are Accuracy and Robustness Correlated?",
        "abstract": "Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.",
        "date": "December 2016",
        "authors": [
            "Andras Rozsa",
            "Manuel G\u00fcnther",
            "Terrance E. Boult"
        ],
        "references": [
            306304648,
            319770378,
            319770272,
            319770183,
            319770131,
            312727767,
            311757216,
            311610675,
            311609772,
            311609041
        ]
    },
    {
        "id": 307307101,
        "title": "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples",
        "abstract": "Deep neural networks have been shown to suffer from a surprising weakness: their classification outputs can be changed by small, non-random perturbations of their inputs. This adversarial example phenomenon has been explained as originating from deep networks being \"too linear\" (Goodfellow et al., 2014). We show here that the linear explanation of adversarial examples presents a number of limitations: the formal argument is not convincing, linear classifiers do not always suffer from the phenomenon, and when they do their adversarial examples are different from the ones affecting deep networks. We propose a new perspective on the phenomenon. We argue that adversarial examples exist when the classification boundary lies close to the submanifold of sampled data, and present a mathematical analysis of this new perspective in the linear case. We define the notion of adversarial strength and show that it can be reduced to the deviation angle between the classifier considered and the nearest centroid classifier. Then, we show that the adversarial strength can be made arbitrarily high independently of the classification performance due to a mechanism that we call boundary tilting. This result leads us to defining a new taxonomy of adversarial examples. Finally, we show that the adversarial strength observed in practice is directly dependent on the level of regularisation used and the strongest adversarial examples, symptomatic of overfitting, can be avoided by using a proper level of regularisation.",
        "date": "August 2016",
        "authors": [
            "Thomas Tanay",
            "Lewis D Griffin"
        ],
        "references": [
            305196650,
            319770291,
            319770272,
            311609041,
            304552788,
            286794765,
            286512696,
            273471270,
            272837232,
            272194743
        ]
    },
    {
        "id": 306304648,
        "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks",
        "abstract": "",
        "date": "May 2016",
        "authors": [
            "Nicolas Papernot",
            "Patrick McDaniel",
            "Xi Wu",
            "Somesh Jha"
        ],
        "references": [
            284788388,
            319770378,
            319770183,
            319769909,
            306218037,
            305386599,
            303256841,
            303032467,
            289733708,
            284500238
        ]
    },
    {
        "id": 305186613,
        "title": "Adversarial examples in the physical world",
        "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
        "date": "July 2016",
        "authors": [
            "Alexey Kurakin",
            "Ian Goodfellow",
            "Samy Bengio"
        ],
        "references": [
            309448167,
            303490367,
            301875216,
            272408120,
            269935591,
            265295439,
            265252627,
            285648386,
            267960550,
            265178583
        ]
    },
    {
        "id": 319770378,
        "title": "Explaining and harnessing adversarial examples",
        "abstract": "Several machine learning models, including neural networks, consistently mis- classify adversarial examples\u2014inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed in- put results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to ad- versarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Us- ing this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
        "date": "January 2015",
        "authors": [
            "Ian Goodfellow",
            "Jonathon Shlens",
            "Christian Szegedy"
        ],
        "references": []
    },
    {
        "id": 317919653,
        "title": "Towards Evaluating the Robustness of Neural Networks",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "Nicholas Carlini",
            "David Wagner"
        ],
        "references": [
            319769813,
            313887527,
            308883509,
            306304648,
            306093833,
            305401842,
            305186613,
            303970050,
            303490367,
            303486514
        ]
    },
    {
        "id": 311610675,
        "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Alhussein Fawzi",
            "Pascal Frossard"
        ],
        "references": [
            307560165,
            305196650,
            272195158,
            269935591,
            269722671,
            269280482,
            265777799,
            264979485,
            259440613,
            241637478
        ]
    },
    {
        "id": 306226844,
        "title": "Towards Evaluating the Robustness of Neural Networks",
        "abstract": "We consider how to measure the robustness of a neural network against adversarial examples. We introduce three new attack algorithms, tailored to three different distance metrics, to find adversarial examples: given an image x and a target class, we can find a new image x' that is similar to x but classified differently. We show that our attacks are significantly more powerful than previously published attacks: in particular, they find adversarial examples that are between 2 and 10 times closer. Then, we study defensive distillation, a recently proposed approach which increases the robustness of neural networks. Our attacks succeed with probability 200 higher than previous attacks against defensive distillation and effectively break defensive distillation, showing that it provides little added security. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.",
        "date": "August 2016",
        "authors": [
            "Nicholas Carlini",
            "David Wagner"
        ],
        "references": [
            319769813,
            306304648,
            306093833,
            292074166,
            284788388,
            284579051,
            284476548,
            284219659,
            284097112,
            259440613
        ]
    },
    {
        "id": 305779814,
        "title": "A study of the effect of JPG compression on adversarial images",
        "abstract": "Neural network image classifiers are known to be vulnerable to adversarial images, i.e., natural images which have been modified by an adversarial perturbation specifically designed to be imperceptible to humans yet fool the classifier. Not only can adversarial images be generated easily, but these images will often be adversarial for networks trained on disjoint subsets of data or with different architectures. Adversarial images represent a potential security risk as well as a serious machine learning challenge---it is clear that vulnerable neural networks perceive images very differently from humans. Noting that virtually every image classification data set is composed of JPG images, we evaluate the effect of JPG compression on the classification of adversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we found that JPG compression often reverses the drop in classification accuracy to a large extent, but not always. As the magnitude of the perturbations increases, JPG recompression alone is insufficient to reverse the effect.",
        "date": "August 2016",
        "authors": [
            "Gintare Karolina Dziugaite",
            "Zoubin Ghahramani",
            "Daniel M. Roy"
        ],
        "references": [
            306304648,
            305186613,
            284097112,
            283043571,
            269935591,
            269722671,
            265295439,
            259440613,
            319770378,
            311609041
        ]
    },
    {
        "id": 309766071,
        "title": "The Loss Surface of Residual Networks: Ensembles and the Role of Batch Normalization",
        "abstract": "Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network's depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.",
        "date": "November 2016",
        "authors": [
            "Etai Littwin",
            "Lior Wolf"
        ],
        "references": [
            303409435,
            269040844,
            215616968,
            321620746,
            319770414,
            286512696,
            272194743,
            267436967,
            45904516
        ]
    },
    {
        "id": 308277743,
        "title": "Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation",
        "abstract": "CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.",
        "date": "October 2016",
        "authors": [
            "Golnaz Ghiasi",
            "Charless C. Fowlkes"
        ],
        "references": [
            319769910,
            304409176,
            302305068,
            301880609,
            301877556,
            281670742,
            269116202,
            268689640,
            257672343,
            256663526
        ]
    },
    {
        "id": 348972616,
        "title": "Baby FACS Workshop: Facial Action Coding System for Infants and Young Children (ICIS 2016)",
        "abstract": "",
        "date": "May 2016",
        "authors": [
            "Harriet Oster",
            "Marco Dondi"
        ],
        "references": []
    },
    {
        "id": 333685797,
        "title": "The recognition of 18 facial-bodily expressions across nine cultures",
        "abstract": "An enduring focus in the science of emotion is the question of which psychological states are signaled in expressive behavior. Based on empirical findings from previous studies, we created photographs of facial-bodily expressions of 18 states and presented these to participants in nine cultures. In a well-validated recognition paradigm, participants matched stories of causal antecedents to one of four expressions of the same valence. All 18 facial-bodily expressions were recognized at well above chance levels. We conclude by discussing the methodological shortcomings of our study and the conceptual implications of its findings. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
        "date": "June 2019",
        "authors": [
            "Daniel Cordaro",
            "Rui Sun",
            "Shanmukh Vasant Kamble",
            "Niranjan Hodder"
        ],
        "references": [
            335169837,
            323338145,
            320567202,
            316526239,
            304715744,
            301562754,
            325671498,
            318692796,
            317555921,
            309644912
        ]
    },
    {
        "id": 330941751,
        "title": "Emotional Expression: Advances in Basic Emotion Theory",
        "abstract": "In this article, we review recent developments in the study of emotional expression within a basic emotion framework. Dozens of new studies find that upwards of 20 emotions are signaled in multimodal and dynamic patterns of expressive behavior. Moving beyond word to stimulus matching paradigms, new studies are detailing the more nuanced and complex processes involved in emotion recognition and the structure of how people perceive emotional expression. Finally, we consider new studies documenting contextual influences upon emotion recognition. We conclude by extending these recent findings to questions about emotion-related physiology and the mammalian precursors of human emotion.",
        "date": "June 2019",
        "authors": [
            "Dacher Keltner",
            "Disa Sauter",
            "Jessica Tracy",
            "Alan Cowen"
        ],
        "references": [
            322868902,
            320668282,
            319195614,
            334728014,
            320700601,
            320645262,
            320318322,
            319195782,
            318327198,
            317555921
        ]
    },
    {
        "id": 329825604,
        "title": "Probabilistic Learning of Emotion Categories",
        "abstract": "Although the configurations of facial muscles that humans perceive vary continuously, we often represent emotions as categories. This suggests that, as in other domains of categorical perception such as speech and color perception, humans become attuned to features of emotion cues that map onto meaningful thresholds for these signals given their environments. However, little is known about the learning processes underlying the representation of these salient social signals. In Experiment 1 we test the role of statistical distributions of facial cues in the maintenance of an emotion category in both children (6\u20138 years old) and adults (18\u201322 years old). Children and adults learned the boundary between neutral and angry when provided with explicit feedback (supervised learning). However, after we exposed participants to different statistical distributions of facial cues, they rapidly shifted their category boundaries for each emotion during a testing phase. In Experiments 2 and 3, we replicated this finding and also tested the extent to which learners are able to track statistical distributions for multiple actors. Not only did participants form actor-specific categories, but the distributions of facial cues also influenced participants\u2019 trait judgments about the actors. Taken together, these data are consistent with the view that the way humans construe emotion (in this case, anger) is not only flexible, but reflects complex learning about the distributions of the myriad cues individuals experience in their social environments.",
        "date": "December 2018",
        "authors": [
            "Rista C Plate",
            "Adrienne Wood",
            "Kristina Woodard",
            "Seth D Pollak"
        ],
        "references": [
            324172308,
            321329409,
            318696146,
            291327980,
            326048713,
            322321709,
            320071198,
            318692796,
            309590894,
            291947931
        ]
    },
    {
        "id": 350442415,
        "title": "Culture and the Expression of Emotion",
        "abstract": "",
        "date": "December 1975",
        "authors": [
            "E. RICHARD SORENSON"
        ],
        "references": []
    },
    {
        "id": 344472047,
        "title": "Emotional Development: The Organization of Emotional Life in the Early Years",
        "abstract": "",
        "date": "January 1996",
        "authors": [
            "L. Alan Sroufe"
        ],
        "references": []
    },
    {
        "id": 331103751,
        "title": "Face Value: The Irresistible Influence of First Impressions",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "ALEXANDER TODOROV"
        ],
        "references": []
    },
    {
        "id": 330860463,
        "title": "The Expression of the Emotions in Man and Animals",
        "abstract": "",
        "date": "January 1965",
        "authors": [
            "Charles Darwin",
            "Konrad Lorenz"
        ],
        "references": []
    },
    {
        "id": 329768563,
        "title": "Cross-Cultural and Cultural-Specific Production and Perception of Facial Expressions of Emotion in the Wild",
        "abstract": "Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different.",
        "date": "December 2018",
        "authors": [
            "Ramprakash Srinivasan",
            "Aleix M. Martinez"
        ],
        "references": []
    },
    {
        "id": 329648042,
        "title": "The Big Book of Concepts",
        "abstract": "",
        "date": "January 2002",
        "authors": [
            "Gregory Murphy"
        ],
        "references": []
    },
    {
        "id": 321371928,
        "title": "When the Algorithm Itself Is a Racist: Diagnosing Ethical Harm in the Basic Components of Software",
        "abstract": "Computer algorithms organize and select information across a wide range of applications and industries, from search results to social media. Abuses of power by Internet platforms have led to calls for algorithm transparency and regulation. Algorithms have a particularly problematic history of processing information about race. Yet some analysts have warned that foundational computer algorithms are not useful subjects for ethical or normative analysis due to complexity, secrecy, technical character, or generality. We respond by investigating what it is an analyst needs to know to determine whether the algorithm in a computer system is improper, unethical, or illegal in itself. We argue that an \"algorithmic ethics\" can analyze a particular published algorithm. We explain the importance of developing a practical algorithmic ethics that addresses virtues, consequences, and norms: We increasingly delegate authority to algorithms, and they are fast becoming obscure but important elements of social structure. \u00a9 2016 Christian Sandvig, Kevin Hamilton, Karrie Karahalios, & Cedric Langbort.",
        "date": "January 2016",
        "authors": [
            "Christian Sandvig",
            "Kevin Hamilton",
            "Karrie Karahalios",
            "Cedric Langbort"
        ],
        "references": [
            297746742,
            282803178,
            346624964,
            330332396,
            313467350,
            305039654,
            285645757,
            283192578,
            282970489,
            282670710
        ]
    },
    {
        "id": 320061136,
        "title": "We get the algorithms of our ground truths: Designing referential databases in digital image processing",
        "abstract": "This article documents the practical efforts of a group of scientists designing an image-processing algorithm for saliency detection. By following the actors of this computer science project, the article shows that the problems often considered to be the starting points of computational models are in fact provisional results of time-consuming, collective and highly material processes that engage habits, desires, skills and values. In the project being studied, problematization processes lead to the constitution of referential databases called \u2018ground truths\u2019 that enable both the effective shaping of algorithms and the evaluation of their performances. Working as important common touchstones for research communities in image processing, the ground truths are inherited from prior problematization processes and may be imparted to subsequent ones. The ethnographic results of this study suggest two complementary analytical perspectives on algorithms: (1) an \u2018axiomatic\u2019 perspective that understands algorithms as sets of instructions designed to solve given problems computationally in the best possible way, and (2) a \u2018problem-oriented\u2019 perspective that understands algorithms as sets of instructions designed to computationally retrieve outputs designed and designated during specific problematization processes. If the axiomatic perspective on algorithms puts the emphasis on the numerical transformations of inputs into outputs, the problem-oriented perspective puts the emphasis on the definition of both inputs and outputs.",
        "date": "September 2017",
        "authors": [
            "Florian Jaton"
        ],
        "references": [
            333026320,
            325449921,
            325445986,
            317108288,
            313596747,
            311477573,
            309043557,
            308161218,
            302937305,
            298883947
        ]
    },
    {
        "id": 334384387,
        "title": "Multiaccuracy: Black-Box Post-Processing for Fairness in Classification",
        "abstract": "Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for \"black women\") even when the sensitive features (e.g. \"race\", \"gender\") are not given to the algorithm explicitly.",
        "date": "January 2019",
        "authors": [
            "Michael P. Kim",
            "Amirata Ghorbani",
            "James Zou"
        ],
        "references": [
            323142021,
            319534462,
            313107433,
            308327297,
            274317962,
            268988287,
            221618845,
            45872876,
            2669468,
            336336399
        ]
    },
    {
        "id": 330692906,
        "title": "Unthought: The Power of the Cognitive Nonconscious",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Katherine Hayles"
        ],
        "references": []
    },
    {
        "id": 328468455,
        "title": "The Epistemology of the Facebook News Feed as a News Source",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Anja Bechmann"
        ],
        "references": [
            319166164
        ]
    },
    {
        "id": 325779498,
        "title": "We Are Data: Algorithms and The Making of Our Digital Selves",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "JOHN CHENEY-LIPPOLD"
        ],
        "references": []
    },
    {
        "id": 323983704,
        "title": "Big Data's Disparate Impact",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Solon Barocas",
            "Andrew D. Selbst"
        ],
        "references": []
    },
    {
        "id": 323321910,
        "title": "The public and its problems",
        "abstract": "",
        "date": "January 1954",
        "authors": [
            "John Dewey"
        ],
        "references": []
    },
    {
        "id": 319912549,
        "title": "Situating methods in the magic of Big Data and AI",
        "abstract": "\u201cBig Data\u201d and \u201cartificial intelligence\u201d have captured the public imagination and are profoundly shaping social, economic, and political spheres. Through an interrogation of the histories, perceptions, and practices that shape these technologies, we problematize the myths that animate the supposed \u201cmagic\u201d of these systems. In the face of an increasingly widespread blind faith in data-driven technologies, we argue for grounding machine learning-based practices and untethering them from hype and fear cycles. One path forward is to develop a rich methodological framework for addressing the strengths and weaknesses of doing data analysis. Through provocatively reimagining machine learning as computational ethnography, we invite practitioners to prioritize methodological reflection and recognize that all knowledge work is situated practice.",
        "date": "September 2017",
        "authors": [
            "M. C. Elish",
            "danah boyd"
        ],
        "references": [
            315852155,
            297656557,
            292074166,
            289555278,
            281562384,
            276043196,
            273065430,
            271525133,
            270104965,
            254296668
        ]
    },
    {
        "id": 325671148,
        "title": "Belief and feeling: Evidence for an accessibility model of emotional self-report",
        "abstract": "This review organizes a variety of phenomena related to emotional self-report. In doing so, the authors offer an accessibility model that specifies the types of factors that contribute to emotional self-reports under different reporting conditions. One important distinction is between emotion, which is episodic, experiential, and contextual, and beliefs about emotion, which are semantic, conceptual, and decontextualized. This distinction is important in understanding the discrepancies that often occur when people are asked to report on feelings they are currently experiencing versus those that they are not currently experiencing. The accessibility model provides an organizing framework for understanding self-reports of emotion and suggests some new directions for research.",
        "date": "November 2002",
        "authors": [
            "Michael D Robinson",
            "Gerald Clore"
        ],
        "references": [
            325671525,
            350686258,
            325146592,
            324346780,
            313188480,
            313051504,
            312991296,
            312942261,
            312893002,
            310439608
        ]
    },
    {
        "id": 316633386,
        "title": "The theory of constructed emotion: an active inference account of interoception and categorization",
        "abstract": "",
        "date": "May 2017",
        "authors": [
            "Lisa Feldman Barrett"
        ],
        "references": []
    },
    {
        "id": 313117282,
        "title": "A two way road: Efferent and Afferent Pathways of Autonomic Activity in Emotion",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "Neil A Harrison",
            "Sylvia D Kreibig",
            "Hugo Critchley"
        ],
        "references": [
            302459692,
            297900087,
            262106219,
            295762628,
            285446507,
            284688908,
            272033180,
            263694021,
            252163627,
            247218653
        ]
    },
    {
        "id": 312917228,
        "title": "Psychological essentialism",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "Doug L Medin",
            "Andrew Ortony"
        ],
        "references": []
    },
    {
        "id": 323424582,
        "title": "Statistical power analysis for the behavioral sciences",
        "abstract": "",
        "date": "January 1988",
        "authors": [
            "J. Cohen"
        ],
        "references": []
    },
    {
        "id": 320182358,
        "title": "The Origin of Species",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "Charles Darwin"
        ],
        "references": []
    },
    {
        "id": 313610258,
        "title": "The triple Helix: Gene, Organism, and Environment",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "R.C. Lewontin"
        ],
        "references": []
    },
    {
        "id": 309590895,
        "title": "The theory of constructed emotion: An active inference account of interoception and categorization",
        "abstract": "The science of emotion has been using folk psychology categories derived from philosophy to search for the brain basis of emotion. The last two decades of neuroscience research have brought us to the brink of a paradigm shift in understanding the workings of the brain, however, setting the stage to revolutionize our understanding of what emotions are and how they work. In this paper, we begin with the structure and function of the brain, and from there deduce what the biological basis of emotions might be. The answer is a brain-based, computational account called the theory of constructed emotion.",
        "date": "October 2016",
        "authors": [
            "Lisa Feldman Barrett"
        ],
        "references": [
            345754147,
            312940535,
            310481807,
            301829059,
            301645872,
            301562754,
            292672576,
            283736933,
            282219803,
            280119608
        ]
    },
    {
        "id": 309590894,
        "title": "Functionalism Cannot Save the Classical View of Emotion",
        "abstract": "",
        "date": "October 2016",
        "authors": [
            "Lisa Feldman Barrett"
        ],
        "references": [
            312917228,
            310752474,
            232481843,
            232253630,
            11380627,
            330334618,
            236224317,
            216301091,
            51378911
        ]
    },
    {
        "id": 308760403,
        "title": "Adjusting for Publication Bias in Meta-Analysis",
        "abstract": "We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis\u2014that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.",
        "date": "September 2016",
        "authors": [
            "Blakeley B. McShane",
            "Ulf Bockenholt",
            "Karsten Hansen"
        ],
        "references": [
            308761280,
            280117028,
            268985081,
            259310513,
            254287922,
            235726244,
            220365391,
            200051683,
            47204540,
            43100294
        ]
    },
    {
        "id": 323008113,
        "title": "Autonomous UAV for Suspicious Action Detection using Pictorial Human Pose Estimation and Classi\ufb01cation",
        "abstract": "Visual autonomous systems capable of monitoring crowded areas and alerting the authorities in occurrence of a suspicious action can play a vital role in controlling crime rate. Previous atte mpts have been made to monitor crime using posture recognition but nothing exclusive to investigating actions of people in large populated area has been cited. In order resolve this shortcoming, we propose an autonomous unmanned aerial vehicle (UAV) visual surveillance system that locates humans in image frames followed by pose estimation using weak constraints on position, appearance of body parts and image parsing. The estimated pose, represented as a pictorial structure, is flagged using the proposed Hough Orientation Calculator (HOC) on close resemblance with any pose in the suspicious action dataset. The robustness of the system is demonstrated on videos recorded using a UAV with no prior knowledge of background, lighting or location and scale of the human in the image. The system produces an accuracy of 71% and can also be applied on various other video sources such as CCTV camera.",
        "date": "June 2014",
        "authors": [
            "Surya Penmetsa",
            "Fatima Minhuj",
            "Amarjot Singh",
            "Omkar S N"
        ],
        "references": [
            279885582,
            243168498,
            225480588,
            224605168,
            319770820,
            225223935,
            224579272,
            224135934,
            221619514,
            221566576
        ]
    },
    {
        "id": 319391723,
        "title": "Efficient Convolutional Network Learning using Parametric Log based Dual-Tree Wavelet ScatterNet",
        "abstract": "We propose a DTCWT ScatterNet Convolutional Neural Network (DTSCNN) formed by replacing the first few layers of a CNN network with a parametric log based DTCWT ScatterNet. The ScatterNet extracts edge based invariant representations that are used by the later layers of the CNN to learn high-level features. This improves the training of the network as the later layers can learn more complex patterns from the start of learning because the edge representations are already present. The efficient learning of the DTSCNN network is demonstrated on CIFAR-10 and Caltech-101 datasets. The generic nature of the ScatterNet front-end is shown by an equivalent performance to pre-trained CNN front-ends. A comparison with the state-of-the-art on CIFAR-10 and Caltech-101 datasets is also presented.",
        "date": "August 2017",
        "authors": [
            "Amarjot Singh",
            "Nick Kingsbury"
        ],
        "references": [
            319501306,
            313671597,
            313642484,
            282072645,
            347495711,
            321789431,
            310752533,
            308278775,
            306218037,
            303449354
        ]
    },
    {
        "id": 319391371,
        "title": "ScatterNet Hybrid Deep Learning (SHDL) Network For Object Classification",
        "abstract": "The paper proposes the ScatterNet Hybrid Deep Learning (SHDL) network that extracts invariant and discriminative image representations for object recognition. SHDL framework is constructed with a multi-layer ScatterNet front-end, an unsupervised learning middle, and a supervised learning back-end module. Each layer of the SHDL network is automatically designed as an explicit optimization problem leading to an optimal deep learning architecture with improved computational performance as compared to the more usual deep network architectures. SHDL network produces the state-of-the-art classification performance against unsupervised and semi-supervised learning (GANs) on two image datasets. Advantages of the SHDL network over supervised methods (NIN, VGG) are also demonstrated with experiments performed on training datasets of reduced size.",
        "date": "August 2017",
        "authors": [
            "Amarjot Singh",
            "Nick Kingsbury"
        ],
        "references": [
            313671597,
            313642484,
            305881127,
            301857470,
            272092231,
            308278775,
            303449354,
            287273202,
            279065417,
            265385906
        ]
    },
    {
        "id": 313671597,
        "title": "Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification",
        "abstract": "This paper introduces a Deep Scattering network that utilizes Dual-Tree complex wavelets to extract translation invariant representations from an input signal. The computationally efficient Dual-Tree wavelets decompose the input signal into densely spaced representations over scales. Translation invariance is introduced in the representations by applying a non-linearity over a region followed by averaging. The discriminatory information in the densely spaced, locally smooth, signal representations aids the learning of the classifier. The proposed network is shown to outperform Mallat's ScatterNet on four datasets with different modalities on classification accuracy.",
        "date": "February 2017",
        "authors": [
            "Amarjot Singh",
            "Nick Kingsbury"
        ],
        "references": [
            271218718,
            270287280,
            240308778,
            308278775,
            301954085,
            271738228,
            265603532,
            261114727,
            248192679,
            222658553
        ]
    },
    {
        "id": 313642484,
        "title": "Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for Object Classification",
        "abstract": "We introduce a ScatterNet that uses a parametric log transformation with Dual-Tree complex wavelets to extract translation invariant representations from a multi-resolution image. The parametric transformation aids the OLS pruning algorithm by converting the skewed distributions into relatively mean-symmetric distributions while the Dual-Tree wavelets improve the computational efficiency of the network. The proposed network is shown to outperform Mallat's ScatterNet on two image datasets, both for classification accuracy and computational efficiency. The advantages of the proposed network over other supervised and some unsupervised methods are also presented using experiments performed on different training dataset sizes.",
        "date": "February 2017",
        "authors": [
            "Amarjot Singh",
            "Nick Kingsbury"
        ],
        "references": [
            270287280,
            265908778,
            260642043,
            240308778,
            228638826,
            228095620,
            319770820,
            316240097,
            261226002,
            261121846
        ]
    },
    {
        "id": 286840424,
        "title": "Fire detection in the buildings using image processing",
        "abstract": "To reduce loss of life and property from fire, an early warning is an imperative. A fire detectionsystem based on light detection and analysis is proposed in the paper. This system uses HSV and YCbCr color models with given conditions to separate orange, yellow, and high brightness light from background and ambient light. Fire growth is analysed and calculated based on frame differences. The overall accuracy from the experiments has been greater than 90%.",
        "date": "March 2014",
        "authors": [
            "Jareerat Seebamrungsat",
            "Suphachai Praising",
            "Panomkhawn Riyamongkol"
        ],
        "references": [
            300861934,
            289639698,
            269358470,
            261314986,
            256818229,
            252252777,
            247162027,
            247161944,
            222968384
        ]
    },
    {
        "id": 278048481,
        "title": "Flowing ConvNets for Human Pose Estimation in Videos",
        "abstract": "The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a new network architecture that: (i) regresses a confidence heatmap of joint position predictions; (ii) incorporates optical flow at a mid-layer to align heatmap predictions from neighbouring frames; and (iii) includes a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, and one that regresses joint coordinates directly. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset.",
        "date": "June 2015",
        "authors": [
            "Tomas Pfister",
            "James Charles",
            "Andrew Zisserman"
        ],
        "references": [
            319769910,
            319770430,
            319770276,
            319770268,
            319770183,
            311610893,
            310752533,
            309076254,
            307719713,
            286594663
        ]
    },
    {
        "id": 277334861,
        "title": "Robust Optimization for Deep Regression",
        "abstract": "Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. We demonstrate faster convergence and better generalization of our robust loss function for the task of human pose estimation, on four publicly available datasets. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in these datasets.",
        "date": "May 2015",
        "authors": [
            "Vasileios Belagiannis",
            "Christian Rupprecht",
            "Gustavo Carneiro",
            "Nassir Navab"
        ],
        "references": [
            324346860,
            319770430,
            319770362,
            319770269,
            319770268,
            319770183,
            319770168,
            312830570,
            308300968,
            304533937
        ]
    },
    {
        "id": 273316746,
        "title": "Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos",
        "abstract": "Our objective is to efficiently and accurately estimate the upper body pose of humans in gesture videos. To this end, we build on the recent successful applications of deep convolutional neural networks (ConvNets). Our novelties are: (i) our method is the first to our knowledge to use ConvNets for estimating human pose in videos; (ii) a new network that exploits temporal information from multiple frames, leading to better performance; (iii) showing that pre-segmenting the foreground of the video improves performance; and (iv) demonstrating that even without foreground segmentations, the network learns to abstract away from the background and can estimate the pose even in the presence of a complex, varying background. We evaluate our method on the BBC TV Signing dataset and show that our pose predictions are significantly better, and an order of magnitude faster to compute, than the state of the art [3].",
        "date": "November 2014",
        "authors": [
            "Tomas Pfister",
            "Karen Simonyan",
            "James Charles",
            "Andrew Zisserman"
        ],
        "references": [
            271573182,
            269250104,
            319770430,
            319770357,
            319770276,
            319770268,
            319770183,
            286594438,
            267960550,
            265385906
        ]
    },
    {
        "id": 272092231,
        "title": "A Novel Method to Improve Model fitting for Stock Market Prediction",
        "abstract": "Forecasting the trends of stock market is of extreme importance and profitable to stock market traders and also to the researchers who are always trying to find an analogy to describe the behaviour of stock market. Various data mining techniques have been implemented in the recent past to predict the behaviour of stock market. Every method tries to fit a model to the training data to predict the future. As it is obvious the accuracy of prediction depends on the model fitting. We propose a linear model for stock market prediction and further elaborate on improving the fit of the model. The proposed model and the correction method are tested on Istanbul stock exchange. The proposed model fits the dataset with an average error of 12% which is corrected by proposed fitting method to an average of 6%.",
        "date": "August 2013",
        "authors": [
            "Amarjot Singh",
            "Samarth Gupta",
            "Sukriti Jain"
        ],
        "references": [
            223172600,
            43655911,
            313704273,
            224654606,
            224300554,
            222428763,
            222192848,
            221947909,
            220220002,
            43789350
        ]
    },
    {
        "id": 234837419,
        "title": "Facial and Vocal Expressions of Emotion",
        "abstract": "A flurry of theoretical and empirical work concerning the production of and response to facial and vocal expressions has occurred in the past decade. That emotional expressions express emotions is a tautology but may not be a fact. Debates have centered on universality, the nature of emotion, and the link between emotions and expressions. Modern evolutionary theory is informing more models, emphasizing that expressions are directed at a receiver, that the interests of sender and receiver can conflict, that there are many determinants of sending an expression in addition to emotion, that expressions influence the receiver in a variety of ways, and that the receiver's response is more than simply decoding a message.",
        "date": "November 2003",
        "authors": [
            "James A. Russell",
            "Jo-Anne Bachorowski",
            "Jos\u00e9-Miguel Fernandez-Dols"
        ],
        "references": [
            291697408,
            346674939,
            344045961,
            313168642,
            312892586,
            303517739,
            303266474,
            300978621,
            297198615,
            296962014
        ]
    },
    {
        "id": 209436299,
        "title": "Coherence Between Expressive and Experiential Systems in Emotion",
        "abstract": "In order to assess the extent of coherence in emotional response systems, we examined the relationship between facial expression and self-report of emotion at multiple points in time during an affective episode. We showed subjects brief films that were selected for their ability to elicit disgust and fear, and we asked them to report on their emotions using a new reporting procedure. This procedure, called cued-review, allows subjects to rate the degree to which they experienced each of several categories of emotion for many locations over the time interval of a stimulus period. When facial expressions and reports of emotion were analysed for specific moments in film time, there was a high degree of temporal linkage and categorical agreement between facial expression and self-report, as predicted. Coherence was even stronger for more intense emotional events. This is the first evidence of linkage between facial expression and self-report of emotion on a momentary basis.",
        "date": "April 2005",
        "authors": [
            "Erika L Rosenberg",
            "Paul Ekman"
        ],
        "references": [
            279063515,
            233446172,
            232484301,
            232460206,
            325674248,
            248233333,
            247497121,
            239443612,
            232569983,
            232554482
        ]
    },
    {
        "id": 26674080,
        "title": "Amygdala Activation Predicts Gaze toward Fearful Eyes",
        "abstract": "The human amygdala can be robustly activated by presenting fearful faces, and it has been speculated that this activation has functional relevance for redirecting the gaze toward the eye region. To clarify this relationship between amygdala activation and gaze-orienting behavior, functional magnetic resonance imaging data and eye movements were simultaneously acquired in the current study during the evaluation of facial expressions. Fearful, angry, happy, and neutral faces were briefly presented to healthy volunteers in an event-related manner. We controlled for the initial fixation by unpredictably shifting the faces downward or upward on each trial, such that the eyes or the mouth were presented at fixation. Across emotional expressions, participants showed a bias to shift their gaze toward the eyes, but the magnitude of this effect followed the distribution of diagnostically relevant regions in the face. Amygdala activity was specifically enhanced for fearful faces with the mouth aligned to fixation, and this differential activation predicted gazing behavior preferentially targeting the eye region. These results reveal a direct role of the amygdala in reflexive gaze initiation toward fearfully widened eyes. They mirror deficits observed in patients with amygdala lesions and open a window for future studies on patients with autism spectrum disorder, in which deficits in emotion recognition, probably related to atypical gaze patterns and abnormal amygdala activation, have been observed.",
        "date": "August 2009",
        "authors": [
            "Matthias Gamer",
            "Christian B\u00fcchel"
        ],
        "references": [
            233085509,
            228079340,
            51408330,
            313572164,
            291823162,
            285854995,
            273037919,
            245970871,
            245633047,
            51429023
        ]
    },
    {
        "id": 12086759,
        "title": "Davis M, Whalen PJ. The amygdala: vigilance and emotion. Mol Psychiatr 6: 13-34",
        "abstract": "Here we provide a review of the animal and human literature concerning the role of the amygdala in fear conditioning, considering its potential influence over autonomic and hormonal changes, motor behavior and attentional processes. A stimulus that predicts an aversive outcome will change neural transmission in the amygdala to produce the somatic, autonomic and endocrine signs of fear, as well as increased attention to that stimulus. It is now clear that the amygdala is also involved in learning about positively valenced stimuli as well as spatial and motor learning and this review strives to integrate this additional information. A review of available studies examining the human amygdala covers both lesion and electrical stimulation studies as well as the most recent functional neuroimaging studies. Where appropriate, we attempt to integrate basic information on normal amygdala function with our current understanding of psychiatric disorders, including pathological anxiety.",
        "date": "February 2001",
        "authors": [
            "Marilyn Owens Davis",
            "Paul J Whalen"
        ],
        "references": [
            324119636,
            327748123,
            324125727,
            316238643,
            313716093,
            313050690,
            312955041,
            302205871,
            299068448,
            297867994
        ]
    },
    {
        "id": 11220159,
        "title": "Is the Human Amygdala Critical for the Subjective Experience of Emotion? Evidence of Intact Dispositional Affect in Patients with Amygdala Lesions",
        "abstract": "It is thought that the human amygdala is a critical component of the neural substrates of emotional experience, involved particularly in the generation of fear, anxiety, and general negative affectivity. Although many neuroimaging studies demonstrate findings consistent this notion, little evidence of altered emotional experience following amygdala damage has been gathered in humans. In a preliminary test of the amygdala's role in phenomenal affective states, we assessed the extent of experienced positive and negative affective states in patients with amygdala damage and age-, sex-, and education-matched controls. To assess chronic changes in experienced affect, all groups were administered the Positive and Negative Affect Schedules (PANAS, Watson, Clark, & Tellegen, 1988). In the first study, we examined the effects of amygdala lesions on affective traits in 10 left and 10 right amygdala-damaged patients, 1 patient with bilateral amygdala damage (SP), and 20 control subjects. Subjects were asked to indicate the typicality of different experiential states of positive (e.g., inspired, excited) and negative (e.g., afraid, nervous) valence. In a second study, we examined more closely the effects of bilateral amygdala damage on the day-to-day generation of affective states by administering the PANAS daily for a 30-day period to patient SP and age-, sex-, and education-matched controls. In both experiments, no differences in the magnitude and frequency of self-reported positive or negative affect were found between control subjects and patients with amygdala damage. Moreover, principal components analyses of the covariation among different affects (across individuals in Study 1 and within individuals across days in Study 2) confirmed a two-factor (positive vs. negative) description of experienced affect in controls. A highly similar two-factor description of experienced affect was found in patients with amygdala lesions. This suggests that the underlying structure of affective states was intact following amygdala damage. It is concluded that the human amygdala may be recruited during phenomenal affective states in the intact brain, but is not necessary for the production of these states.",
        "date": "July 2002",
        "authors": [
            "Adam Anderson",
            "Elizabeth A Phelps"
        ],
        "references": [
            276951115,
            324125727,
            321198741,
            316238643,
            297267240,
            286210298,
            284761023,
            283997013,
            279613676,
            275396304
        ]
    },
    {
        "id": 8126364,
        "title": "Human Amygdala Responsivity to Masked Fearful Eye Whites",
        "abstract": "The amygdala was more responsive to fearful (larger) eye whites than to happy (smaller) eye whites presented in a masking paradigm that mitigated subjects' awareness of their presence and aberrant nature. These data demonstrate that the amygdala is responsive to elements of.",
        "date": "January 2005",
        "authors": [
            "Paul J Whalen",
            "Jerome Kagan",
            "Robert George Cook",
            "F. Caroline Davis"
        ],
        "references": [
            275396304,
            254222274,
            231513975,
            35983278,
            13816769,
            12491530,
            11765842,
            10991079,
            10923541,
            10764882
        ]
    },
    {
        "id": 8096655,
        "title": "A mechanism for impaired fear recognition after amygdala damage",
        "abstract": "Ten years ago, we reported that SM, a patient with rare bilateral amygdala damage, showed an intriguing impairment in her ability to recognize fear from facial expressions. Since then, the importance of the amygdala in processing information about facial emotions has been borne out by a number of lesion and functional imaging studies. Yet the mechanism by which amygdala damage compromises fear recognition has not been identified. Returning to patient SM, we now show that her impairment stems from an inability to make normal use of information from the eye region of faces when judging emotions, a defect we trace to a lack of spontaneous fixations on the eyes during free viewing of faces. Although SM fails to look normally at the eye region in all facial expressions, her selective impairment in recognizing fear is explained by the fact that the eyes are the most important feature for identifying this emotion. Notably, SM's recognition of fearful faces became entirely normal when she was instructed explicitly to look at the eyes. This finding provides a mechanism to explain the amygdala's role in fear recognition, and points to new approaches for the possible rehabilitation of patients with defective emotion perception.",
        "date": "February 2005",
        "authors": [
            "Ralph Adolphs",
            "Fr\u00e9d\u00e9ric Gosselin",
            "Tony W Buchanan",
            "Daniel Tranel"
        ],
        "references": [
            317285424,
            317333757,
            297003381,
            292410175,
            291823162,
            284761023,
            278028780,
            275509173,
            275396304,
            275289705
        ]
    },
    {
        "id": 298869676,
        "title": "From interpretation to identification: a history of facial images in the sciences of emotion",
        "abstract": "Although images of faces have long been employed in the scientific study of emotion, the objectives and assumptions motivating their use have shifted according to the various fields and research programs within which they have been put to use. This article traces these shifts through three such fields-the social psychology of interwar America, cross-cultural research of the 1970s, and the contemporary neurosciences of emotion-in order to assess the recent use of facial images as a means of correlating particular emotions with particular locations in the brain.",
        "date": "February 2004",
        "authors": [
            "JM Watson"
        ],
        "references": [
            234837419,
            19795647,
            12942607,
            12276478,
            2523285,
            246157715,
            232494474,
            31978943,
            17411831,
            14995841
        ]
    },
    {
        "id": 308026381,
        "title": "Generative Visual Manipulation on the Natural Image Manifold",
        "abstract": "Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to \"fall off\" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.",
        "date": "August 2016",
        "authors": [
            "Jun-Yan Zhu",
            "Philipp Kr\u00e4henb\u00fchl",
            "Eli Shechtman",
            "Alexei Efros"
        ],
        "references": [
            301836893,
            319770820,
            319770416,
            319770355,
            319770269,
            319770144,
            313556846,
            308825079,
            308278061,
            301847900
        ]
    },
    {
        "id": 310673366,
        "title": "A Causal Framework for Discovering and Removing Direct and Indirect Discrimination",
        "abstract": "Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data is used for predictive analysis (e.g., building classifiers). We make use of the causal network to capture the causal structure of the data. Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish the two types of discrimination as the causal effects transmitted along different paths in the network. Based on that, we propose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Different from previous works, our approaches can ensure that the predictive models built from the modified data will not incur discrimination in decision making. Experiments using real datasets show the effectiveness of our approaches.",
        "date": "August 2017",
        "authors": [
            "Lu Zhang",
            "Yongkai Wu",
            "Xintao Wu"
        ],
        "references": [
            306228544,
            282567875,
            315398633,
            303912795,
            301872656,
            275697437,
            272825857,
            271947918,
            269416801,
            261803693
        ]
    },
    {
        "id": 280243276,
        "title": "Fairness Constraints: A Mechanism for Fair Classification",
        "abstract": "Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead to user discrimination, even in the absence of intent. In this paper, we introduce fairness constraints, a mechanism to ensure fairness in a wide variety of classifiers in a principled manner. Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data. We then instantiate fairness constraints on three well-known classifiers -- logistic regression, hinge loss and support vector machines (SVM) -- and evaluate their performance in a real-world dataset with meaningful sensitive human attributes. Experiments show that fairness constraints allow for an optimal trade-off between accuracy and fairness.",
        "date": "July 2015",
        "authors": [
            "Muhammad Bilal Zafar",
            "Isabel Valera",
            "Manuel Gomez Rodriguez",
            "Krishna P. Gummadi"
        ],
        "references": [
            289557353,
            262176212,
            261637367,
            258517824,
            254860037,
            224440330,
            221654695,
            220906796,
            347909277,
            261040912
        ]
    },
    {
        "id": 263012109,
        "title": "Generative Adversarial Networks",
        "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
        "date": "June 2014",
        "authors": [
            "Ian Goodfellow",
            "Jean Pouget-Abadie",
            "Mehdi Mirza",
            "Bing Xu"
        ],
        "references": [
            319770387,
            319770229,
            319770134,
            306218037,
            286620929,
            272195054,
            271554057,
            267960550,
            265748773,
            265178583
        ]
    },
    {
        "id": 228975972,
        "title": "Data Pre-Processing Techniques for Classification without Discrimination",
        "abstract": "Recently the Discrimination-Aware Classification Problem has been proposed: given a situation in which our training data contains (e.g., gender or racial) discrimination, learn a classifier that optimizes accuracy, but does not discriminate in its predictions on the test data. Such situations occur naturally as artifacts of the data collection process when the training data is collected from different sources with different labeling criteria, when the data is generated by a biased decision process, or when the sensitive attribute serves as a proxy for unobserved features. In many situations, a classifier that detects and uses the racial or gender discrimination is undesirable for legal reasons. In this paper we survey and extend our existing data pre-processing techniques for removing discrimination from the input dataset after which the standard classifier inducers can be used. We propose three pre-processing techniques which are empirically validated, showing good performance on real-life census data.",
        "date": "October 2011",
        "authors": [
            "Faisal Kamiran",
            "Toon Calders"
        ],
        "references": [
            228343218,
            224440330,
            223713209,
            221654695,
            221539096,
            221213498,
            254844563,
            242744181,
            225509786,
            221654559
        ]
    },
    {
        "id": 224440330,
        "title": "Classifying without discriminating",
        "abstract": "Classification models usually make predictions on the basis of training data. If the training data is biased towards certain groups or classes of objects, e.g., there is racial discrimination towards black people, the learned model will also show discriminatory behavior towards that particular community. This partial attitude of the learned model may lead to biased outcomes when labeling future unlabeled data objects. Often, however, impartial classification results are desired or even required by law for future data objects in spite of having biased training data. In this paper, we tackle this problem by introducing a new classification scheme for learning unbiased models on biased training data. Our method is based on massaging the dataset by making the least intrusive modifications which lead to an unbiased dataset. On this modified dataset we then learn a non-discriminating classifier. The proposed method has been implemented and experimental results on a credit approval dataset show promising results: in all experiments our method is able to reduce the prejudicial behavior for future classification significantly without loosing too much predictive accuracy.",
        "date": "March 2009",
        "authors": [
            "Faisal Kamiran",
            "Toon Calders"
        ],
        "references": [
            221654695,
            215990518,
            2935110,
            248848344,
            248192679,
            244487092,
            221025657,
            2806775
        ]
    },
    {
        "id": 220766841,
        "title": "Handling Conditional Discrimination",
        "abstract": "Historical data used for supervised learning may contain discrimination. We study how to train classifiers on such data, so that they are discrimination free with respect to a given sensitive attribute, e.g., gender. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes, such as, e.g., education level. In this context, we introduce and analyze the issue of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and hence tolerable. We observe that in such cases, the existing discrimination aware techniques will introduce a reverse discrimination, which is undesirable as well. Therefore, we develop local techniques for handling conditional discrimination when one of the attributes is considered to be explanatory. Experimental evaluation demonstrates that the new local techniques remove exactly the bad discrimination, allowing differences in decisions as long as they are explainable.",
        "date": "December 2011",
        "authors": [
            "Indre Zliobaite",
            "Faisal Kamiran",
            "Toon Calders"
        ],
        "references": [
            254860037,
            221654695,
            220765322,
            220764898,
            220451718,
            220345100,
            266598568,
            228213300,
            227458509,
            200033852
        ]
    },
    {
        "id": 334844029,
        "title": "Achieving Causal Fairness through Generative Adversarial Networks",
        "abstract": "Achieving fairness in learning models is currently an imperative task in machine learning. Meanwhile, recent research showed that fairness should be studied from the causal perspective, and proposed a number of fairness criteria based on Pearl's causal modeling framework. In this paper, we investigate the problem of building causal fairness-aware generative adversarial networks (CFGAN), which can learn a close distribution from a given dataset, while also ensuring various causal fairness criteria based on a given causal graph. CFGAN adopts two generators, whose structures are purposefully designed to reflect the structures of causal graph and interventional graph. Therefore, the two generators can respectively simulate the underlying causal model that generates the real data, as well as the causal model after the intervention. On the other hand, two discriminators are used for producing a close-to-real distribution, as well as for achieving various fairness criteria based on causal quantities simulated by generators. Experiments on a real-world dataset show that CFGAN can generate high quality fair data.",
        "date": "August 2019",
        "authors": [
            "Depeng Xu",
            "Yongkai Wu",
            "Shuhan Yuan",
            "Lu Zhang"
        ],
        "references": []
    },
    {
        "id": 335678842,
        "title": "On-Line Adaptative Curriculum Learning for GANs",
        "abstract": "Generative Adversarial Networks (GANs) can successfully approximate a probability distribution and produce realistic samples. However, open questions such as sufficient convergence conditions and mode collapse still persist. In this paper, we build on existing work in the area by proposing a novel framework for training the generator against an ensemble of discriminator networks, which can be seen as a one-student/multiple-teachers setting. We formalize this problem within the full-information adversarial bandit framework, where we evaluate the capability of an algorithm to select mixtures of discriminators for providing the generator with feedback during learning. To this end, we propose a reward function which reflects the progress made by the generator and dynamically update the mixture weights allocated to each discriminator. We also draw connections between our algorithm and stochastic optimization methods and then show that existing approaches using multiple discriminators in literature can be recovered from our framework. We argue that less expressive discriminators are smoother and have a general coarse grained view of the modes map, which enforces the generator to cover a wide portion of the data distribution support. On the other hand, highly expressive discriminators ensure samples quality. Finally, experimental results show that our approach improves samples quality and diversity over existing baselines by effectively learning a curriculum. These results also support the claim that weaker discriminators have higher entropy improving modes coverage.",
        "date": "July 2019",
        "authors": [
            "Thang Doan",
            "Jo\u00e3o Batista Monteiro Filho",
            "Isabela Maria Carneiro de Albuquerque",
            "Bogdan Mazoure"
        ],
        "references": [
            322498528,
            321796129,
            321768081,
            317062226,
            316184685,
            320707565,
            320609139,
            318122053,
            317189075,
            317062285
        ]
    },
    {
        "id": 323217470,
        "title": "cGANs with Projection Discriminator",
        "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.",
        "date": "February 2018",
        "authors": [
            "Takeru Miyato",
            "Masanori Koyama"
        ],
        "references": [
            322060135,
            321796129,
            311586074,
            311299462,
            319770355,
            318700099,
            315095500,
            309572857,
            309424680,
            308277201
        ]
    },
    {
        "id": 321796129,
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\u00e9chet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark. https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium",
        "date": "December 2017",
        "authors": [
            "Martin Heusel",
            "Hubert Ramsauer",
            "Thomas Unterthiner",
            "Bernhard Nessler"
        ],
        "references": [
            320966887,
            319769813,
            317558516,
            325889978,
            325421873,
            320968363,
            319770355,
            319770183,
            319770144,
            318413336
        ]
    },
    {
        "id": 318830253,
        "title": "Demystifying Neural Style Transfer",
        "abstract": "Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.",
        "date": "August 2017",
        "authors": [
            "Yanghao Li",
            "Naiyan Wang",
            "Jiaying Liu",
            "Xiaodi Hou"
        ],
        "references": [
            308152499,
            306012616,
            301839835,
            311610841,
            308278061,
            307090671,
            305218341,
            274480981,
            274404070,
            265385906
        ]
    },
    {
        "id": 317930102,
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\\'{e}chet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
        "date": "June 2017",
        "authors": [
            "Martin Heusel",
            "Hubert Ramsauer",
            "Thomas Unterthiner",
            "Bernhard Nessler"
        ],
        "references": [
            319769813,
            318029081,
            317558516,
            325889978,
            325421873,
            319770355,
            319770183,
            319770144,
            318413336,
            317578245
        ]
    },
    {
        "id": 317356551,
        "title": "On the Emergence of Invariance and Disentangling in Deep Representations",
        "abstract": "Using classical notions of statistical decision and information theory, we show that invariance in a deep neural network is equivalent to minimality of the representation it computes, and can be achieved by stacking layers and injecting noise in the computation, under realistic and empirically validated assumptions. We use an Information Decomposition of the empirical loss to show that overfitting can be reduced by limiting the information content stored in the weights. We then present a sharp inequality that relates the information content in the weights -- which are a representation of the training set and inferred by generic optimization agnostic of invariance and disentanglement -- and the minimality and total correlation of the activation functions, which are a representation of the test datum. This allows us to tackle recent puzzles concerning the generalization properties of deep networks and their relation to the geometry of the optimization residual.",
        "date": "June 2017",
        "authors": [
            "Alessandro Achille",
            "Stefano Soatto"
        ],
        "references": [
            309729924,
            319770355,
            319770229,
            315096447,
            313910493,
            311609041,
            311486035,
            311299697,
            310122390,
            309738713
        ]
    },
    {
        "id": 317300265,
        "title": "Megapixel Size Image Creation using Generative Adversarial Networks",
        "abstract": "Since its appearance, Generative Adversarial Networks (GANs) have received a lot of interest in the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far did not look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional Generative Adversarial Networks (DCGANs), in order to create photorealistic high-resolution images (up to 1024x1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects.",
        "date": "May 2017",
        "authors": [
            "Marco Marchesi"
        ],
        "references": [
            315710637,
            315748793,
            308027538,
            284476553
        ]
    },
    {
        "id": 317087692,
        "title": "Universal Style Transfer via Feature Transforms",
        "abstract": "Universal style transfer aims to transfer any arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.",
        "date": "May 2017",
        "authors": [
            "Yijun Li",
            "Chen Fang",
            "Jimei Yang",
            "Zhaowen Wang"
        ],
        "references": [
            316643049,
            315666513,
            315514619,
            314256222,
            313910272,
            312170783,
            315457372,
            313157340,
            311612367,
            311610841
        ]
    },
    {
        "id": 305779418,
        "title": "Accelerating the Super-Resolution Convolutional Neural Network",
        "abstract": "As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.",
        "date": "August 2016",
        "authors": [
            "Chao Dong",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "references": [
            308277687,
            305401666,
            319770367,
            319770269,
            319770168,
            310752533,
            308852884,
            303505510,
            301921832,
            289386894
        ]
    },
    {
        "id": 305654682,
        "title": "Semantic Image Inpainting with Perceptual and Contextual Losses",
        "abstract": "In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image. Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space. The mapped vector is then passed through the generative model to predict the missing content. The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80% corruption and large blocky corruption. Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods.",
        "date": "July 2016",
        "authors": [
            "Raymond Yeh",
            "Chen Chen",
            "Teck Yian Lim",
            "Mark Hasegawa-Johnson"
        ],
        "references": [
            313910272,
            320968363,
            319770355,
            319770144,
            312771295,
            311611060,
            311610841,
            311610454,
            311609301,
            308278825
        ]
    },
    {
        "id": 301836893,
        "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results .",
        "date": "March 2016",
        "authors": [
            "Justin Johnson",
            "Alexandre Alahi",
            "Fei-Fei Li"
        ],
        "references": [
            313910272,
            319770411,
            319770198,
            319770168,
            319770151,
            319770144,
            312771295,
            311609301,
            308852884,
            308820111
        ]
    },
    {
        "id": 286301796,
        "title": "Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images",
        "abstract": "Image representations, from SIFT and bag of visual words to Convolutional Neural Networks (CNNs), are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of \"natural pre-image\", namely a naturally-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose this as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.",
        "date": "December 2016",
        "authors": [
            "Aravindh Mahendran",
            "Andrea Vedaldi"
        ],
        "references": [
            307560165,
            283725380,
            281312423,
            279839496,
            319770820,
            319770183,
            319770151,
            310752533,
            307822569,
            281327886
        ]
    },
    {
        "id": 284219455,
        "title": "Super-Resolution with Deep Convolutional Sufficient Statistics",
        "abstract": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",
        "date": "November 2015",
        "authors": [
            "Joan Bruna",
            "Pablo Sprechmann",
            "Yann Lecun"
        ],
        "references": [
            277334140,
            270454670,
            319770355,
            319770229,
            319770198,
            281886334,
            278733352,
            277023018,
            273471664,
            272422583
        ]
    },
    {
        "id": 284097296,
        "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.",
        "date": "November 2015",
        "authors": [
            "Jiwon Kim",
            "Jung Kwon Lee",
            "Kyoung Mu Lee"
        ],
        "references": [
            275523282,
            319770183,
            319770168,
            308852884,
            308846031,
            304417592,
            301921832,
            288640908,
            280940018,
            274645044
        ]
    },
    {
        "id": 279068412,
        "title": "Understanding Neural Networks Through Deep Visualization",
        "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",
        "date": "June 2015",
        "authors": [
            "Jason Yosinski",
            "Jeff Clune",
            "Anh Nguyen",
            "Thomas Fuchs"
        ],
        "references": [
            269935591,
            269935290,
            265022827,
            319770411,
            319770183,
            273471270,
            269722411,
            269040983,
            263564119,
            259399927
        ]
    },
    {
        "id": 319769907,
        "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking",
        "abstract": "We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.",
        "date": "October 2015",
        "authors": [
            "Hyeonseob Nam",
            "Bohyung Han"
        ],
        "references": [
            307747289,
            304604756,
            319770430,
            319770183,
            319770168,
            313550366,
            312371270,
            311610546,
            310752944,
            304417592
        ]
    },
    {
        "id": 311905092,
        "title": "Applying Detection Proposals to Visual Tracking for Scale and Aspect Ratio Adaptability",
        "abstract": "The newly proposed correlation filter based trackers can achieve appealing performance despite their great simplicity and superior speed. However, this kind of object trackers is not born with scale and aspect ratio adaptability, thus resulting in suboptimal tracking accuracy. To tackle this problem, this paper integrates the class-agnostic detection proposal method, which is widely adopted in object detection area, into a correlation filter tracker. In the tracker part, optimizations such as feature integration, robust model updating and proposal rejection are applied for efficient integration. As for proposal generation, through integrating and comparing four detection proposal generators along with two baseline methods, the quality of detection proposals is found to have considerable influence on tracking accuracy. Therefore, as the most promising proposal generator, EdgeBoxes is chosen and further enhanced with background suppression. Evaluations are mainly performed on a challenging 50-sequence dataset (OTB50) and its two subsets, 28 sequences with significant scale variation and 14 sequences with obvious aspect ratio change. Among the trackers equipped with different proposal generators, state-of-the-art trackers and existing correlation filter variants, our proposed tracker reports the highest accuracy while running efficiently at an average speed of 20.4 frames per second. Additionally, numerical performance analysis in per-sequence manner and experiment results on VOT2014 dataset are also presented to enable deeper insights into our approach.",
        "date": "May 2017",
        "authors": [
            "Dafei Huang",
            "Lei Luo",
            "Zhaoyun Chen",
            "Mei Wen"
        ],
        "references": [
            319770820,
            319770430,
            319770284,
            319770283,
            319769911,
            313550366,
            311753760,
            311610694,
            304554999,
            304408491
        ]
    },
    {
        "id": 319277106,
        "title": "RATM: Recurrent Attentive Tracking Model",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Samira Ebrahimi Kahou",
            "Vincent Michalski",
            "Roland Memisevic",
            "Christopher Pal"
        ],
        "references": [
            319769907,
            309640954,
            305006910,
            304642478,
            302569301,
            273327857,
            273279481,
            272423336,
            272194766,
            269933088
        ]
    },
    {
        "id": 313550366,
        "title": "Accurate scale estimation for robust visual tracking",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Martin Danelljan",
            "G. H\u00e4ger",
            "Fahad Khan"
        ],
        "references": []
    },
    {
        "id": 313550197,
        "title": "Hierarchical convolutional features for visual tracking",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Chao Ma",
            "Jia-Bin Huang",
            "X.K. Yang",
            "Ming-Hsuan Yang"
        ],
        "references": []
    },
    {
        "id": 311611370,
        "title": "Efficient Deep Learning for Stereo Matching",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Wenjie Luo",
            "Alexander G. Schwing",
            "Raquel Urtasun"
        ],
        "references": [
            275588416,
            255564212,
            225069465,
            222941742,
            221363114,
            220320677,
            4351129,
            308868643,
            304604590,
            290109880
        ]
    },
    {
        "id": 311611188,
        "title": "Object Tracking via Dual Linear Structured SVM and Explicit Feature Map",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Jifeng Ning",
            "Jimei Yang",
            "Shaojie Jiang",
            "Lei Zhang"
        ],
        "references": [
            288624267,
            288001323,
            273279481,
            262329149,
            262234333,
            258924422,
            257672257,
            234786663,
            224135160,
            221345222
        ]
    },
    {
        "id": 311610678,
        "title": "Learning Multi-domain Convolutional Neural Networks for Visual Tracking",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Hyeonseob Nam",
            "Bohyung Han"
        ],
        "references": [
            319770160,
            307747289,
            304604756,
            301973397,
            288624267,
            288001323,
            273279481,
            272845640,
            265295439,
            262331177
        ]
    },
    {
        "id": 315765130,
        "title": "Snapshot Ensembles: Train 1, get M for free",
        "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.",
        "date": "March 2017",
        "authors": [
            "Gao Huang",
            "Yixuan Li",
            "Geoff Pleiss",
            "Zhuang Liu"
        ],
        "references": [
            301879329,
            319770411,
            319770239,
            319770226,
            319769909,
            308964680,
            308277201,
            306218037,
            306093962,
            303409493
        ]
    },
    {
        "id": 306186068,
        "title": "Residual Networks of Residual Networks: Multilevel Residual Networks",
        "abstract": "Residual networks family with hundreds or even thousands of layers dominate major image recognition tasks, but building a network by simply stacking residual blocks inevitably limits its optimization ability. This paper proposes a novel residual-network architecture, Residual networks of Residual networks (RoR), to dig the optimization ability of residual networks. RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping, in particular, adding level-wise shortcut connections upon original residual networks, to promote the learning capability of residual networks. More importantly, RoR can be applied to various kinds of residual networks (Pre-ResNets and WRN) and significantly boost their performance. Our experiments demonstrate the effectiveness and versatility of RoR, where it achieves the best performance in all residual-network-like structures. Our RoR-3-WRN58-4 models achieve new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59% respectively. These results outperform 1001-layer Pre-ResNets by 18.4% on CIFAR-10 and 13.1% on CIFAR-100.",
        "date": "August 2016",
        "authors": [
            "Ke Zhang",
            "Miao Sun",
            "Tony X. Han",
            "Xingfang Yuan"
        ],
        "references": [
            319769813,
            305196650,
            319770414,
            319770272,
            319770191,
            319770183,
            312256274,
            309019964,
            306218037,
            304017613
        ]
    },
    {
        "id": 319770239,
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
        "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.",
        "date": "January 2014",
        "authors": [
            "Yann N Dauphin",
            "Razvan Pascanu",
            "Caglar Gulcehre",
            "Kyunghyun Cho"
        ],
        "references": []
    },
    {
        "id": 313181074,
        "title": "Functional minimization by conjugate gradients",
        "abstract": "",
        "date": "January 1964",
        "authors": [
            "Ramona Fletcher",
            "C.M. Reeves"
        ],
        "references": []
    },
    {
        "id": 30770288,
        "title": "Neural Networks for Fingerprint Recognition",
        "abstract": "After collecting a data base of fingerprint images, we design a neural network algorithm for fingerprint recognition. When presented with a pair of fingerprint images, the algorithm outputs an estimate of the probability that the two images originate from the same finger. In one experiment, the neural network is trained using a few hundred pairs of images and its performance is subsequently tested using several thousand pairs of images originated from a subset of the database corresponding to 20 individuals. The error rate currently achieved is less than 0.5%. Additional results, extensions, and possible applications are also briefly discussed.",
        "date": "May 1993",
        "authors": [
            "Pierre Baldi",
            "Yves Chauvin"
        ],
        "references": [
            294767589,
            243782463,
            242479108,
            240446548
        ]
    },
    {
        "id": 248848344,
        "title": "Pattern Classification and Scene Analysis",
        "abstract": "",
        "date": "January 1973",
        "authors": [
            "R. O. Duda",
            "P. E Hart"
        ],
        "references": []
    },
    {
        "id": 243766097,
        "title": "A Time-Delay Neural Network Architecture for Speech Recognition",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "K. J. Lang",
            "G. E. Hinton"
        ],
        "references": []
    },
    {
        "id": 243742616,
        "title": "Dynamic approaches to handwritten signature veri cation",
        "abstract": "Dynamic signature verification consists of on-line signature acquisition and verification i.e., verification of the author's identity occurs during the signing process itself. The present paper reviews developments in dynamic signature verification methods and techniques. The various approaches are classified in two separated categories dealing with functions and parameters respectively. In both cases the data acquisition, preprocessing, feature extraction and comparison steps are analyzed and the problems of each are pointed out. The performance of existing systems is presented and some remaining problems are highlighted.",
        "date": "October 1990",
        "authors": [
            "GUY LORETTE",
            "R\u00e9jean Plamondon"
        ],
        "references": []
    },
    {
        "id": 224104314,
        "title": "Automatic Signature Verification Based on Accelerometry",
        "abstract": "The fine structure of the muscle forces that are exerted during the writing of a signature is constant and well defined for most people. In general, the fine structure is not subject to conscious control. Based on these observations, an experimental system has been designed that utilizes a person's signature dynamics to verify identities. The design and operational features of this system are described. Experiments on 70 subjects during a four-week period show a 2.9 percent rejection of valid signatures and a 2.1 percent acceptance of forgeries. An average of 1.2 trials was necessary for verification. The forgers were knowledgeable about the verification technique and did their best to deceive the system. The acceptance rate of random forgeries, i.e., accidental matching of two separate signatures, was 0.16 percent.",
        "date": "June 1977",
        "authors": [
            "Noel M. Herbst",
            "C. N. Liu"
        ],
        "references": [
            291866532,
            284953227,
            284743129,
            275809996,
            254859278,
            244498871,
            235027983,
            234564258,
            2994383
        ]
    },
    {
        "id": 220603073,
        "title": "Design of a neural network character recognizer for a touch terminal",
        "abstract": "We describe a system which can recognize digits and uppercase letters handprinted on a touch terminal. A character is input as a sequence of [x(t), y(t)] coordinates, subjected to very simple preprocessing, and then classified by a trainable neural network. The classifier is analogous to \u201ctime delay neural networks\u201d previously applied to speech recognition. The network was trained on a set of 12,000 digits and uppercase letters, from approximately 250 different writers, and tested on 2500 such characters from other writers. Classification accuracy exceeded 96% on the test examples.",
        "date": "December 1991",
        "authors": [
            "Isabelle Guyon",
            "P. Albrecht",
            "Yann Lecun",
            "John Denker"
        ],
        "references": [
            265896254,
            224663342,
            221619244,
            216792889,
            19685105,
            3175480,
            284578281,
            243766097,
            243743707,
            242530350
        ]
    },
    {
        "id": 220029087,
        "title": "On-line Signature Verification Incorporting the Direction of Pen Movement",
        "abstract": "This paper deals with an on-line signature verification system. It is assumed that the system requires a person approaching it to declare his name and to write his signature. The system compares the written signature with reference signatures registered in advance and admits his access if the dissimilarity is below a threshold. New ideas in this paper on the design of such a system are (1) to construct an effective dissimilarity measure by including the direction of pen movement, (2) to select a few representative signatures from the reference set by using a clustering procedure, and (3) to decide the threshold as the maximum of dissimilarity measures among reference signatures multiplied by a constant. The effectiveness of the designed by CADIX Co., Ltd. It consists of 2203 writings for 28 signatures which contain 10 in Latin letters (English alphabet) by foreigners, 14 in Japanese letters by Japanese, and 4 in Latin letters by Japanese. It comprises genuine signatures and forgeries. As a result of experiment, the system turned out to work highly satisfactory : it achieved about 99% of an average correct verification rate. The inclusion of the direction of pen movement into the dissimilarity measure leads to about 3% of an average increase in the correct verification rate.",
        "date": "July 1991",
        "authors": [
            "Mitsu Yoshimura",
            "Yutaka Kato",
            "Shin-ichi Matsuda",
            "Isao Yoshimura"
        ],
        "references": []
    },
    {
        "id": 216792884,
        "title": "Generalization and Network Design Strategies",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "Yann Lecun"
        ],
        "references": []
    },
    {
        "id": 216792871,
        "title": "A time delay neural network character recognizer for a touch terminal",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "Isabelle Guyon",
            "P. Albrecht",
            "Yann Lecun",
            "John Denker"
        ],
        "references": []
    },
    {
        "id": 216721999,
        "title": "Pattern Classification and Scene Analysis",
        "abstract": "",
        "date": "January 1973",
        "authors": [
            "Richard O. Duda",
            "Peter E. Hart"
        ],
        "references": []
    },
    {
        "id": 270283023,
        "title": "Signature Verification using a \"Siamese\" Time Delay Neural Network",
        "abstract": "This paper describes the development of an algorithm for verification of signatures written on a touch-sensitive pad. The signature verification algorithm is based on an artificial neural network. The novel network presented here, called a \u201cSiamese\u201d time delay neural network, consists of two identical networks joined at their output. During training the network learns to measure the similarity between pairs of signatures. When used for verification, only one half of the Siamese network is evaluated. The output of this half network is the feature vector for the input signature. Verification consists of comparing this feature vector with a stored feature vector for the signer. Signatures closer than a chosen threshold to this stored representation are accepted, all other signatures are rejected as forgeries. System performance is illustrated with experiments performed in the laboratory.",
        "date": "August 1993",
        "authors": [
            "Jane Bromley",
            "James W. Bentz",
            "Leon Bottou",
            "Isabelle Guyon"
        ],
        "references": []
    },
    {
        "id": 314733943,
        "title": "Multidimensional Scaling",
        "abstract": "",
        "date": "November 1996",
        "authors": [
            "Ian Jolliffe",
            "Trevor F. Cox",
            "Mike Cox"
        ],
        "references": []
    },
    {
        "id": 304827012,
        "title": "Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "Y. Bengio"
        ],
        "references": []
    },
    {
        "id": 285599593,
        "title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction",
        "abstract": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs\u201430,000 auditory nerve fibers or 106 optic nerve fibers\u2014a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.",
        "date": "January 2000",
        "authors": [
            "Joshua B. Tenenbaum",
            "Vin de Silva",
            "John C. Langford"
        ],
        "references": []
    },
    {
        "id": 284035345,
        "title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "S. T. Roweis",
            "L. K. Saul"
        ],
        "references": []
    },
    {
        "id": 279384963,
        "title": "Principal Component Analysis and Factor Analysis",
        "abstract": "Principal component analysis has often been dealt with in textbooks as a special case of factor analysis, and this tendency has been continued by many computer packages which treat PCA as one option in a program for factor analysis\u2014see Appendix A2. This view is misguided since PCA and factor analysis, as usually defined, are really quite distinct techniques. The confusion may have arisen, in part, because of Hotelling\u2019s (1933) original paper, in which principal components were introduced in the context of providing a small number of \u2018more fundamental\u2019 variables which determine the values of the p original variables. This is very much in the spirit of the factor model introduced in Section 7.1, although Girschick (1936) indicates that there were soon criticisms of Hotelling\u2019s method of PCs, as being inappropriate for factor analysis. Further confusion results from the fact that practitioners of \u2018factor analysis\u2019 do not always have the same definition of the technique (see Jackson, 1981). The definition adopted in this chapter is, however, fairly standard.",
        "date": "January 1986",
        "authors": [
            "Ian T. Jolliffe"
        ],
        "references": [
            287104765,
            253822570,
            248421874,
            232502628,
            221996178,
            52012631
        ]
    },
    {
        "id": 263696764,
        "title": "Principal Component Analysis. Springer-Verlag, 2nd Ed., New York",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "Ian T. Jolliffe"
        ],
        "references": []
    },
    {
        "id": 246365307,
        "title": "Hessian eigenmaps: new locally linear techniques for high-dimensional data",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "David Donoho",
            "Carrie Grimes"
        ],
        "references": []
    },
    {
        "id": 233530439,
        "title": "Pincipal Component Analysis",
        "abstract": "Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is",
        "date": "January 2002",
        "authors": [
            "Ian T. Jolliffe"
        ],
        "references": []
    },
    {
        "id": 232970799,
        "title": "Multidimensional scaling on a sphere",
        "abstract": "Nonmetric multidimensional scaling (MDS) is adapted to give configurations of points that lie on the surface of a sphere.There are data sets where it can be argued that spherical MDS is more relevant than the usual planar MDS.The theory behind the adaption of planar MDS to spherical MDS is outlined and then its use is illustrated on three data sets.",
        "date": "January 1991",
        "authors": [
            "Trevor F Cox",
            "Mike Cox"
        ],
        "references": [
            269520956,
            246528879,
            243764191,
            243047930,
            229580838,
            226269201,
            222944508,
            44543191,
            24062326,
            24061688
        ]
    },
    {
        "id": 220320709,
        "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
        "abstract": "We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the pa- rameters, and analyze the asymptotic vari- ance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parame- ter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized mod- els, including score matching, contrastive di- vergence, and maximum-likelihood where the normalization constant is estimated with im- portance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statis- tical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.",
        "date": "January 2010",
        "authors": [
            "Michael Gutmann",
            "Aapo Hyv\u00e4rinen"
        ],
        "references": [
            221460970,
            221078426,
            23975831,
            296924802,
            237067978,
            221620295,
            221079521,
            220695762,
            220320841,
            11207765
        ]
    },
    {
        "id": 317100995,
        "title": "Self-supervised learning of visual features through embedding images into text topic spaces",
        "abstract": "End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.",
        "date": "May 2017",
        "authors": [
            "Lluis Gomez",
            "Yash Patel",
            "Mar\u00e7al Rusi\u00f1ol",
            "Dimosthenis Karatzas"
        ],
        "references": [
            306885916,
            303816027,
            289288968,
            320967460,
            313346892,
            305492614,
            304409339,
            303606103,
            287175820,
            285630148
        ]
    },
    {
        "id": 316948359,
        "title": "Unsupervised Joint Mining of Deep Features and Image Labels for Large-Scale Radiology Image Categorization and Scene Recognition",
        "abstract": "",
        "date": "March 2017",
        "authors": [
            "Xiaosong Wang",
            "Le Lu",
            "Hoo-Chang Shin",
            "Lauren Kim"
        ],
        "references": [
            319770242,
            319770453,
            319770396,
            319770395,
            319770276,
            319770249,
            319770183,
            319770163,
            318395244,
            314089322
        ]
    },
    {
        "id": 307536552,
        "title": "CliqueCNN: Deep Unsupervised Exemplar Learning",
        "abstract": "Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.",
        "date": "August 2016",
        "authors": [
            "Miguel Angel Bautista",
            "Artsiom Sanakoyeu",
            "Ekaterina Sutter",
            "Bj\u00f6rn Ommer"
        ],
        "references": [
            319770430,
            314727145,
            308872819,
            308731813,
            307632169,
            304409339,
            304408738,
            290109966,
            289933059,
            288484843
        ]
    },
    {
        "id": 301876671,
        "title": "Joint Unsupervised Learning of Deep Representations and Image Clusters",
        "abstract": "In this paper, we propose a recurrent framework for joint unsupervised learning of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.",
        "date": "April 2016",
        "authors": [
            "Jianwei Yang",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "references": [
            306061750,
            319770820,
            319770453,
            319770430,
            319770395,
            319770183,
            319769911,
            312538118,
            305397619,
            304409339
        ]
    },
    {
        "id": 275974132,
        "title": "Learning to See by Moving",
        "abstract": "The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.",
        "date": "May 2015",
        "authors": [
            "Pulkit Agrawal",
            "J. Carreira",
            "Jitendra Malik"
        ],
        "references": [
            319770352,
            274902700,
            269935600,
            319770430,
            319770183,
            292740784,
            286594459,
            272837232,
            269407900,
            268988398
        ]
    },
    {
        "id": 324104512,
        "title": "Mining on Manifolds: Metric Learning Without Labels",
        "abstract": "In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss. The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.",
        "date": "June 2018",
        "authors": [
            "Ahmet Iscen",
            "Giorgos Tolias",
            "Yannis Avrithis",
            "Ondrej Chum"
        ],
        "references": [
            322059505,
            321124875,
            322058914,
            320971333,
            320971290,
            319770261,
            319770255,
            319770183,
            319327462,
            318814222
        ]
    },
    {
        "id": 322160369,
        "title": "Significance of Softmax-Based Features in Comparison to Distance Metric Learning-Based Features",
        "abstract": "The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmax-based network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmax-based features perform competitive, or even better, to the state-of-the-art DML features when the size of the dataset, that is, the number of training samples per class, is large. The results suggest that softmax-based features should be properly taken into account when evaluating the performance of deep features.",
        "date": "December 2017",
        "authors": [
            "Shota Horiguchi",
            "Daiki Ikami",
            "Kiyoharu Aizawa"
        ],
        "references": [
            284162910,
            282467248,
            270283023,
            264979485,
            261100864,
            308868551,
            304552788,
            282414800,
            265385906,
            263564119
        ]
    },
    {
        "id": 322059505,
        "title": "Smart Mining for Deep Metric Learning",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Ben Harwood",
            "Vijay Kumar B G",
            "Gustavo Carneiro",
            "Ian Reid"
        ],
        "references": [
            310461796,
            309663722,
            308818469,
            305196650,
            320971333,
            311610690,
            311609668,
            311609396,
            311609097,
            307632169
        ]
    },
    {
        "id": 321241844,
        "title": "AlignedReID: Surpassing Human-Level Performance in Person Re-Identification",
        "abstract": "In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.0% on Market1501 and 96.1% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.",
        "date": "November 2017",
        "authors": [
            "Xuan Zhang",
            "Hao Luo",
            "Xing Fan",
            "Weilai Xiang"
        ],
        "references": [
            322060624,
            320179809,
            322058470,
            320971101,
            320971054,
            320968559,
            320968382,
            320098449,
            319700298,
            318222242
        ]
    },
    {
        "id": 320075450,
        "title": "Dynamic Label Graph Matching for Unsupervised Video Re-Identification",
        "abstract": "Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.",
        "date": "September 2017",
        "authors": [
            "Mang Ye",
            "Andy J Ma",
            "Liang Zheng",
            "Jiawei Li"
        ],
        "references": [
            320543254,
            317241155,
            311610183,
            311610003,
            320967150,
            319770183,
            317559087,
            312751508,
            312457781,
            311610793
        ]
    },
    {
        "id": 315514719,
        "title": "In Defense of the Triplet Loss for Person Re-Identification",
        "abstract": "In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this, thanks to the notable publication of the Market-1501 and MARS datasets and several strong deep learning approaches. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms any other published method by a large margin.",
        "date": "March 2017",
        "authors": [
            "Alexander Hermans",
            "Lucas Beyer",
            "Bastian Leibe"
        ],
        "references": [
            313097662,
            312945018,
            311611434,
            319770414,
            319770387,
            319770183,
            313879237,
            312727767,
            312283368,
            311610684
        ]
    },
    {
        "id": 305652274,
        "title": "Unsupervised Visual Representation Learning by Graph-Based Consistent Constraints",
        "abstract": "Learning rich visual representations often require training on datasets of millions of manually annotated examples. This substantially limits the scalability of learning effective representations as labeled data is expensive or scarce. In this paper, we address the problem of unsupervised visual representation learning from a large, unlabeled collection of images. By representing each image as a node and each nearest-neighbor matching pair as an edge, our key idea is to leverage graph-based analysis to discover positive and negative image pairs (i.e., pairs belonging to the same and different visual categories). Specifically, we propose to use a cycle consistency criterion for mining positive pairs and geodesic distance in the graph for hard negative mining. We show that the mined positive and negative image pairs can provide accurate supervisory signals for learning effective representations using Convo-lutional Neural Networks (CNNs). We demonstrate the effectiveness of the proposed unsupervised constraint mining method in two settings: (1) unsupervised feature learning and (2) semi-supervised learning. For unsupervised feature learning, we obtain competitive performance with several state-of-the-art approaches on the PASCAL VOC 2007 dataset. For semi-supervised learning, we show boosted performance by incorporating the mined constraints on three image classification datasets.",
        "date": "October 2016",
        "authors": [
            "Dong Li",
            "Wei-Chih Hung",
            "Jia-Bin Huang",
            "Shengjin Wang"
        ],
        "references": [
            308873484,
            325100183,
            319770183,
            319770168,
            319769903,
            317083951,
            314727145,
            308851327,
            308838479,
            308188776
        ]
    },
    {
        "id": 301817268,
        "title": "Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification",
        "abstract": "Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform state-of-the-art methods on multiple datasets by large margins.",
        "date": "April 2016",
        "authors": [
            "Tong Xiao",
            "Hongsheng Li",
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "references": [
            311610573,
            311609797,
            311609211,
            333160438,
            319770430,
            319770357,
            319770291,
            319770183,
            319770168,
            308822434
        ]
    },
    {
        "id": 316505674,
        "title": "SphereFace: Deep Hypersphere Embedding for Face Recognition",
        "abstract": "This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific $m$ to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.",
        "date": "April 2017",
        "authors": [
            "Weiyang Liu",
            "Yandong Wen",
            "Zhiding Yu",
            "Ming Li"
        ],
        "references": [
            305196650,
            319770183,
            312727767,
            311612843,
            311609513,
            311492579,
            308190438,
            307466861,
            304552788,
            301876276
        ]
    },
    {
        "id": 316437893,
        "title": "NormFace: L2 Hypersphere Embedding for Face Verification",
        "abstract": "Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2% to 0.4% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98%.",
        "date": "October 2017",
        "authors": [
            "Feng Wang",
            "Xiang Xiang",
            "Jian Cheng",
            "Alan Loddon Yuille"
        ],
        "references": [
            317590808,
            315683968,
            313893539,
            309484226,
            322060201,
            312231713,
            311609396,
            311609041,
            311118615,
            311066916
        ]
    },
    {
        "id": 314234312,
        "title": "Learning Image Representations Tied to Egomotion from Unlabeled Video",
        "abstract": "Understanding how images of objects and scenes behave in response to specific egomotions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose a new \u201cembodied\u201d visual learning paradigm, exploiting proprioceptive motor signals to train visual representations from egocentric video with no manual supervision. Specifically, we enforce that our learned features exhibit equivariance i.e., they respond predictably to transformations associated with distinct egomotions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.",
        "date": "December 2017",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "references": [
            311411454,
            319770820,
            319770493,
            319770357,
            319770183,
            314622073,
            312612590,
            312429903,
            311610409,
            308846657
        ]
    },
    {
        "id": 311106922,
        "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction",
        "abstract": "We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.",
        "date": "November 2016",
        "authors": [
            "Richard Zhang",
            "Phillip Isola",
            "Alexei Efros"
        ],
        "references": [
            320968627,
            319770420,
            319770355,
            319770183,
            319770168,
            314942807,
            314100361,
            311736860,
            311611060,
            311609902
        ]
    },
    {
        "id": 308277256,
        "title": "Instance-Sensitive Fully Convolutional Networks",
        "abstract": "Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.",
        "date": "October 2016",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Yi Li",
            "Shaoqing Ren"
        ],
        "references": [
            319770352,
            308833537,
            321616020,
            319770420,
            319770198,
            319770183,
            319769911,
            311609232,
            310752071,
            308859280
        ]
    },
    {
        "id": 301880946,
        "title": "Convolutional Pose Machines",
        "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
        "date": "January 2016",
        "authors": [
            "Shih-En Wei",
            "Varun Ramakrishna",
            "Takeo Kanade",
            "Yaser Sheikh"
        ],
        "references": [
            305196650,
            319770268,
            319770183,
            319770168,
            319769908,
            311610924,
            311610609,
            311609147,
            311609041,
            307719713
        ]
    },
    {
        "id": 272521784,
        "title": "What Makes for Effective Detection Proposals?",
        "abstract": "Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.",
        "date": "February 2015",
        "authors": [
            "Jan Hosang",
            "Rodrigo Benenson",
            "Piotr Doll\u00e1r",
            "Bernt Schiele"
        ],
        "references": [
            286404956,
            319770430,
            319770284,
            308584119,
            287085249,
            286943772,
            286758960,
            286594662,
            286594360,
            286594278
        ]
    },
    {
        "id": 320971842,
        "title": "Fully Convolutional Instance-Aware Semantic Segmentation",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Yi Li",
            "Haozhi Qi",
            "Jifeng Dai",
            "Xiangyang Ji"
        ],
        "references": [
            319769910,
            308278279,
            308277256,
            305196650,
            288714023,
            286513835,
            281670744,
            278413297,
            273388101,
            268689640
        ]
    },
    {
        "id": 320971305,
        "title": "Towards Accurate Multi-person Pose Estimation in the Wild",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "George Papandreou",
            "Tyler Zhu",
            "Nori Kanazawa",
            "Alexander Toshev"
        ],
        "references": [
            311223153,
            307897694,
            307438996,
            305196650,
            284476297,
            269332682,
            265787949,
            265295439,
            263048918,
            259478452
        ]
    },
    {
        "id": 320968627,
        "title": "Colorization as a Proxy Task for Visual Understanding",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Gustav Larsson",
            "Michael Maire",
            "Gregory Shakhnarovich"
        ],
        "references": [
            311106922,
            308295512,
            303993074,
            302305068,
            301877264,
            284579529,
            284476548,
            279839496,
            277023095,
            272423336
        ]
    },
    {
        "id": 319327462,
        "title": "Multi-task Self-Supervised Visual Learning",
        "abstract": "We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for \"harmonizing\" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.",
        "date": "August 2017",
        "authors": [
            "Carl Doersch",
            "Andrew Zisserman"
        ],
        "references": [
            310611045,
            309551169,
            306885916,
            304350732,
            303750237,
            277023095,
            275974132,
            268689640,
            266225209,
            263471626
        ]
    },
    {
        "id": 319235502,
        "title": "Representation Learning by Learning to Count",
        "abstract": "We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.",
        "date": "August 2017",
        "authors": [
            "Mehdi Noroozi",
            "Hamed Pirsiavash",
            "Paolo Favaro"
        ],
        "references": [
            308295512,
            308139028,
            301876679,
            284579529,
            277023095,
            275974132,
            273284676,
            268689412,
            264979485,
            263012109
        ]
    },
    {
        "id": 316235160,
        "title": "Unsupervised Learning by Predicting Noise",
        "abstract": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and Pascal VOC.",
        "date": "April 2017",
        "authors": [
            "Piotr Bojanowski",
            "Armand Joulin"
        ],
        "references": [
            284579529,
            277023095,
            275974132,
            263699257,
            263471626,
            263091774,
            261100864,
            257672261,
            234131319,
            228095615
        ]
    },
    {
        "id": 311736860,
        "title": "Learning Features by Watching Objects Move",
        "abstract": "This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.",
        "date": "December 2016",
        "authors": [
            "Deepak Pathak",
            "Ross Girshick",
            "Piotr Doll\u00e1r",
            "Trevor Darrell"
        ],
        "references": [
            306885916,
            303755744,
            301837491,
            301292064,
            277023095,
            275974132,
            268689640,
            268079628,
            259350442,
            240308775
        ]
    },
    {
        "id": 311611060,
        "title": "Context Encoders: Feature Learning by Inpainting",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Deepak Pathak",
            "Philipp Krahenbuhl",
            "Jeff Donahue",
            "Trevor Darrell"
        ],
        "references": [
            319770366,
            284579529,
            277023095,
            276211157,
            275974132,
            265295439,
            264979485,
            257882504,
            244418126,
            241770100
        ]
    },
    {
        "id": 311610615,
        "title": "Joint Unsupervised Learning of Deep Representations and Image Clusters",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Jianwei Yang",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "references": [
            306061750,
            286765883,
            277023095,
            265295439,
            264979485,
            263471626,
            259441043,
            256822571,
            231119170,
            230738508
        ]
    },
    {
        "id": 319770352,
        "title": "Analyzing the Performance of Multilayer Neural Networks for Object Recognition",
        "abstract": "In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.",
        "date": "July 2014",
        "authors": [
            "Pulkit Agrawal",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "references": [
            289917319,
            319770820,
            319770430,
            319770357,
            319770276,
            319770183,
            313060232,
            310752533,
            294688997,
            281327886
        ]
    },
    {
        "id": 284579529,
        "title": "Data-dependent Initializations of Convolutional Neural Networks",
        "abstract": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.",
        "date": "November 2015",
        "authors": [
            "Philipp Kr\u00e4henb\u00fchl",
            "Carl Doersch",
            "Jeff Donahue",
            "Trevor Darrell"
        ],
        "references": [
            305196650,
            277023095,
            275974132,
            269935401,
            304409339,
            275896969,
            272194743,
            272027131,
            269935079,
            269375158
        ]
    },
    {
        "id": 307822569,
        "title": "Understanding deep image representations by inverting them",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Aravindh Mahendran",
            "Andrea Vedaldi"
        ],
        "references": [
            279839496,
            277023437,
            265295439,
            265022827,
            264979485,
            260685753,
            259764958,
            259441043,
            259440613,
            258848619
        ]
    },
    {
        "id": 304409339,
        "title": "Unsupervised Learning of Visual Representations Using Videos",
        "abstract": "",
        "date": "December 2015",
        "authors": [
            "Xiaolong Wang",
            "Abhinav Gupta"
        ],
        "references": [
            319770366,
            308818469,
            303137904,
            281082007,
            277023095,
            274644994,
            272752419,
            272423336,
            268226700,
            264979485
        ]
    },
    {
        "id": 301816789,
        "title": "Context Encoders: Feature Learning by Inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
        "date": "April 2016",
        "authors": [
            "Deepak Pathak",
            "Philipp Krahenbuhl",
            "Jeff Donahue",
            "Trevor Darrell"
        ],
        "references": [
            319770366,
            277023095,
            275974132,
            264979485,
            263012109,
            257882504,
            244418126,
            241770100,
            221346269,
            220184392
        ]
    },
    {
        "id": 285599320,
        "title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "M Belkin",
            "P Niyogi"
        ],
        "references": []
    },
    {
        "id": 314727145,
        "title": "What makes Paris look like Paris?",
        "abstract": "",
        "date": "July 2012",
        "authors": [
            "Carl Doersch",
            "Saurabh Singh",
            "Abhinav Gupta",
            "Josef Sivic"
        ],
        "references": [
            228766475,
            220183844,
            220183492
        ]
    },
    {
        "id": 303606103,
        "title": "Discovering objects and their location in images",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "J. Sivic",
            "B.C. Russell",
            "A.A. Efros",
            "A. Zisserman"
        ],
        "references": [
            3940582,
            3906151,
            3854180,
            3077852,
            224744237,
            4022999
        ]
    },
    {
        "id": 300358778,
        "title": "Context as Supervisory Signal: Discovering Objects with Predictable Context",
        "abstract": "This paper addresses the well-established problem of unsupervised object discovery with a novel method inspired by weakly-supervised approaches. In particular, the ability of an object patch to predict the rest of the object (its context) is used as supervisory signal to help discover visually consistent object clusters. The main contributions of this work are: 1) framing unsupervised clustering as a leave-one-out context prediction task; 2) evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models; and 3) an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences. The proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches, and is shown to provide correspondences detailed enough to transfer keypoint annotations.",
        "date": "September 2014",
        "authors": [
            "Carl Doersch",
            "Abhinav Gupta",
            "Alexei Efros"
        ],
        "references": [
            261479531,
            241770100,
            51968606,
            14562158,
            303606103,
            289358818,
            286976996,
            267365469,
            262408098,
            262361420
        ]
    },
    {
        "id": 313619815,
        "title": "Unsupervised feature learning for RGB-D based object recognition",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "L. Bo",
            "X. Ren",
            "Dieter Fox"
        ],
        "references": []
    },
    {
        "id": 313561101,
        "title": "Tangent prop - A formalism for specifying selected invariances in an adaptive network",
        "abstract": "",
        "date": "January 1992",
        "authors": [
            "Patrice Y. Simard",
            "Bernard Victorri",
            "Yann Lecun",
            "John Denker"
        ],
        "references": []
    },
    {
        "id": 319035676,
        "title": "Recent Trends in Deep Learning Based Natural Language Processing",
        "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",
        "date": "August 2017",
        "authors": [
            "Tom Young",
            "Devamanyu Hazarika",
            "Soujanya Poria",
            "Erik Cambria"
        ],
        "references": [
            322583998,
            319770465,
            319770439,
            319770355,
            319770229,
            319770184,
            319770183,
            319769995,
            319769905,
            318828956
        ]
    },
    {
        "id": 311586074,
        "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks",
        "abstract": "Synthesizing photo-realistic images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose stacked Generative Adversarial Networks (StackGAN) to generate photo-realistic images conditioned on text descriptions. The Stage-I GAN sketches the primitive shape and basic colors of the object based on the given text description, yielding Stage-I low resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high resolution images with photo-realistic details. The Stage-II GAN is able to rectify defects and add compelling details with the refinement process. Samples generated by StackGAN are more plausible than those generated by existing approaches. Importantly, our StackGAN for the first time generates realistic 256 x 256 images conditioned on only text descriptions, while state-of-the-art methods can generate at most 128 x 128 images. To demonstrate the effectiveness of the proposed StackGAN, extensive experiments are conducted on CUB and Oxford-102 datasets, which contain enough object appearance variations and are widely-used for text-to-image generation analysis.",
        "date": "December 2016",
        "authors": [
            "Han Zhang",
            "Tao Xu",
            "Hongsheng Li",
            "Shaoting Zhang"
        ],
        "references": [
            309738475,
            309192180,
            326403446,
            320968363,
            319770355,
            319770269,
            319770134,
            311613031,
            311609041,
            308980732
        ]
    },
    {
        "id": 310953155,
        "title": "Texture Synthesis with Spatial Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.",
        "date": "November 2016",
        "authors": [
            "Nikolay Jetchev",
            "Urs Bergmann",
            "Roland Vollgraf"
        ],
        "references": [
            313910272,
            301842292,
            301836893,
            319770355,
            308278061,
            301838494,
            284476553,
            278733352,
            278048050,
            272194743
        ]
    },
    {
        "id": 303822179,
        "title": "Very Deep Convolutional Networks for Natural Language Processing",
        "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.",
        "date": "June 2016",
        "authors": [
            "Alexis Conneau",
            "Holger Schwenk",
            "Lo\u00efc Barrault",
            "Yann Lecun"
        ],
        "references": [
            281607724,
            274380447,
            266201822,
            319770465,
            319770414,
            301857163,
            289758666,
            286512696,
            272194743,
            272027131
        ]
    },
    {
        "id": 269723220,
        "title": "Audio-visual speech recognition using deep learning",
        "abstract": "Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for reliable speech recognition, particularly when the audio is corrupted by noise. However, cautious selection of sensory features is crucial for attaining high recognition performance. In the machine-learning community, deep learning approaches have recently attracted increasing attention because deep neural networks can effectively extract robust latent features that enable various recognition algorithms to demonstrate revolutionary generalization capabilities under diverse application conditions. This study introduces a connectionist-hidden Markov model (HMM) system for noise-robust AVSR. First, a deep denoising autoencoder is utilized for acquiring noise-robust audio features. By preparing the training data for the network with pairs of consecutive multiple steps of deteriorated audio features and the corresponding clean features, the network is trained to output denoised audio features from the corresponding features deteriorated by noise. Second, a convolutional neural network (CNN) is utilized to extract visual features from raw mouth area images. By preparing the training data for the CNN as pairs of raw images and the corresponding phoneme label outputs, the network is trained to predict phoneme labels from the corresponding mouth area input images. Finally, a multi-stream HMM (MSHMM) is applied for integrating the acquired audio and visual HMMs independently trained with the respective features. By comparing the cases when normal and denoised mel-frequency cepstral coefficients (MFCCs) are utilized as audio features to the HMM, our unimodal isolated word recognition results demonstrate that approximately 65 % word recognition rate gain is attained with denoised MFCCs under 10 dB signal-to-noise-ratio (SNR) for the audio signal input. Moreover, our multimodal isolated word recognition results utilizing MSHMM with denoised MFCCs and acquired visual features demonstrate that an additional word recognition rate gain is attained for the SNR conditions below 10 dB.",
        "date": "December 2014",
        "authors": [
            "Kuniaki Noda",
            "Yuki Yamaguchi",
            "Kazuhiro Nakadai",
            "Hiroshi G. Okuno"
        ],
        "references": [
            264859588,
            319770183,
            309076254,
            289250634,
            287762956,
            269295658,
            267960550,
            265178583,
            262163115,
            261308861
        ]
    },
    {
        "id": 222464584,
        "title": "Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network",
        "abstract": "A new approach to unsupervised learning in a single-layer linear feedforward neural network is discussed. An optimality principle is proposed which is based upon preserving maximal information in the output units. An algorithm for unsupervised learning based upon a Hebbian learning rule, which achieves the desired optimality is presented. The algorithm finds the eigenvectors of the input correlation matrix, and it is proven to converge with probability one. An implementation which can train neural networks using only local \u201csynaptic\u201d modification rules is described. It is shown that the algorithm is closely related to algorithms in statistics (Factor Analysis and Principal Components Analysis) and neural networks (Self-supervised Backpropagation, or the \u201cencoder\u201d problem). It thus provides an explanation of certain neural network behavior in terms of classical statistical techniques. Examples of the use of a linear network for solving image coding and texture segmentation problems are presented. Also, it is shown that the algorithm can be used to find \u201cvisual receptive fields\u201d which are qualitatively similar to those found in primate retina and visual cortex.",
        "date": "December 1989",
        "authors": [
            "Terence D Sanger"
        ],
        "references": [
            284192255,
            324110517,
            323464028,
            305381127,
            292812531,
            291320272,
            285828395,
            285058764,
            284291824,
            282260583
        ]
    },
    {
        "id": 325000039,
        "title": "Secure genome-wide association analysis using multiparty computation",
        "abstract": "Most sequenced genomes are currently stored in strict access-controlled repositories1,2,3. Free access to these data could improve the power of genome-wide association studies (GWAS) to identify disease-causing genetic variants and aid the discovery of new drug targets4,5. However, concerns over genetic data privacy6,7,8,9 may deter individuals from contributing their genomes to scientific studies10 and could prevent researchers from sharing data with the scientific community11. Although cryptographic techniques for secure data analysis exist12,13,14, none scales to computationally intensive analyses, such as GWAS. Here we describe a protocol for large-scale genome-wide analysis that facilitates quality control and population stratification correction in 9K, 13K, and 23K individuals while maintaining the confidentiality of underlying genotypes and phenotypes. We show the protocol could feasibly scale to a million individuals. This approach may help to make currently restricted data available to the scientific community and could potentially enable secure genome crowdsourcing, allowing individuals to contribute their genomes to a study without compromising their privacy.",
        "date": "July 2018",
        "authors": [
            "Hyunghoon Cho",
            "David J Wu",
            "Bonnie Berger"
        ],
        "references": [
            329819460,
            310611134,
            309655539,
            301879859,
            326561698,
            319168084,
            309582451,
            308604836,
            303843357,
            296478930
        ]
    },
    {
        "id": 321873477,
        "title": "Health Data in an Open World",
        "abstract": "With the aim of informing sound policy about data sharing and privacy, we describe successful re-identification of patients in an Australian de-identified open health dataset. As in prior studies of similar datasets, a few mundane facts often suffice to isolate an individual. Some people can be identified by name based on publicly available information. Decreasing the precision of the unit-record level data, or perturbing it statistically, makes re-identification gradually harder at a substantial cost to utility. We also examine the value of related datasets in improving the accuracy and confidence of re-identification. Our re-identifications were performed on a 10% sample dataset, but a related open Australian dataset allows us to infer with high confidence that some individuals in the sample have been correctly re-identified. Finally, we examine the combination of the open datasets with some commercial datasets that are known to exist but are not in our possession. We show that they would further increase the ease of re-identification.",
        "date": "December 2017",
        "authors": [
            "Chris Culnane",
            "Benjamin I. P. Rubinstein",
            "Vanessa Teague"
        ],
        "references": [
            298909822,
            271591449,
            262110951,
            313617833,
            309444608,
            304007758,
            284332229,
            265660320,
            262422180,
            262212418
        ]
    },
    {
        "id": 312571840,
        "title": "Privacy violations in Riga open data public transport system",
        "abstract": "Over the recent years public transportation systems around the world have been migrating to digital ticketing solutions. This paper investigates security and privacy aspects of the one such system implemented by Riga municipality called e-talons by analysing published open data containing ride registrations.",
        "date": "November 2016",
        "authors": [
            "Arturs Lavrenovs",
            "Karlis Podins"
        ],
        "references": [
            236076438,
            287031292,
            285827412,
            227979327,
            225451636,
            220779175,
            12260117,
            4339941
        ]
    },
    {
        "id": 298909822,
        "title": "Comment on \"Unique in the shopping mall: On the reidentifiability of credit card metadata\"",
        "abstract": "De Montjoye et al. (Reports, 30 January 2015, p. 536) claimed that most individuals can be reidentified from a deidentified transaction database and that anonymization mechanisms are not effective against reidentification. We demonstrate that anonymization can be performed by techniques well established in the literature.",
        "date": "March 2016",
        "authors": [
            "David S\u00e1nchez",
            "Sergio Mart\u00ednez",
            "Josep Domingo-Ferrer"
        ],
        "references": [
            284219157,
            283932499,
            277950165,
            271591449,
            236076438,
            284253680,
            280156259,
            274722963,
            254463868,
            239576354
        ]
    },
    {
        "id": 325250558,
        "title": "Data breach exposed",
        "abstract": "Files containing personality tests from millions of Facebook users were left accessible online",
        "date": "May 2018",
        "authors": [
            "Phee Waterfield",
            "Timothy Revell"
        ],
        "references": []
    },
    {
        "id": 313095392,
        "title": "Assessing identification risk in survey microdata using log-linear models",
        "abstract": "This article considers the assessment of the risk of identification of respondents in survey microdata, in the context of applications at the United Kingdom (UK) Office for National Statistics (ONS). The threat comes from the matching of categorical 'key' variables between microdata records and external data sources and from the use of log-linear models to facilitate matching. While the potential use of such statistical models is well-established in the literature, little consideration has been given to model specification nor to the sensitivity of risk assessment to this specification. In this article we develop new criteria for assessing the specification of a log-linear model in relation to the accuracy of risk estimates. We find that, within a class of 'reasonable' models, risk estimates tend to decrease as the complexity of the model increases. We develop criteria to detect 'underfitting' (associated with overestimation of the risk). The criteria may also reveal 'overfitting' (associated with underestimation) although not so clearly, so we suggest employing a forward model selection approach. We show how our approach may be used for both file-level and record-level measures of risk. We evaluate the proposed procedures using samples drawn from the 2001 UK Census where the true risks can be determined. We also apply our approach to a large survey dataset.",
        "date": "October 2006",
        "authors": [
            "C.J. Skinner",
            "Natalie Shlomo"
        ],
        "references": []
    },
    {
        "id": 311919575,
        "title": "Anonymizing NYC Taxi Data: Does It Matter?",
        "abstract": "",
        "date": "October 2016",
        "authors": [
            "Marie Douriez",
            "Harish Doraiswamy",
            "Juliana Freire",
            "Claudio Silva"
        ],
        "references": [
            233866087,
            221310386,
            220966272,
            220520096,
            220095241,
            45850932,
            4335160,
            4331040,
            312973479,
            304007758
        ]
    },
    {
        "id": 304535150,
        "title": "Big data: The management revolution",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "A. McAfee",
            "Erik Brynjolfsson",
            "Thomas H. Davenport",
            "D.J. Patil"
        ],
        "references": []
    },
    {
        "id": 290937815,
        "title": "Preparing raw clinical data for publication: guidance for journal editors, authors, and peer reviewers",
        "abstract": "Many peer reviewed journals now require authors to be prepared to share their raw, unprocessed data with other scientists or state the availability of raw data in published articles, but little information on how such data should be prepared for sharing has emerged. Iain Hrynaszkiewicz and colleagues propose a minimum standard for de-identifying datasets to ensure patient privacy when sharing clinical research data #### Summary points Many peer-reviewed journals\u2019 instructions for authors require that authors should be prepared to share their raw (that is, unprocessed) data with other scientists on request. Although data sharing is commonplace in some scientific disciplines and is a requirement of a number of major research funding agencies\u2019 policies, this culture has not yet been widely adopted by the clinical research community. Some journals have appealed to their authors to increase the availability of medical research data,1 2 3 recognising the benefits of such transparency. These benefits are well documented and include replication of previous findings, comparisons with independent datasets, testing of additional hypotheses, teaching, and patient safety.3 4 5 6 Moreover, patients themselves are increasingly seeing the benefits of openly sharing their experiences with others (www.patientslikeme.com/). Online journals with unlimited \u2026",
        "date": "January 2010",
        "authors": [
            "Iain Hrynaszkiewicz"
        ],
        "references": [
            250953448,
            24210791,
            14068618,
            7076373,
            6915749,
            232263311,
            6466350
        ]
    },
    {
        "id": 289999194,
        "title": "Customer data: Designing for transparency and trust",
        "abstract": "",
        "date": "May 2015",
        "authors": [
            "Timothy Morey",
            "Theodore Theo Forbath",
            "Allison Schoop"
        ],
        "references": []
    },
    {
        "id": 334166840,
        "title": "A Unified Framework of Five Principles for AI in Society",
        "abstract": "Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of \u2018principle proliferation\u2019 be solved? In this paper, we report the results of a fine-grained analysis of several of the highest profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes \u2018ethical AI.\u2019 Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question \u2018how does it work?\u2019) and in the ethical sense of accountability (as an answer to the question: \u2018who is responsible for the way it works?\u2019). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.",
        "date": "June 2019",
        "authors": [
            "Luciano Floridi",
            "Josh Cowls"
        ],
        "references": []
    },
    {
        "id": 332974675,
        "title": "Establishing the Rules for Building Trustworthy AI",
        "abstract": "The European Commission\u2019s report \u2018Ethics guidelines for trustworthy AI\u2019 provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",
        "date": "May 2019",
        "authors": [
            "Luciano Floridi"
        ],
        "references": [
            329192820,
            328292318,
            6043521,
            5998173
        ]
    },
    {
        "id": 329192820,
        "title": "AI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations",
        "abstract": "This article reports the findings of AI4People, an Atomium\u2014EISMD initiative designed to lay the foundations for a \u201cGood AI Society\u201d. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations\u2014to assess, to develop, to incentivise, and to support good AI\u2014which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.",
        "date": "December 2018",
        "authors": [
            "Luciano Floridi",
            "Josh Cowls",
            "Monica Beltrametti",
            "Raja Chatila"
        ],
        "references": [
            331099286,
            323248541,
            320166470,
            346642943,
            325750757
        ]
    },
    {
        "id": 328292318,
        "title": "Soft Ethics, The Governance of The Digital and The General Data Protection Regulation",
        "abstract": "The article discusses the governance of the digital as the new challenge posed by technological innovation. It then introduces a new distinction between soft ethics , which applies after legal compliance with legislation, such as the General Data Protection Regulation in the European Union, and hard ethics , which precedes and contributes to shape legislation. It concludes by developing an analysis of the role of digital ethics with respect to digital regulation and digital governance. This article is part of the theme issue \u2018Governing artificial intelligence: ethical, legal, and technical opportunities and challenges\u2019.",
        "date": "November 2018",
        "authors": [
            "Luciano Floridi"
        ],
        "references": [
            310393920,
            295540532,
            284709092,
            257226878,
            239543844
        ]
    },
    {
        "id": 315705213,
        "title": "Artificial Intelligence and the 'Good Society': the US, EU, and UK approach",
        "abstract": "In October 2016, the White House, the European Parliament, and the UK House of Commons each issued a report outlining their visions on how to prepare society for the widespread use of artificial intelligence (AI). In this article, we provide a comparative assessment of these three reports in order to facilitate the design of policies favourable to the development of a 'good AI society'. To do so, we examine how each report addresses the following three topics: (a) the development of a 'good AI society'; (b) the role and responsibility of the government, the private sector, and the research community (including academia) in pursuing such a development; and (c) where the recommendations to support such a development may be in need of improvement. Our analysis concludes that the reports address adequately various ethical, social, and economic topics, but come short of providing an overarching political vision and long-term strategy for the development of a 'good AI society'. In order to contribute to fill this gap, in the conclusion we suggest a two-pronged approach.",
        "date": "April 2018",
        "authors": [
            "Corinne Cath",
            "Sandra Wachter",
            "Brent Mittelstadt",
            "Mariarosaria Taddeo"
        ],
        "references": [
            346620854,
            312597416,
            310393920,
            310167745,
            309322060,
            317997499,
            315806782,
            312378887,
            311314597,
            310742382
        ]
    },
    {
        "id": 233806053,
        "title": "Distributed Morality in an Information Society",
        "abstract": "The phenomenon of distributed knowledge is well-known in epistemic logic. In this paper, a similar phenomenon in ethics, somewhat neglected so far, is investigated, namely distributed morality. The article explains the nature of distributed morality, as a feature of moral agency, and explores the implications of its occurrence in advanced information societies. In the course of the analysis, the concept of infraethics is introduced, in order to refer to the ensemble of moral enablers, which, although morally neutral per se, can significantly facilitate or hinder both positive and negative moral behaviours.",
        "date": "November 2012",
        "authors": [
            "Luciano Floridi"
        ],
        "references": [
            276185152,
            262281572,
            257802615,
            323667493,
            290463606,
            274148661,
            269824174,
            257931380,
            256068871,
            245345665
        ]
    },
    {
        "id": 228133505,
        "title": "The Drivers of Greenwashing",
        "abstract": "More and more firms are engaging in greenwashing, misleading consumers about firm environmental performance or the environmental benefits of a product or service. The skyrocketing incidence of greenwashing can have profound negative effects on consumer and investor confidence in environmentally friendly firms and products. Mitigating greenwashing is particularly challenging in a context of limited and uncertain regulation. This article examine the external (both institutional and market), organizational and individual drivers of greenwashing and offers recommendations for managers, policymakers, and NGOs to decrease its prevalence.",
        "date": "November 2011",
        "authors": [
            "Magali A. Delmas",
            "Vanessa Cuerel Burbano"
        ],
        "references": [
            274205197,
            272171124,
            325003999,
            325000300,
            324986529,
            277469286,
            275697703,
            274354344,
            268041466,
            266330592
        ]
    },
    {
        "id": 351401372,
        "title": "Faultless Responsibility: On the Nature and Allocation of Moral Responsibility for Distributed Moral Actions",
        "abstract": "The concept of distributed moral responsibility (DMR) has a long history. When it is understood as being entirely reducible to the sum of (some) human, individual and already morally loaded actions, then the allocation of DMR, and hence of praise and reward or blame and punishment, may be pragmatically difficult, but not conceptually problematic. However, in distributed environments, it is increasingly possible that a network of agents, some human, some artificial (e.g. a program) and some hybrid (e.g. a group of people working as a team thanks to a software platform), may cause distributed moral actions (DMAs). These are morally good or evil (i.e. morally loaded) actions caused by local interactions that are in themselves neither good nor evil (morally neutral). In this article, I analyse DMRs that are due to DMAs, and argue in favour of the allocation, by default and overridably, of full moral responsibility (faultless responsibility) to all the nodes/agents in the network causally relevant for bringing about the DMA in question, independently of intentionality. The mechanism proposed is inspired by, and adapts, three concepts: back propagation from network theory, strict liability from jurisprudence and common knowledge from epistemic logic.",
        "date": "January 2016",
        "authors": [
            "Luciano Floridi"
        ],
        "references": []
    },
    {
        "id": 332811345,
        "title": "Don\u2019t let industry write the rules for AI",
        "abstract": "Technology companies are running a campaign to bend research and regulation for their benefit; society must fight back, says Yochai Benkler. Technology companies are running a campaign to bend research and regulation for their benefit; society must fight back, says Yochai Benkler. \u201cInside an algorithmic black box, societal biases are rendered invisible and unaccountable.\u201d",
        "date": "May 2019",
        "authors": [
            "Yochai Benkler"
        ],
        "references": []
    },
    {
        "id": 326167496,
        "title": "Europe\u2019s biggest research fund cracks down on \u2018ethics dumping\u2019",
        "abstract": "The practice of conducting \ufeffethically dubious research in foreign countries is under fresh scrutiny. The practice of conducting \ufeffethically dubious research in foreign countries is under fresh scrutiny.",
        "date": "July 2018",
        "authors": [
            "Linda Nordling"
        ],
        "references": []
    },
    {
        "id": 319770328,
        "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours",
        "abstract": "Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.",
        "date": "September 2016",
        "authors": [
            "Lerrel Pinto",
            "Abhinav Gupta"
        ],
        "references": [
            319770233,
            282856821,
            264979485,
            258140407,
            257909554,
            256487932,
            254041261,
            239763436,
            221072191,
            221072021
        ]
    },
    {
        "id": 319770001,
        "title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition",
        "abstract": "Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.",
        "date": "November 2016",
        "authors": [
            "Jonathan Krause",
            "Benjamin Sapp",
            "Andrew Howard",
            "Howard Zhou"
        ],
        "references": [
            308872089,
            308845461,
            305196650,
            286384383,
            281902133,
            278048515,
            275669246,
            275279603,
            273327809,
            271659916
        ]
    },
    {
        "id": 312170738,
        "title": "Towards Accurate Multi-person Pose Estimation in the Wild",
        "abstract": "We propose a method for multi-person detection and 2-D keypoint localization (human pose estimation) that achieves state-of-the-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector with an Inception-ResNet architecture. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Our final system achieves average precision of 0.636 on the COCO test-dev set and the 0.628 test-standard sets, outperforming the CMU-Pose winner of the 2016 COCO keypoints challenge. Further, by using additional labeled data we obtain an even higher average precision of 0.668 on the test-dev set and 0.658 on the test-standard set, thus achieving a roughly 10% improvement over the previous best performing method on the same challenge.",
        "date": "January 2017",
        "authors": [
            "George Papandreou",
            "Tyler Zhu",
            "Nori Kanazawa",
            "Alexander Toshev"
        ],
        "references": [
            307897694,
            307438996,
            305196650,
            301880946,
            301839500,
            284476297,
            269332682,
            259478452,
            259212328,
            320971374
        ]
    },
    {
        "id": 310953055,
        "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
        "abstract": "We present a realtime approach for multi-person 2D pose estimation that predicts vector fields, which we refer to as Part Affinity Fields (PAFs), that directly expose the association between anatomical parts in an image. The architecture is designed to jointly learn part locations and their association, via two branches of the same sequential prediction process. The sequential prediction enables the part confidence maps and the association fields to encode global context, while allowing an efficient bottom-up parsing step that maintains tractable runtime complexity. Our method has set the state-of-the-art performance on the inaugural MSCOCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",
        "date": "November 2016",
        "authors": [
            "Zhe Cao",
            "Tomas Simon",
            "Shih-En Wei",
            "Yaser Sheikh"
        ],
        "references": [
            307897694,
            307438996,
            302826933,
            286662859,
            286513835,
            284476297,
            278048481,
            269332682,
            263048918,
            221364433
        ]
    },
    {
        "id": 309444714,
        "title": "Deep Classifiers from Image Tags in the Wild",
        "abstract": "This paper proposes direct learning of image classification from image tags in the wild, without filtering. Each wild tag is supplied by the user who shared the image online. Enormous numbers of these tags are freely available, and they give insight about the image categories important to users and to image classification. Our main contribution is an analysis of the Flickr 100 Million Image dataset, including several useful observations about the statistics of these tags. We introduce a large-scale robust classification algorithm, in order to handle the inherent noise in these tags, and a calibration procedure to better predict objective annotations. We show that freely available, wild tag can obtain similar or superior results to large databases of costly manual annotations.",
        "date": "October 2015",
        "authors": [
            "Hamid Izadinia",
            "Bryan C. Russell",
            "Ali Farhadi",
            "Matthew D. Hoffman"
        ],
        "references": [
            279839496,
            269935614,
            265295439,
            264979485,
            258520603,
            257871844,
            234119596,
            225741505,
            224135977,
            221515578
        ]
    },
    {
        "id": 308965018,
        "title": "Deep Learning with Separable Convolutions",
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the recently introduced \"separable convolution\" operation. In this light, a separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
        "date": "October 2016",
        "authors": [
            "Fran\u00e7ois Chollet"
        ],
        "references": [
            319769813,
            305196650,
            301839500,
            284579051,
            265787949,
            260642043,
            236736831,
            230867026,
            319770291,
            319770183
        ]
    },
    {
        "id": 312164958,
        "title": "Thinking Sociologically About Image-Based Sexual Abuse: The Contribution of Male Peer Support Theory",
        "abstract": "Often referred to by journalists, policy makers, and the general public as revenge porn, image-based sexual abuse is starting to garner serious legal and social scientific attention. However, theoretical developments have thus far not kept pace with the growing empirical and legal literature on this electronic variant of woman abuse. Further, this problem cannot adequately be explained by gender-blind theories, as there is a strong relationship between gender and women's risk of being harmed by image-based sexual abuse by current and former intimate male partners. Thus, the main objective of this article is to address this concern by applying male peer support theory.",
        "date": "October 2016",
        "authors": [
            "Walter S. Dekeseredy",
            "Martin D. Schwartz"
        ],
        "references": [
            316990640,
            303470751,
            296938550,
            294787403,
            348103697,
            320299926,
            313419349,
            313047789,
            311885193,
            305567661
        ]
    },
    {
        "id": 311100762,
        "title": "More than a breach of privacy: image-based sexual abuse and the Irish Law Reform Commission on harmful communications",
        "abstract": "In September 2016, the Irish Law Commission published its long-awaited report on \u2018Harmful Communications and Digital Safety\u2019. This compelling document puts forward a comprehensive package of law reforms to tackle the growing problems of online harassment and abuse perpetrated using modern technologies and social media. Amongst its many provisions, the Report recommends new laws to tackle the phenomenon colloquially known as \u2018revenge porn\u2019 which is rightly identified as a significant harm deserving of legislative intervention. In this paper, I step back from the detailed technical provisions, to focus on the more conceptual issue of the nature of these online harms, why they are being perpetrated and what are the implications of this. Drawing on work with my colleague Erika Rackley, I name and conceptualise harmful communications as \u2018image-based sexual abuse\u2019. My aim is to emphasise: that these are gendered and sexual harms that are forms of sexual offending; that this is how victim-survivors understand the harm perpetrated against them; and that the victim-survivors are mainly women and girls. I also hope to explain why this conceptualisation matters: it shapes shape legislative responses, educational approaches and our broader awareness of the phenomenon.",
        "date": "January 2017",
        "authors": [
            "Clare Mcglynn"
        ],
        "references": []
    },
    {
        "id": 310612916,
        "title": "Image-Based Sexual Abuse (Oxford Journal of Legal Studies)",
        "abstract": "Advances in technology have transformed and expanded the ways in which sexual violence can be perpetrated. One new manifestation of such violence is the non-consensual creation and/or distribution of private sexual images: what we conceptualise as \u2018image-based sexual abuse\u2019. This article delineates the scope of this new concept and identifies the individual and collective harms it engenders. We argue that the individual harms of physical and mental illness, together with the loss of dignity, privacy and sexual autonomy, combine to constitute a form of cultural harm, impacting directly on individuals, as well as on society as a whole. While recognizing the limits of law, we conclude by considering the options for redress and the role of law, seeking to justify the deployment of the expressive and coercive powers of the criminal and civil law as a means of encouraging cultural change. https://academic.oup.com/ojls/article/doi/10.1093/ojls/gqw033/2965256/ImageBased-Sexual-Abuse?guestAccessKey=349e7a45-7633-4b34-8e69-9d39aef546ab",
        "date": "September 2017",
        "authors": [
            "Clare Mcglynn",
            "Erika Rackley"
        ],
        "references": [
            272607423,
            346473597,
            304033130,
            299028407,
            261902918
        ]
    },
    {
        "id": 348103697,
        "title": "Surviving Sexual Violence.",
        "abstract": "",
        "date": "May 1990",
        "authors": [
            "Linda M Williams",
            "Liz Kelly"
        ],
        "references": []
    },
    {
        "id": 344077641,
        "title": "Hate Crimes in Cyberspace",
        "abstract": "",
        "date": "September 2014",
        "authors": [
            "Danielle Keats Citron"
        ],
        "references": []
    },
    {
        "id": 333555653,
        "title": "Intimate intrusions: Women's experience of male violence",
        "abstract": "First published in 1985, this book looks at the victimisation of women, focusing on the four main areas of incest, rape, physical violence, and sexual harassment. Elizabeth Stanko's work is based on original research and interviews with police forces, victims and others involved. It examines women's experiences of male violence and looks at the reactions of those to whom women complain, including police officers, judges and union officials. The book analyses the decision making process of the criminal justice system and of administrative personnel at the time of publication, and Stanko shows how such institutions can be carriers of a male point of view.",
        "date": "October 2013",
        "authors": [
            "E. Stanko"
        ],
        "references": []
    },
    {
        "id": 320300944,
        "title": "Outlook: Girlhood, Agency, and Embodied Space for Action",
        "abstract": "The sexual double standard and the realities of sexual violence have a significant impact on the embodied freedoms of girls and young women, as shown in Chaps. 5 and 6. The work of both Aaltonen and Honkasalo reveals ongoing challenges for feminists seeking to conceptualize the ambiguous balance of girls\u2019 agency, particularly sexual agency, as it is lived. There may be, however, untapped potential in further working with the concepts of embodied space for action and situated agency, employing both alongside an intersectional perspective that pays attention to how a hierarchy of worth situates girls in relation to each other.",
        "date": "October 2017",
        "authors": [
            "F. Vera-Gray"
        ],
        "references": [
            227202858,
            2822720,
            347507066,
            344799397,
            313291403,
            301233527,
            281773470,
            257663730,
            253028499,
            241897254
        ]
    },
    {
        "id": 308878469,
        "title": "Technology-Facilitated Sexual Violence Victimization",
        "abstract": "Online forms of sexual harassment and abuse as experienced by adults represent an emerging yet under-researched set of behaviors, such that very few studies have sought to estimate the extent of the problem. This article presents the results of an online survey of 2,956 Australian adult (aged 18 to 54 years) experiences of technology-facilitated sexual violence (TFSV) victimization. The prevalence of TFSV was analyzed in relation to a 21-item scale developed in accordance with prior conceptual research identifying multiple dimensions of TFSV including digital sexual harassment, image-based sexual abuse, sexual aggression and/or coercion, and, gender and/or sexuality-based harassment (including virtual sexual violence). Results revealed significant differences in lifetime TFSV victimization for younger (18-24) and non-heterosexual identifying adults. Lifetime TFSV victimization for men and women was not significantly different, though women were more likely to report sexual harassment victimization and men were more likely to report victimization through the distribution of non-consensual images, as well as gender and/or sexuality-based harassment. The authors conclude that although women and men report experiencing similar overall prevalence of TFSV victimization, the nature and impacts of those experiences differ in particular gendered ways that reflect broader patterns in both gender relations and \u201coffline\u201d sexual harassment.",
        "date": "October 2016",
        "authors": [
            "Anastasia Powell",
            "Nicola Henry"
        ],
        "references": [
            282488311,
            281174005,
            279515271,
            277533008,
            276182115,
            276071982,
            274399083,
            274068432,
            271947150,
            266392546
        ]
    },
    {
        "id": 305942764,
        "title": "Punishment, Feminism, and Political Identity: A Case Study in the Expressive Meaning of the Law",
        "abstract": "In the Spring of 1995,1 was asked to testify as an expert witness in a case in Canada that raised a number of different philosophical and jurisprudential issues. The case concerned whether prisoners sentenced to two years or more in a Canadian penitentiary had the right to vote. For many years, Canada has denied those incarcerated in its prisons voting rights (following the British practice of doing so), but after the enactment of the Canadian Charter of Rights and Freedoms in 1982, which grants each citizen of Canada the right to vote, that practice was challenged; in a series of court cases, prisoners maintained that denying them the right to vote during their incarceration amounted to denying them one of their basic constitutional rights as Canadian citizens. One of the most important issues raised by this case was the nature of Canada\u2019s political identity. The fact that the political identity of a state can be partly at stake in a law is, I believe, important and insufficiently recognized. A law can be not only a tool for the organization of the community (e.g., by promoting order, or coordination, or public wellbeing), but also a significant expressive force in that community, symbolizing the community\u2019s sense of its values and (what I will call) its \u201cpolitical personality\u201d. Indeed, for countries which are not culturally homogeneous and in which the unity of the community is primarily purchased through the principles of its polity, the expressive nature of certain laws can be essential in the creation, maintenance or revision of a unifying identity for that society; this is an identity that not only helps to hold the pluralist society together but also helps people to have a sense of themselves as members of that political community. I hope to argue that the controversy surrounding the issue of whether or not prisoners\u2019 voting rights should be suspended reflects controversy about what kind of state Canada is and shows the ways in which law can be expressive.",
        "date": "January 1998",
        "authors": [
            "Jean Hampton"
        ],
        "references": []
    },
    {
        "id": 304338281,
        "title": "Revenge Porn and Mental Health: A Qualitative Analysis of the Mental Health Effects of Revenge Porn on Female Survivors",
        "abstract": "This study examines the emotional and mental health effects revenge porn has on female survivors. To date, no other academic studies have exclusively focused on mental health effects in revenge porn cases. In-depth qualitative interviews were conducted between February 2014 and January 2015 with 18 female revenge porn survivors, and inductive analysis revealed participants\u2019 experiences of trust issues, posttraumatic stress disorder (PTSD), anxiety, depression, suicidal thoughts, and several other mental health effects. These findings reveal the seriousness of revenge porn, the devastating impacts it has on survivors\u2019 mental health, and similarities between revenge porn and sexual assault.",
        "date": "June 2016",
        "authors": [
            "Samantha Bates"
        ],
        "references": [
            263155387,
            261582556,
            248070666,
            241742854,
            235371865,
            232390704,
            47566226,
            44131718,
            24416607,
            23761827
        ]
    },
    {
        "id": 309185766,
        "title": "Support vector regression machines",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "Harris Drucker",
            "Christopher J. C. Burges",
            "Linda Kaufman",
            "Alexander J. Smola"
        ],
        "references": [
            265619863,
            222798530,
            38359483
        ]
    },
    {
        "id": 310666547,
        "title": "PICASO: PIxel correspondences and SOft match selection for real-time tracking",
        "abstract": "Visual tracking is one of the computer vision\u2019s longstanding challenges, with many methods as a result. While most state-of-the-art methods trade-off performance for speed, we propose PICASO, an efficient, yet strongly performing tracking scheme. The target object is modeled as a set of pixel-level templates with weak configuration constraints. The pixels of a search window are matched against those of the surrounding context and of the object model. To increase the robustness, we match also from the object to the search window, and the pairs matching in both directions are the correspondences used to localize. This localization process is robust, also against occlusions which are explicitly modeled. Another source of robustness is that the model \u2013 as in several other modern trackers \u2013gets constantly updated over time with newly incoming information about the target appearance. Each pixel is described by its local neighborhood. The match of a pixel is taken to be the one with the largest contribution in its sparse decomposition over a set of pixels. For this soft match selection, we analyze both l1 and l2-regularized least squares formulations and the recently proposed l1-constrained \u2018Iterative Nearest Neighbors\u2019 approach. We evaluate our tracker on standard videos for rigid and non-rigid object tracking. We obtain excellent performance at 42fps with Matlab on a CPU.",
        "date": "December 2016",
        "authors": [
            "Radu Timofte",
            "Junseok Kwon",
            "Luc Van Gool"
        ],
        "references": [
            271551248,
            265210916,
            259464776,
            258848394,
            255569350,
            225597873,
            221111132,
            221110715,
            220660094,
            220566062
        ]
    },
    {
        "id": 305748100,
        "title": "Structured Output SVM Prediction of Apparent Age, Gender and Smile From Deep Features",
        "abstract": "We propose structured output SVM for predicting the apparent age as well as gender and smile from a single face image represented by deep features. We pose the problem of apparent age estimation as an instance of the multi-class structured output SVM classifier followed by a softmax expected value refinement. The gender and smile predictions are treated as binary classification problems. The proposed solution first detects the face in the image and then extracts deep features from the cropped image around the detected face. We use a convolutional neural network with VGG-16 architecture [25] for learning deep features. The network is pretrained on the ImageNet [24] database and then fine-tuned on IMDB-WIKI [21] and ChaLearn 2015 LAP datasets [8]. We validate our methods on the ChaLearn 2016 LAP dataset [9]. Our structured output SVMs are trained solely on ChaLearn 2016 LAP data. We achieve excellent results for both apparent age prediction and gender and smile classification.",
        "date": "July 2016",
        "authors": [
            "Michal U\u0159i\u010d\u00e1\u0159",
            "Rasmus Rothe",
            "Jiri Matas",
            "Luc Van Gool"
        ],
        "references": [
            305196650,
            304407874,
            289601821,
            283356929,
            275364436,
            268988676,
            268988287,
            268988012,
            265295439,
            265125544
        ]
    },
    {
        "id": 305327283,
        "title": "Multi-view facial landmark detector learned by the Structured Output SVM",
        "abstract": "We propose a real-time multi-view landmark detector based on Deformable Part Models (DPM). The detector is composed of a mixture of tree based DPMs, each component describing landmark configurations in a specific range of viewing angles. The usage of view specific DPMs allows to capture a large range of poses and to deal with the problem of self-occlusions. Parameters of the detector are learned from annotated examples by the Structured Output Support Vector Machines algorithm. The learning objective is directly related to the performance measure used for detector evaluation. The tree based DPM allows to find a globally optimal landmark configuration by the dynamic programming. We propose a coarse-to-fine search strategy which allows real-time processing by the dynamic programming also on high resolution images. Empirical evaluation on \u201cin the wild\u201d images shows that the proposed detector is competitive with the state-of-the-art methods in terms of speed and accuracy yet it keeps the guarantee of finding a globally optimal estimate in contrast to other methods.",
        "date": "March 2016",
        "authors": [
            "Michal U\u0159i\u010d\u00e1\u0159",
            "Vojt\u011bch Franc",
            "Diego Thomas",
            "Akihiro Sugimoto"
        ],
        "references": [
            264419855,
            262309544,
            261498067,
            240446286,
            236850781,
            222408097,
            221430174,
            221363088,
            221345723,
            221111251
        ]
    },
    {
        "id": 257663820,
        "title": "Children\u2019s Gender-Typed Activity Choices Across Preschool Social Contexts",
        "abstract": "Variability in children\u2019s gender-typed activity preferences was examined across several preschool social contexts--solitary play, interactions with female peers, male peers, and both, and interactions with teachers. Participants were preschool children (N = 264; 49 % girls, M age = 52 months, range 37\u201360) attending Head Start classes in the Southwest United States. Seventy-three percent were Mexican/Mexican-American, and 82 % of families earned less than $30,000 per year. Children\u2019s preferences for gender-typed activities varied as a function of their own gender and the identity of their interactional partners. Girls and boys preferred gender-typed activities (e.g., girls preferred feminine activities) when in solitary play but activity preferences changed across social contexts. Specifically, girls played significantly more with masculine activities when with male peers and boys played significantly more with feminine activities during interactions with teachers. Findings suggest that through social interactions with peers and teachers, children are exposed to a greater range of activities than what they experience when they play by themselves.",
        "date": "October 2012",
        "authors": [
            "Priscilla Goble",
            "Carol Martin",
            "Laura Hanish",
            "Richard A. Fabes"
        ],
        "references": [
            254110417,
            323424582,
            309050814,
            297095776,
            284052292,
            272589124,
            271782340,
            270761711,
            250185483,
            249021677
        ]
    },
    {
        "id": 312528237,
        "title": "Gender Bias and Musical Instrument Preference",
        "abstract": "",
        "date": "October 1993",
        "authors": [
            "Susan M. Tarnowski"
        ],
        "references": [
            249808451,
            275285489,
            274448971,
            254346401,
            249808719,
            249808643,
            240262530,
            240053818,
            238436229,
            238436166
        ]
    },
    {
        "id": 298596122,
        "title": "Gender and musical instrument choice: A phenomenological investigation",
        "abstract": "The purpose of this study was to explore the current perceptions of high school instrumental music students regarding gender and instrument choice. Research questions included: Where did students think gender stereotypes derived from? What were the self-stated personal characteristics of those who broke gender stereotypes and those who did not break the stereotypes in instrument choice? What did students report as parent reactions to instrument choice? What categories emerged from these interviews which suggest the possible influences on these students regarding instrument choice? Were there other issues in instrument choice talked about by many of the students? Using a phenomenological interview design, 37 high school instrumental music students were individually interviewed. Interview tapes were transcribed and data were examined for commonalities and discrepancies. Results are interpreted with regard to previous gender and musical instrument choice research as well as the gendered musical meaning theory presented by Green (1997).",
        "date": "September 2000",
        "authors": [
            "Colleen Conway"
        ],
        "references": [
            249808451,
            249808643,
            249808571,
            238436229,
            222951955
        ]
    },
    {
        "id": 285980681,
        "title": "Gender and Musical Instrument Stereotypes in Middle School Children: Have Trends Changed?",
        "abstract": "Previous studies have established that gender stereotypes are associated with children\u2019s choice of musical instrument. Though some have suggested that these gender stereotypes may be trending toward change, other studies have indicated that gender stereotypes are long-standing and still very much at issue. This descriptive study of middle school band students (N = 99) examined instrument gender stereotypes using improved methods of measurement and found evidence that instrument gender stereotypes remain entrenched and pose a persisting problem facing music educators. Importantly, younger and more inexperienced band members may be more open to counterstereotypical views. Suggested changes in research methods and directions for future study are discussed, as are implications for music educators.",
        "date": "December 2014",
        "authors": [
            "Elizabeth R. Wrape",
            "Alexandra L. Dittloff",
            "Jennifer L Callahan"
        ],
        "references": [
            258103869,
            249839860,
            249808451,
            249752819,
            240725668,
            240725650,
            232004596,
            226709842,
            225248305,
            311575161
        ]
    },
    {
        "id": 284857953,
        "title": "Sexism and stereotypes in modern society: The gender science of Janet Taylor Spence",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "Rebecca Bigler"
        ],
        "references": []
    },
    {
        "id": 283121792,
        "title": "A Comparison of Aural and Visual Instrument Preferences of Third and Fifth-Grade Students",
        "abstract": "Instrument preferences of third and fifth-grade students (N = 90) were investigated for eight instruments commonly found in orchestra and band: flute, clarinet, saxophone, violin, cello, trumpet, French horn, and trombone. Participants were divided into two groups and asked to identify their favorite and least favorite instrument from a list of the eight instruments. One class each of third and fifth-grade students, Group A, listened to aural examples of the instruments and rated preferences for each instrument. The other two classes, Group B, rated pictures of each instrument. Overall ratings placed the eight instruments in the following order of preference: violin, flute, cello, saxophone, clarinet, trumpet, trombone, and French horn. A significant interaction between grade, gender, and instruments indicated little difference between genders in instrument preference at the third-grade level, but in fifth-grade, females preferred flute, violin, and cello more than males. No significant difference was found between the two methods of testing.",
        "date": "December 2009",
        "authors": [
            "Rebecca B. Macleod"
        ],
        "references": [
            249808451,
            275285489,
            249839981,
            249808740,
            249808643,
            249808571,
            249808530,
            247733456,
            247732911,
            238436229
        ]
    },
    {
        "id": 270626559,
        "title": "Instrument, gender and musical style associations in young children",
        "abstract": "Numerous studies have explored the relationship between musical instruments and their associations with a particular gender. This study focussed on the developing association between gender and musical instruments in young children and further explored the interaction between gender, instrument and musical style. The research was carried out on 65 participants aged three and four years old. Each participant took part in a short musical game which involved matching 14 musical excerpts with photographs of the individuals who might play the instruments represented within each excerpt. The research used a 2 (gender) x 2 (musical style) x 7 (instrument) factoral design in which a 'feminine' or 'masculine' instrument was featured playing in a 'masculine' or 'feminine' associated style. Our results suggested that prominent gender stereotypes for some instruments do appear to exist in very young children whilst in other instruments, gender associations appear to be also linked to the musical style in which they are represented and possibly the performance context in which they are experienced.",
        "date": "June 2011",
        "authors": [
            "Nigel A. Marshall",
            "Kagari Shibazaki"
        ],
        "references": [
            249808451,
            249752819,
            240725668,
            240725650,
            225248305,
            344554002,
            269560411,
            249812810,
            249808643,
            249808571
        ]
    },
    {
        "id": 258198928,
        "title": "Gender and Instrument Associations, Stereotypes, and Stratification A Literature Review",
        "abstract": "This literature review examines and synthesizes 30 years of research into the relationship between gender and musical instruments. Specifically, the review focuses on how this relationship affects instrument selection by grade school students entering a school music program. Topics include the gender typing of musical instruments, instrument preferences of young (preband or preorchestra) students, beliefs about which gender should play which instrument, gender influences during the instrument selection process, the status of gender stratification within instrumental ensembles, and perceptions of musicians in relation to their gender and instrument. Also discussed are theories as to why gender associations and stereotypes occur within instrumental music, including social role theory, and ways in which researchers have attempted to counteract gender as a factor in the instrument selection process. The review concludes with implications drawn from the body of research and direction for future studies.",
        "date": "May 2012",
        "authors": [
            "Gina M. F. Wych"
        ],
        "references": [
            271808434,
            249808451,
            240725650,
            344550308,
            315180611,
            311575161,
            298596122,
            286360570,
            283121792,
            249808755
        ]
    },
    {
        "id": 258198313,
        "title": "The Effect of Demonstrator Gender on Wind Instrument Preferences of Kindergarten, Third-Grade, and Fifth-Grade Students",
        "abstract": "This study examined possible influences of demonstrator gender on children\u2019s instrument choices. Participants (N = 104) included boys (n = 53) and girls ( n = 51) in fifth grade (n = 27), third grade (n = 41), and kindergarten (n = 36) in six intact music classes from a single elementary school. Pretest and posttest consisted of circling (Grades 3 and 5) or placing a sticker on (kindergarten) the instrument \u201cyou most want to play\u201d from a picture containing flute, clarinet, alto saxophone, trumpet, trombone, and tuba. Treatment consisted of intact classes viewing a live demonstration of all six instruments performed by either all male or all female university music majors. Results indicated that boys who viewed male demonstrators chose more brass instruments, whereas girls who viewed female demonstrators chose more woodwind instruments, although these differences were not statistically significant. Both boys and girls who saw opposite-gender demonstrators picked brass and woodwind in nearly equal numbers.",
        "date": "May 2011",
        "authors": [
            "Janice N. Killian",
            "Shauna L. Satrom"
        ],
        "references": [
            249839860,
            249808451,
            249752819,
            240725668,
            240725650,
            249808740,
            249808643,
            249808571,
            249808530,
            249808375
        ]
    },
    {
        "id": 257663835,
        "title": "Self-Perceived Gender Typicality, Gender-Typed Attributes, and Gender Stereotype Endorsement in Elementary-School-Aged Children",
        "abstract": "This study examined relations among self-perceived gender typicality, gender-typed attributes, and gender stereotype endorsement with a sample of elementary-school-aged children (N = 100, ages 6\u201312) from the Midwestern United States. Children who perceived themselves as more gender-typical were more interested in same-gender-typed activities and occupations and less interested in other-gender-typed activities and occupations than children who perceived themselves as less gender-typical. Gender typicality was linked to gender stereotype endorsement, as predicted based on Liben and Bigler\u2019s (2002) dual-pathway model of gender development, with children who perceived themselves as less gender-typical having more egalitarian (less stereotyped) attitudes than children who perceived themselves as more gender-typical. The observed relations between gender-typed attributes and self-perceived gender typicality and between self-perceived gender typicality and gender stereotype endorsement did not differ across gender or age. These findings indicate that even young elementary-school-aged children use their knowledge of cultural gender roles to make subjective judgments regarding the self, and, conversely, that views of the self may influence personal endorsement of cultural gender stereotypes. Although the majority of extant research has focused on negative outcomes associated with low self-perceived gender typicality (e.g., low self-esteem), this research indicates that positive outcomes (e.g., flexible gender role attitudes) may also be associated with low self-perceived gender typicality.",
        "date": "October 2012",
        "authors": [
            "Meagan M. Patterson"
        ],
        "references": [
            249707878,
            233692354,
            232574526,
            225886667,
            225717649,
            51484189,
            51224770,
            44569225,
            41123893,
            11895912
        ]
    },
    {
        "id": 264436461,
        "title": "Gender Recognition on Real World Faces based on Shape Representation and Neural Network",
        "abstract": "Gender as a soft biometric attribute has been extensively investigated in the domain of computer vision because of its numerous potential application areas. However, studies have shown that gender recognition performance can be hindered by improper alignment of facial images. As a result, previous experiments have adopted face alignment as an important stage in the recognition process, before performing feature extraction. In this paper, the problem of recognizing human gender from unaligned real world faces using single image per individual is investigated. The use of feature descriptor to form shape representation of face images with any arbitrary orientation from the cropped version of Labeled Faces in the Wild (LFW) dataset is proposed. By combining the feature extraction technique with artificial neural network for classification, a recognition rate of 89.3% is attained.",
        "date": "August 2014",
        "authors": [
            "Vahab Iranmanesh"
        ],
        "references": [
            262152447,
            261527832,
            319770820,
            289937383,
            284688002,
            281327886,
            278653827,
            256721218,
            254051931,
            235890195
        ]
    },
    {
        "id": 264435804,
        "title": "Gender Recognition on Real World Faces Based on Shape Representation and Neural Network",
        "abstract": "Gender as a soft biometric attribute has been extensively investigated in the domain of computer vision because of its numerous potential application areas. However, studies have shown that gender recognition performance can be hindered by improper alignment of facial images. As a result, previous experiments have adopted face alignment as an important stage in the recognition process, before performing feature extraction. In this paper, the problem of recognizing human gender from unaligned real world faces using single image per individual is investigated. The use of feature descriptor to form shape representation of face images with any arbitrary orientation from the cropped version of Labeled Faces in the Wild (LFW) dataset is proposed. By combining the feature extraction technique with artificial neural network for classification, a recognition rate of 89.3% is attained.",
        "date": "June 2014",
        "authors": [
            "Olasimbo Ayodeji Arigbabu",
            "Sharifah Mumtazah",
            "Syed Aijaz Ahmad",
            "Wan Azizun"
        ],
        "references": [
            262152447,
            261527832,
            319770820,
            289937383,
            284688002,
            281327886,
            278653827,
            256721218,
            254051931,
            224565683
        ]
    },
    {
        "id": 318850845,
        "title": "Gender inequality at work",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "David A. Cotter",
            "Joan Hermsen",
            "R. Vanneman"
        ],
        "references": []
    },
    {
        "id": 314610873,
        "title": "Social Groups and Social Stereotypes",
        "abstract": "A series of studies by Taylor and Simard (1975) demonstrated that cross-cultural communication can be, in objective terms, as effective as within-group communication. We should ask then, why this is not always the case, and subjectively too. A major part of the answer, we believe, lies in the role played by stereotypes. We therefore consider the nature of stereotypes, their cognitive foundations and consequences, social functions, resistance to change, and relationship to behaviour.",
        "date": "January 1997",
        "authors": [
            "Miles Hewstone",
            "Howard NOTE: SCOtton is not the co-author Giles"
        ],
        "references": [
            281995848,
            269710388,
            258143974,
            248047301,
            232580846,
            232438516,
            222205593,
            344906326,
            299061736,
            290894542
        ]
    },
    {
        "id": 289963745,
        "title": "Stereotype threat and women\u00b4s math performance",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "Steven John Spencer",
            "Claude M Steele",
            "Diane M Quinn"
        ],
        "references": []
    },
    {
        "id": 281453286,
        "title": "Categorically Unequal: The American Stratification System",
        "abstract": "The United States holds the dubious distinction of having the most unequal income distribution of any advanced industrialized nation. While other developed countries face similar challenges from globalization and technological change, none rivals America\u2019s singularly poor record for equitably distributing the benefits and burdens of recent economic shifts. In Categorically Unequal, Douglas Massey weaves together history, political economy, and even neuropsychology to provide a comprehensive explanation of how America\u2019s culture and political system perpetuates inequalities between different segments of the population. Categorically Unequal is striking both for its theoretical originality and for the breadth of topics it covers. Massey argues that social inequalities arise from the universal human tendency to place others into social categories. In America, ethnic minorities, women, and the poor have consistently been the targets of stereotyping, and as a result, they have been exploited and discriminated against throughout the nation\u2019s history. African-Americans continue to face discrimination in markets for jobs, housing, and credit. Meanwhile, the militarization of the U.S.-Mexican border has discouraged Mexican migrants from leaving the United States, creating a pool of exploitable workers who lack the legal rights of citizens. Massey also shows that women\u2019s advances in the labor market have been concentrated among the affluent and well-educated, while low-skilled female workers have been relegated to occupations that offer few chances for earnings mobility. At the same time, as the wages of low-income men have fallen, more working-class women are remaining unmarried and raising children on their own. Even as minorities and women continue to face these obstacles, the progressive legacy of the New Deal has come under frontal assault. The government has passed anti-union legislation, made taxes more regressive, allowed the real value of the federal minimum wage to decline, and drastically cut social welfare spending. As a result, the income gap between the richest and poorest has dramatically widened since 1980. Massey attributes these anti-poor policies in part to the increasing segregation of neighborhoods by income, which has insulated the affluent from the social consequences of poverty, and to the disenfranchisement of the poor, as the population of immigrants, prisoners, and ex-felons swells. America\u2019s unrivaled disparities are not simply the inevitable result of globalization and technological change. As Massey shows, privileged groups have systematically exploited and excluded many of their fellow Americans. By delving into the root causes of inequality in America, Categorically Unequal provides a compelling argument for the creation of a more equitable society.",
        "date": "January 2007",
        "authors": [
            "Douglas Massey"
        ],
        "references": [
            258194030,
            253899227,
            249707490,
            240618038,
            227582530,
            227582495,
            225549309,
            5196127,
            299040800,
            291243638
        ]
    },
    {
        "id": 279542701,
        "title": "Description and prescription: How gender stereotypes prevent women's ascent up the organizational ladder",
        "abstract": "This review article posits that the scarcity of women at the upper levels of organizations is a consequence of gender bias in evaluations. It is proposed that gender stereotypes and the expectations they produce about both what women are like (descriptive) and how they should behave (prescriptive) can result in devaluation of their performance, denial of credit to them for their successes, or their penalization for being competent. The processes giving rise to these outcomes are explored, and the procedures that are likely to encourage them are identified. Because of gender bias and the way in which it influences evaluations in work settings, it is argued that being competent does not ensure that a woman will advance to the same organizational level as an equivalently performing man.",
        "date": "January 2001",
        "authors": [
            "Madeline Heilman"
        ],
        "references": [
            272593608,
            232549066,
            232509164,
            232338405,
            4883329
        ]
    },
    {
        "id": 320964512,
        "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Mark Yatskar",
            "Vicente Ordonez",
            "Luke Zettlemoyer",
            "Ali Farhadi"
        ],
        "references": [
            319770160,
            308809117,
            307747289,
            307303751,
            305805951,
            305341979,
            277959356,
            275897099,
            265295439,
            264979485
        ]
    },
    {
        "id": 311611436,
        "title": "Situation Recognition: Visual Semantic Role Labeling for Image Understanding",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Mark Yatskar",
            "Luke Zettlemoyer",
            "Ali Farhadi"
        ],
        "references": [
            319770160,
            308809117,
            307747289,
            277959356,
            267237251,
            265295439,
            264979485,
            262242230,
            233815759,
            221303985
        ]
    },
    {
        "id": 311609207,
        "title": "Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Ishan Misra",
            "C. Lawrence Zitnick",
            "Margaret Mitchell",
            "Ross Girshick"
        ],
        "references": [
            313769134,
            308161118,
            308034527,
            279030815,
            277022946,
            273327809,
            269935614,
            268525230,
            265295439,
            264979485
        ]
    },
    {
        "id": 303367536,
        "title": "Stereotyping and Bias in the Flickr30K Dataset",
        "abstract": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \"focus only on the information that can be obtained from the image alone\" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",
        "date": "May 2016",
        "authors": [
            "Emiel van Miltenburg"
        ],
        "references": [
            319770160,
            307747289,
            291783641,
            258290822,
            46576676,
            37624398,
            1913681,
            304408739,
            303721259,
            290504141
        ]
    },
    {
        "id": 312151732,
        "title": "Recognizing What Matters: Value Improves Recognition by Selectively Enhancing Recollection",
        "abstract": "We examined the effects of value on recognition by assessing its contribution to recollection and familiarity. In three experiments, participants studied English words, each associated with a point-value they would earn for correct recognition, with the goal of maximizing their score. In Experiment 1, participants provided Remember/Know judgments. In Experiment 2 participants indicated whether items were recollected or if not, their degree of familiarity along a 6-point scale. In Experiment 3, recognition of words was accompanied by a test of memory for incidental details. Across all experiments, participants were more likely to recognize items with higher point-value. Furthermore, value appeared to primarily enhance recollection, as effects on familiarity were small and not consistent across experiments. Recollection of high-value items appears to be accompanied by fewer incidental details, suggesting that value increases focus on items at the expense of irrelevant information.",
        "date": "June 2017",
        "authors": [
            "Joseph P. Hennessee",
            "Alan Castel",
            "Barbara J Knowlton"
        ],
        "references": [
            312152804,
            289637448,
            288664633,
            236186386,
            310051087,
            282153422,
            278853599,
            262920461,
            258349808,
            236924218
        ]
    },
    {
        "id": 306550226,
        "title": "Norms of valence, arousal, concreteness, familiarity, imageability, and context availability for 1,100 Chinese words",
        "abstract": "In the present study, we collected valence, arousal, concreteness, familiarity, imageability, and context availability ratings for a total of 1,100 Chinese words. The ratings for all variables were collected with 9-point Likert scales. We tested the reliability of the present database by comparing it to the extant Chinese Affective Word System, and performed split-half correlations for all six variables. We then evaluated the relationships between all variables. Regarding the affective variables, we found a typical quadratic relation between valence and arousal, in line with previous findings. Likewise, significant correlations were found between the semantic variables. Importantly, we explored the relationships between ratings for the affective variables (i.e., valence and arousal) and concreteness ratings, suggesting that valence and arousal ratings can predict concreteness ratings. This database of affective norms will be a valuable source of information for emotion research that makes use of Chinese words, and will enable researchers to use highly controlled Chinese verbal stimuli to more reliably investigate the relation between cognition and emotion.",
        "date": "August 2016",
        "authors": [
            "Zhao Yao",
            "Jia Wu",
            "Yanyan Zhang",
            "Zhenhong Wang"
        ],
        "references": [
            285805831,
            273137309,
            267816883,
            263205964,
            259700732,
            325671474,
            285014368,
            283544666,
            279236824,
            264693234
        ]
    },
    {
        "id": 283123157,
        "title": "The Impact of Word Prevalence on Lexical Decision Times: Evidence From the Dutch Lexicon Project 2",
        "abstract": "Keuleers, Stevens, Mandera, and Brysbaert (2015) presented a new variable, word prevalence, defined as word knowledge in the population. Some words are known to more people than other. This is particularly true for low-frequency words (e.g., screenshot vs. scourage). In the present study, we examined the impact of the measure by collecting lexical decision times for 30,000 Dutch word lemmas of various lengths (the Dutch Lexicon Project 2). Word prevalence had the second highest correlation with lexical decision times (after word frequency): Words known by everyone in the population were responded to 100 ms faster than words known to only half of the population, even after controlling for word frequency, word length, age of acquisition, similarity to other words, and concreteness. Because word prevalence has rather low correlations with the existing measures (including word frequency), the unique variance it contributes to lexical decision times is higher than that of the other variables. We consider the reasons why word prevalence has an impact on word processing times and we argue that it is likely to be the most important new variable protecting researchers against experimenter bias in selecting stimulus materials.",
        "date": "October 2015",
        "authors": [
            "Marc Brysbaert",
            "Michael Stevens",
            "Pawe\u0142 Mandera",
            "emmanuel keuleers"
        ],
        "references": [
            287471787,
            286363969,
            281748849,
            281286234,
            280941312,
            276192511,
            317618293,
            295120540,
            285873774,
            276358161
        ]
    },
    {
        "id": 281768543,
        "title": "Emotional Valence, Arousal, and Threat Ratings of 160 Chinese Words among Adolescents",
        "abstract": "This study was conducted to provide ratings of valence/pleasantness, arousal/excitement, and threat/potential harm for 160 Chinese words. The emotional valence classification (positive , negative, or neutral) of all of the words corresponded to that of the equivalent English language words. More than 90% of the participants, junior high school students aged between 12 and 17 years, understood the words. The participants were from both mainland China and Hong Kong, thus the words can be applied to adolescents familiar with either simplified (e.g. in mainland China) or traditional Chinese (e.g. in Hong Kong) with a junior secondary school education or higher. We also established eight words with negative valence, high threat, and high arousal ratings to facilitate future research, especially on attentional and memory biases among individuals prone to anxiety. Thus, the new emotional word list provides a useful source of information for affective research in the Chinese language.",
        "date": "August 2015",
        "authors": [
            "Samuel M Y Ho",
            "Christine W Y Mak",
            "Dannii Y Yeung",
            "Wenjie Duan"
        ],
        "references": [
            263412659,
            261641295,
            260527016,
            258767726,
            344102833,
            290852515,
            288743740,
            277559888,
            263473301,
            258184036
        ]
    },
    {
        "id": 273137309,
        "title": "Affective norms of 875 Spanish words for five discrete emotional categories and two emotional dimensions",
        "abstract": "In the present study, we introduce affective norms for a new set of Spanish words, the Madrid Affective Database for Spanish (MADS), that were scored on two emotional dimensions (valence and arousal) and on five discrete emotional categories (happiness, anger, sadness, fear, and disgust), as well as on concreteness, by 660 Spanish native speakers. Measures of several objective psycholinguistic variables-grammatical class, word frequency, number of letters, and number of syllables-for the words are also included. We observed high split-half reliabilities for every emotional variable and a strong quadratic relationship between valence and arousal. Additional analyses revealed several associations between the affective dimensions and discrete emotions, as well as with some psycholinguistic variables. This new corpus complements and extends prior databases in Spanish and allows for designing new experiments investigating the influence of affective content in language processing under both dimensional and discrete theoretical conceptions of emotion. These norms can be downloaded as supplemental materials for this article from www.dropbox.com/s/o6dpw3irk6utfhy/Hinojosa%20et%20al_Supplementary%20materials.xlsx?dl=0 .",
        "date": "March 2015",
        "authors": [
            "Jos\u00e9 A Hinojosa",
            "Natalia Mart\u00ednez-Garc\u00eda",
            "C Villalba-Garc\u00eda",
            "Ux\u00eda Fern\u00e1ndez Folgueiras"
        ],
        "references": [
            272174285,
            270050057,
            263205964,
            262612097,
            262421948,
            325671474,
            279235778,
            272905259,
            270606344,
            261735663
        ]
    },
    {
        "id": 304246676,
        "title": "The Minho Word Pool: Norms for imageability, concreteness and subjective frequency for 3,800 Portuguese words. Behavior Research Methods",
        "abstract": "Words are widely used as stimuli in cognitive research. Because of their complexity, using words requires a strict control of their objective (lexical and sublexical) and subjective properties. In this work we present the Minho Word Pool (MWP), a dataset that provides normative values of imageability, concreteness and subjective frequency for 3,800 (European) Portuguese words, three subjective measures, which in spite of being extensively used in research, were still scarce for Portuguese. Data were collected with 2,357 college students who were native speakers of European Portuguese. Participants rated 100 words drawn randomly from the full set in each of the three subjective indices using a web survey procedure (via a URL link). Analyses comparing the MWP ratings with those obtained for the same words from other national and international databases showed that the MWP norms are reliable and valid, thus providing researchers with a useful tool to support research in all neuroscientific areas using verbal stimuli. The MWP norms can be downloaded at http://brm.psychonomic-journals.org/content/supplemental or at http://p-pal.di.uminho.pt/about/databases.",
        "date": "June 2016",
        "authors": [
            "Ana Paula Soares",
            "Ana Costa",
            "Jo\u00e3o Machado",
            "Montserrat Comesa\u00f1a"
        ],
        "references": [
            270885975,
            268039400,
            267816883,
            266263161,
            263965282,
            263163589,
            259718817,
            259700732,
            258061778,
            236983757
        ]
    },
    {
        "id": 288352391,
        "title": "Concreteness, imageability, subjective frequency and emotionality ratings for 866 words",
        "abstract": "This article presents concreteness, imageability, subjective and emotional valence norms for a set of 866 words. The norms were collected from 97 participants, all French native speakers whose mean age was 23. Descriptive statistics and correlational analyses have been performed on these norms and on other estimated (age of acquistion, conceptual familiarity, image variability) and objective (word frequency) published indexes. The correlational analyses reveal the composite nature of certain estimations. In particular, subjective frequency and imageability are highly correlated with the other variables. Concreteness is highly correlated only with imageability and emotional valence, the latter which is weakly correlated with the other variables. Researchers must have at their disposal such information in order to select stimuli in factorial studies or to perform multiple regression analyses.",
        "date": "October 2003",
        "authors": [
            "Patrick Bonin",
            "Alain M\u00e9ot",
            "L Aubert",
            "N Malardier"
        ],
        "references": [
            278999957,
            247513507,
            247496987,
            232539402,
            232519827,
            232466076,
            232274318,
            226127776,
            225680837,
            49846168
        ]
    },
    {
        "id": 285014368,
        "title": "The pilot establishment and evaluation of Chinese affective words system",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "Y.N. Wang",
            "L.M. Zhou",
            "Y.J. Luo"
        ],
        "references": []
    },
    {
        "id": 283544666,
        "title": "Spanish norms for affective and lexico-semantic variables for 1,400 words",
        "abstract": "Studies of semantic variables (e.g., concreteness) and affective variables (i.e., valence and arousal) have traditionally tended to run in different directions. However, in recent years there has been growing interest in studying the relationship, as well as the potential overlaps, between the two. This article describes a database that provides subjective ratings for 1,400 Spanish words for valence, arousal, concreteness, imageability, context availability, and familiarity. Data were collected online through a process involving 826 university students. The results showed a high interrater reliability for all of the variables examined, as well as high correlations between our affective and semantic values and norms currently available in other Spanish databases. Regarding the affective variables, the typical quadratic correlation between valence and arousal ratings was obtained. Likewise, significant correlations were found between the lexico-semantic variables. Importantly, we obtained moderate negative correlations between emotionality and both concreteness and imageability. This is in line with the claim that abstract words have more affective associations than concrete ones (Kousta, Vigliocco, Vinson, Andrews, & Del Campo, 2011). The present Spanish database is suitable for experimental research into the effects of both affective properties and lexico-semantic variables on word processing and memory.",
        "date": "November 2015",
        "authors": [
            "Marc Guasch",
            "Pilar Ferr\u00e9",
            "Isabel Fraga"
        ],
        "references": [
            280145093,
            273137309,
            263499701,
            263205964,
            260064279,
            259700733,
            259456304,
            258442701,
            258336106,
            258170302
        ]
    },
    {
        "id": 277086910,
        "title": "Simultaneous EEG\u2013fMRI reveals brain networks underlying recognition memory ERP old/new effects",
        "abstract": "",
        "date": "May 2015",
        "authors": [
            "Michael Hoppst\u00e4dter",
            "Christian B\u00e4uchl",
            "Carsten Diener",
            "Herta Flor"
        ],
        "references": [
            257072385,
            233827675,
            233767579,
            232487566,
            231225380,
            230685210,
            228102852,
            224888750,
            224878030,
            200772647
        ]
    },
    {
        "id": 319770438,
        "title": "Long-Term Recurrent Convolutional Networks for Visual Recognition and Description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",
        "date": "November 2014",
        "authors": [
            "Jeff Donahue",
            "Lisa Anne Hendricks",
            "Sergio Guadarrama",
            "Marcus Rohrbach"
        ],
        "references": [
            308809117,
            329977566,
            319770465,
            319770291,
            319770183,
            313060232,
            310752533,
            308859819,
            308813785,
            308804607
        ]
    },
    {
        "id": 319770133,
        "title": "Colorful Image Colorization",
        "abstract": "Given a grayscale photograph as input, this paper attacks the problem of hallucinating a $backslash$em plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \"colorization Turing test\", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20$backslash$% of the time, significantly higher than previous methods.",
        "date": "March 2016",
        "authors": [
            "Richard Zhang",
            "Phillip Isola",
            "Alexei Efros"
        ],
        "references": [
            305196650,
            302305068,
            265787949,
            265295439,
            262353941,
            240308781,
            220659463,
            220184465,
            220183710,
            42345292
        ]
    },
    {
        "id": 311610098,
        "title": "Visually Indicated Sounds",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Andrew Owens",
            "Phillip Isola",
            "Josh Mcdermott",
            "Antonio Torralba"
        ],
        "references": [
            319770438,
            308034527,
            290263673,
            279839496,
            279408623,
            277023095,
            276211157,
            275974132,
            274902700,
            264979485
        ]
    },
    {
        "id": 311609232,
        "title": "Instance-Aware Semantic Segmentation via Multi-task Network Cascades",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Jian Sun"
        ],
        "references": [
            268689640,
            264979485,
            262270555,
            260350533,
            51855547,
            34287664,
            319770430,
            319770420,
            319770284,
            319770272
        ]
    },
    {
        "id": 311609147,
        "title": "Human Pose Estimation with Iterative Error Feedback",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "J. Carreira",
            "Pulkit Agrawal",
            "Katerina Fragkiadaki",
            "Jitendra Malik"
        ],
        "references": [
            308823120,
            304409764,
            278048481,
            275587940,
            269332682,
            268689640,
            264123240,
            263891809,
            263811780,
            263048918
        ]
    },
    {
        "id": 275449771,
        "title": "Special issue: Difference and globalization",
        "abstract": "Our original aspiration for this special issue was to attract a broad base of visual communication scholars working on the nexus of difference and globalization, with the aim of defining the substance and assessing the significance of this particular dialectic in our field. While globalization does entail the ever-growing significance of deterritorialized practices and transcultural flows, these connections, movements and exchanges still largely occur across specific locales and identities, and through appeals to various dimensions of cultural and social difference. Purposefully comprehensive in scope, our call for papers led to over 70 proposals that tackled the relationship between globalization and race, ethnicity, gender, sexuality, class and religion in visual communication from a number of theoretical angles, including but not limited to diasporic, queer, postcolonial, feminist and intercultural perspectives.Taken together, the seven contributions included in this special issue address questions related to the integration and deployment of major dimensions of social and cultural difference in visual communication materials; the perspectives and practices of designers, image-makers and media producers in relation to the work involved in the planning and creation of such materials; and both the dominant ways of seeing and unique experiences that impact on the visual \u2018reading\u2019 of globalization. A combination of well-known and emerging scholars makes for an unusually energetic take on concepts and concerns that underlie several of the major frameworks that have become established in the inherently interdisciplinary field of visual communication, including multimodal and critical discourse analysis, social semiotics, rhetorical criticism, visual anthropology and visual sociology.",
        "date": "July 2014",
        "authors": [
            "Giorgia Aiello",
            "Luc Pauwels"
        ],
        "references": [
            272558986,
            258131476,
            313657328,
            291776029,
            291117156,
            281562557,
            275449590,
            263564110,
            263537728,
            249726053
        ]
    },
    {
        "id": 270720775,
        "title": "The \u201cother\u201d Europeans: The semiotic imperative of style in Euro Visions by Magnum Photos",
        "abstract": "In this article, the author examines Euro Visions, the exhibition created by Magnum Photos to portray the new countries that joined the European Union in 2004 and 2007. She begins by observing that this project's deviations from the world-leading agency's trademark humanist style of photography were discursively ascribed to Euro Visions photographers' authorial style. In this regard, she identifies two key semiotic resources - typing and juxtaposition - that were mobilized as markers of individual style. She then argues that both typing and juxtaposition should instead be seen as generic semiotic resources rooted in corporate styles of visual communication, which contribute to othering the 'new' Europeans. She also argues that in Euro Visions, the notion of 'distinctive' authorial style was deployed as symbolic currency for a global(ist) market that rewards cultural production and, broadly, aestheticization. She finally posits that, in projects like Euro Visions, what is mostly (generic) design may get passed off as (specific) representation, and that this aestheticization of styles and identities may be mystified as the substantial honouring of difference and diversity.",
        "date": "January 2012",
        "authors": [
            "Giorgia Aiello"
        ],
        "references": [
            323321776,
            319484366,
            316564042,
            313737477,
            313185396,
            313157966,
            303548149,
            290846577,
            290485465,
            284943294
        ]
    },
    {
        "id": 263678467,
        "title": "What is multimodal critical discourse studies? INTRODUCTION",
        "abstract": "",
        "date": "November 2013",
        "authors": [
            "David Machin"
        ],
        "references": [
            304047731,
            299534820,
            257198821,
            249935024,
            287118167,
            285680634,
            285088557,
            285070594,
            281562395,
            276935749
        ]
    },
    {
        "id": 311788750,
        "title": "Legitimising immigration control: A discourse-historical analysis",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "T. van Leeuwen",
            "Ruth Wodak"
        ],
        "references": []
    },
    {
        "id": 293428138,
        "title": "Beyond the Image Bank: Digital Commercial Photography",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "Paul Frosh"
        ],
        "references": []
    },
    {
        "id": 293427464,
        "title": "The Image Factory: Consumer Culture, Photography and the Visual Content Industry",
        "abstract": "Quietly but implacably, powerful transnational corporations are gaining power over our visual world. A 'global, visual content industry' increasingly controls images supplied to advertisers, marketers and designers, yet so far the process has, paradoxically, evaded the public eye. This book is the first to expose the interior workings of the visual content industry, which produces approximately 70% of the images that define consumer cultures. The corporate acquisition of major photographic and film archives, as well as the digital rights to much of the world's fine art, is having a profound effect on what we see. From stock photography to new technologies, this book powerfully engages with the historical and cultural issues relating to visual culture and new media. How has stock photography, the system of \u2018renting out' ready-made images, transformed the role of marketing and advertising? What impact are digital technologies having on the practices of industry professionals? How have software programs such as Photoshop enabled professionals to play \u2018God' with photographs and how does this influence our belief in the integrity of images? Combining original research on stock photography with a new theoretical take on the circulation of images in contemporary culture, The Image Factory provides a comprehensive and in-depth exploration of industrialized commercial photography, its uses and abuses.",
        "date": "November 2003",
        "authors": [
            "Paul Frosh"
        ],
        "references": []
    },
    {
        "id": 276935749,
        "title": "Reading Images: A Grammar of Visual Design",
        "abstract": "",
        "date": "May 2006",
        "authors": [
            "Gunther Kress",
            "Theodoor Jacob van Leeuwen"
        ],
        "references": []
    },
    {
        "id": 275449590,
        "title": "The semiotics of texture: From tactile to visual",
        "abstract": "The term 'texture' is often applied beyond the tactile, to describe visual and aural qualities. While tactile, visual and aural texture have been studied separately in various fields, the relationships between them remain largely unexplored. To address this gap, this article proposes parameters for describing tactile surface texture and visual texture, and compares their meaning-making potential. The authors argue that, as new technologies increasingly limit the role of tactile experience and expand the importance of the visual, there is a growing need to study the influence of ubiquitous technologies on our use and understanding of the semiotic potential of resources such as texture. They hypothesize about this influence by reviewing the presentation of texture as a fill option for shapes and backgrounds in Microsoft PowerPoint for Windows from 1992 to 2007.",
        "date": "October 2011",
        "authors": [
            "Emilia Djonov",
            "Theodoor Jacob van Leeuwen"
        ],
        "references": [
            265203737,
            237623450,
            343378091,
            338529424,
            318494453,
            299534493,
            288718742,
            281562395,
            279439297,
            276935749
        ]
    },
    {
        "id": 274330635,
        "title": "Recuperating Feminism, Reclaiming Femininity: Hybrid Postfeminist I-dentity in Consumer Advertisements",
        "abstract": "Traditional gender stereotypes of women have been a common feature of consumer advertisements. In response to feminist criticisms against these stereotypical representations, as well as in recognition of a rising population of female wage earners in industrial societies, advertisers have 'updated' their representations of women as strong, confident and autonomous. This article analyses the construction of this 'postfeminist' femininity in print jewellery advertisements in Singapore, with the aim of showing how it is constituted in terms of a hybrid mix of elements derived from feminism and normative femininity, underscored by neoliberal and consumerist ideologies. Undertaking a multimodal analysis informed by a feminist critical discourse perspective, the article examines, in particular, the emergence of a distinctly pronounced sense of self or 'I-dentity' that is characteristic of postfeminist discourse. It is argued that through this hybrid postfeminist I-dentity, advertisers have found a way to reinstall a new normativity that coexists with the status quo.",
        "date": "June 2014",
        "authors": [
            "Michelle Lazar"
        ],
        "references": [
            258131607,
            27225228,
            350653230,
            321505002,
            316190996,
            310517405,
            307981786,
            300458774,
            260509136,
            255660384
        ]
    },
    {
        "id": 242206025,
        "title": "Building the World's Visual Language: The Increasing Global Importance of Image Banks in Corporate Media",
        "abstract": "Many of the images we now find in magazines, news, promotional material and advertisements are bought cheaply from image banks like Getty Images, which can be accessed by people all around the world. These images are technically of high quality. They have bright lighting and flat colours; attractive models are highly posed and are set in non-descript locations to make them usable across the world. They do not represent actual places or events and they do not document or bear witness, but they symbolically represent marketable concepts and moods such as 'contentment' and 'freedom'. The world in magazines and other similar media therefore comes to resemble the limited world of the image bank categories, which are based on marketing categories. This is therefore an ideologically pre-structured world which is in harmony with consumerism.",
        "date": "October 2004",
        "authors": [
            "David Machin"
        ],
        "references": [
            313157966,
            288911630,
            276935749,
            256599838,
            246091154,
            244425717,
            208574272,
            200026627,
            49551152,
            37688535
        ]
    },
    {
        "id": 319770353,
        "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering",
        "abstract": "In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long-Short Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, a LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 120,000 images and 250,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset are evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human).",
        "date": "May 2015",
        "authors": [
            "Haoyuan Gao",
            "Junhua Mao",
            "Jie Zhou",
            "Zhiheng Huang"
        ],
        "references": []
    },
    {
        "id": 310828080,
        "title": "Ask your neurons: A neural-based approach to answering questions about images",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "M. Fritz"
        ],
        "references": []
    },
    {
        "id": 310439564,
        "title": "Describing objects by their attributes",
        "abstract": "",
        "date": "June 2009",
        "authors": [
            "Ali Farhadi",
            "I. Endres",
            "Derek Hoiem",
            "D. Forsyth"
        ],
        "references": [
            242366361,
            227943302,
            221368798,
            221361988,
            34287664,
            281327886,
            248516115,
            247790139,
            239295682,
            224744237
        ]
    },
    {
        "id": 308871896,
        "title": "Image retrieval using scene graphs",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Justin Johnson",
            "Ranjay Krishna",
            "Michael Stark",
            "Li-Jia Li"
        ],
        "references": [
            279751103,
            275407044,
            273327809,
            268294470,
            265295439,
            264979485,
            264975659,
            264975444,
            262270555,
            261448914
        ]
    },
    {
        "id": 298435975,
        "title": "L'acquisition et usage de scripts dans les diff\u00e9rentes langues de multilingues adultes [Acquisition and use of scripts in the different languages of multilingual adults]",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "Jean-Marc Dewaele"
        ],
        "references": [
            324972279,
            304258504,
            300836396,
            346287199,
            324972221,
            324337273,
            313725788,
            313220254,
            300852434,
            300478879
        ]
    },
    {
        "id": 291836445,
        "title": "The use of web questionnaires in second language acquisition and bilingualism research",
        "abstract": "The present article focuses on data collection through web questionnaires, as opposed to the traditional pen-and-paper method for research in second language acquisition and bilingualism. It is argued that web questionnaires, which have been used quite widely in psychology, have the advantage of reaching out to a larger and more diverse pool of potential participants, which may increase the ecological validity of the resulting database. After considering some issues raised in debates on the strengths and weaknesses of traditional approaches to data collection through questionnaires as opposed to web-based questionnaires, we present two case studies of research designs based on online questionnaires, that is, the bilingualism and emotions questionnaire (Dewaele and Pavlenko, 2001/03) and the feelings questionnaire (Wilson, 2008). We reflect on the issue of participant self-selection and conclude that the potential benefits of web-based questionnaires can outweigh their limitations.",
        "date": "January 2010",
        "authors": [
            "Rosemary Wilson",
            "Jean-Marc Dewaele"
        ],
        "references": [
            313007356,
            298028802,
            289251258,
            324973964,
            324972318,
            324337273,
            292243550,
            288327989,
            287248121,
            277501844
        ]
    },
    {
        "id": 291835354,
        "title": "Sociolinguistics and second language acquisition",
        "abstract": "Book synopsis: From its beginnings in the 1960s, sociolinguistics developed several different subfields with distinct methods and interests: the variationist tradition established by Labov, the anthropological tradition of Hymes, interactional sociolinguistics as developed by Gumperz, and the sociology of language represented by the work of Fishman. All of these areas have seen a great deal of growth in recent decades, and recent studies have led to a more broadly inclusive view of sociolinguistics. Hence there is a need for a handbook that will survey the main areas of the field, point out the lacunae in our existing knowledge base, and provide directions for future research. The Oxford Handbook of Sociolinguistics will differ from existing work in four major respects. First, it will emphasize new methodological developments, particularly the convergence of linguistic anthropology and variationist sociolinguistics. Second, it will include chapters on sociolinguistic developments in areas of the world that have been relatively neglected in the major journals. Third, its chapters are written by contributors who have worked in a range of languages and whose work addresses sociolinguistic issues in bi- and multilingual contexts, i.e. the contexts in which a majority of the world's population lives. Finally, it will include substantial material on the rapidly growing study of sign language sociolinguistics.",
        "date": "January 2013",
        "authors": [
            "M. Howard",
            "Raymond Mougeon",
            "Jean-Marc Dewaele"
        ],
        "references": []
    },
    {
        "id": 285760188,
        "title": "Pragmatic Competence: The Case of Hedging",
        "abstract": "This chapter looks at hedging as an aspect of pragmatic competence. After describing the evolution of the concept of hedging from 1972 to the present day, it reviews the principal properties of hedges and discusses the relationship between hedging and the discourse effects of vagueness, evasion, equivocation, and politeness. The final part of the chapter presents examples of hedging from a range of sources, illustrating the ubiquity of the concept. \u00a9 2010 by Emerald Group Publishing Limited. All rights reserved.",
        "date": "January 2010",
        "authors": [
            "Bruce Fraser"
        ],
        "references": [
            288907460,
            288256546,
            247046558
        ]
    },
    {
        "id": 282052407,
        "title": "The Automatic Activation of Emotion and Emotion-Laden Words: Evidence from a Masked and Unmasked Priming Paradigm",
        "abstract": "A primed lexical decision task (lot) was used to determine whether emotion (e.g., love, fear) and emotion-laden (e.g., puppy, hospital) word processing differs, both explicitly and implicitly. Previous experiments have investigated how emotion word processing differs from both abstract and concrete word processing (Altarriba&Bauer, 2004; Altarriba, Bauer,&Benvenuto, 1999). To assess for differences between emotion and emotion- I aden word processing, 2 experiments were conducted, the first assessing explicit processing (using an unmasked LDT) and the second assessing automatic processing (using a masked LDT). The prediction that semantic priming would differ between emotion word pairs and emotion-laden word pairs was confirmed in both experiments, with shorter response times for emotion targets and greater priming effects for emotion word pairs than for emotion-laden word pairs. The role of valence is discussed, emphasizing the ways valence affects the speed with which these words are accessed and processed.",
        "date": "August 2015",
        "authors": [
            "Stephanie Kazanas",
            "Jeanette Altarriba"
        ],
        "references": [
            272122644,
            263499701,
            247514618,
            247497149,
            233212412,
            231890315,
            224950738,
            284099846,
            281577076,
            232581827
        ]
    },
    {
        "id": 313369235,
        "title": "'Swearing'",
        "abstract": "As a taboo activity, swearing is \u201cforbidden\ufffd? and hence carries the risk of censure. However, many people regularly use \u201cswear-words\ufffd? (expletives) in their everyday lives. Hence, it must be assumed that swearing fulfils particular communicative functions which are not easily accomplished through other linguistic means. Like other forms of speech activity, the functions of swearing are highly dependent on context, which includes social norms, formality, status differentials and social expectations related to speaker categories; in particular, gender and socioeconomic class. In light of these considerations, this chapter discusses the interpersonal functions of swearing under four main headings: Expressing emotion; humour and verbal emphasis; social bonding and solidarity; and constructing and displaying identity. Throughout, the discussion emphasises the nuanced and context-specific nature of swearing as an interpersonal activity.",
        "date": "January 2010",
        "authors": [
            "Karyn Stapleton"
        ],
        "references": []
    },
    {
        "id": 298435974,
        "title": "Swear word offensiveness",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "K. Beers-F\u00e4gersten"
        ],
        "references": []
    },
    {
        "id": 285441683,
        "title": "The pragmatics of swearing,\" Journal of Politeness Research",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "T. Jay",
            "K. Janschewitz"
        ],
        "references": []
    },
    {
        "id": 284618054,
        "title": "More on women's and men's expletives",
        "abstract": "",
        "date": "January 1976",
        "authors": [
            "L.A. Bailey",
            "L.A. Timm"
        ],
        "references": []
    },
    {
        "id": 283226486,
        "title": "British 'Bollocks' versus American 'Jerk': Do native British English speakers swear more - or differently - compared to American English speakers?",
        "abstract": "The present study investigates the differences between 414 L1 speakers of British and 556 L1 speakers of American English in self-reported frequency of swearing and in the understanding of the meaning, the perceived offensiveness and the frequency of use of 30 negative words extracted from the British National Corpus. Words ranged from mild to highly offensive, insulting and taboo. Statistical analysies revealed no significant differences between the groups in self reported frequency of swearing. The British English L1 participants reported a significantly better understanding of nearly half the chosen words from the corpus. They gave significantly higher offensiveness scores to four words (including \"bollocks\") while the American English L1 participants rated a third of words as significantly more offensive (including \"jerk\"). British English L1 participants reported significantly more frequent use of a third of words (including \"bollocks\") while the American English L1 participants reported more frequent use of half of the words (including \"jerk\"). This is interpreted as evidence of differences in semantic and conceptual representations of these words in both variants of English.",
        "date": "September 2015",
        "authors": [
            "J.-M. Dewaele"
        ],
        "references": [
            295853816,
            291836445,
            282730425,
            265289850,
            233620165,
            231890315,
            228084852,
            223956129,
            36725915,
            350683691
        ]
    },
    {
        "id": 283532090,
        "title": "The Variational Fair Auto Encoder",
        "abstract": "We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational auto-encoding architecture with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the ``Maximum Mean Discrepancy'' (MMD) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations.",
        "date": "November 2015",
        "authors": [
            "Christos Louizos",
            "Kevin Swersky",
            "Yujia Li",
            "Max Welling"
        ],
        "references": [
            277333816,
            319770229,
            319770134,
            272194892,
            272194743,
            269935079,
            269721958,
            269416972,
            263316327,
            263056656
        ]
    },
    {
        "id": 277333816,
        "title": "Domain-Adversarial Training of Neural Networks",
        "abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directlyinspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
        "date": "May 2015",
        "authors": [
            "Yaroslav Gani",
            "Evgeniya Ustinova",
            "Hana Ajakan",
            "Pascal Germain"
        ],
        "references": [
            319770355,
            310752533,
            308872817,
            308820677,
            286794765,
            281886334,
            273258920,
            272194892,
            271533315,
            269416972
        ]
    },
    {
        "id": 319649103,
        "title": "Domain-Adversarial Training of Neural Networks",
        "abstract": "We introduce a representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behavior can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new Gradient Reversal Layer. The resulting augmented architecture can be trained using standard backpropagation, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for image classification, where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
        "date": "September 2017",
        "authors": [
            "Yaroslav Ganin",
            "Evgeniya Ustinova",
            "Hana Ajakan",
            "Pascal Germain"
        ],
        "references": [
            273158217,
            269280488,
            264979485,
            264084796,
            261989633,
            258218677,
            258218673,
            257927730,
            257581963,
            256119403
        ]
    },
    {
        "id": 311699861,
        "title": "Processing images by semi-linear predictability minimization",
        "abstract": "In the predictability minimization approach, input patterns are fed into a system consisting of adaptive, initially unstructured feature detectors. There are also adaptive predictors constantly trying to predict current feature detector outputs from other feature detector outputs. Simultaneously, however, the feature detectors try to become as unpredictable as possible, resulting in a co-evolution of predictors and feature detectors. This paper describes the implementation of a visual processing system trained by semi-linear predictability minimization, and presents many experiments that examine its response to artificial and real-world images. In particular, we observe that under a wide variety of conditions, predictability minimization results in the development of well-known visual feature detectors.",
        "date": "January 1999",
        "authors": [
            "Nicol N. Schraudolph",
            "Martin Eldracher",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            304541412,
            220500193,
            220499594,
            220478390,
            14916542,
            13234654,
            13206493,
            3361335,
            2773290,
            2690080
        ]
    },
    {
        "id": 289561252,
        "title": "Learning fair representations",
        "abstract": "We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.",
        "date": "January 2013",
        "authors": [
            "Richard Zemel",
            "Y. Wu",
            "K. Swersky",
            "T. Pitassi"
        ],
        "references": [
            221654695,
            220766348,
            220451718,
            254463371,
            221654559,
            51957869
        ]
    },
    {
        "id": 278733352,
        "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
        "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.",
        "date": "June 2015",
        "authors": [
            "Emily Denton",
            "Soumith Chintala",
            "Arthur Szlam",
            "Rob Fergus"
        ],
        "references": [
            263012109,
            255564387,
            224134574,
            221364240,
            221361415,
            221346269,
            41224942,
            3077959,
            2441269,
            319770355
        ]
    },
    {
        "id": 269935192,
        "title": "Generative Class-conditional Autoencoders",
        "abstract": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.",
        "date": "December 2014",
        "authors": [
            "Jan Rudy",
            "Graham Taylor"
        ],
        "references": [
            259399973,
            236955042,
            221620570,
            221491058,
            221346269,
            220320676,
            319770355,
            319770229,
            267960550,
            247931959
        ]
    },
    {
        "id": 280530166,
        "title": "Unconstrained Face Detection: State of the Art Baseline and Challenges",
        "abstract": "",
        "date": "May 2015",
        "authors": [
            "Jordan Cheney",
            "Ben Klein",
            "Anil K. Jain",
            "Brendan Klare"
        ],
        "references": [
            280530043,
            265125544,
            319770820,
            286594490,
            286553396,
            285671424,
            281327886,
            266298783,
            263564119,
            255126020
        ]
    },
    {
        "id": 269326213,
        "title": "Open source biometric recognition",
        "abstract": "The biometrics community enjoys an active research field that has produced algorithms for several modalities suitable for real-world applications. Despite these developments, there exist few open source implementations of complete algorithms that are maintained by the community or deployed outside a laboratory environment. In this paper we motivate the need for more community-driven open source software in the field of biometrics and present OpenBR as a candidate to address this deficiency. We overview the OpenBR software architecture and consider still-image frontal face recognition as a case study to illustrate its strengths and capabilities. All of our work is available at www.openbiometrics.org.",
        "date": "September 2013",
        "authors": [
            "Joshua C Klontz",
            "Brendan Klare",
            "Scott Klum",
            "Anil Kumar Jain"
        ],
        "references": [
            317083802,
            261496196,
            260299859,
            232653506,
            322276713,
            317219350,
            299197766,
            281327990,
            261421326,
            256075302
        ]
    },
    {
        "id": 265125544,
        "title": "Face Detection without Bells and Whistles",
        "abstract": "Face detection is a mature problem in computer vision. While diverse high performing face detectors have been proposed in the past, we present two surprising new top performance results. First, we show that a properly trained vanilla DPM reaches top performance, improving over commercial and research systems. Second, we show that a detector based on rigid templates-similar in structure to the Viola&Jones detector-can reach similar top performance on this task. Importantly, we discuss issues with existing evaluation benchmark and propose an improved procedure.",
        "date": "September 2014",
        "authors": [
            "Markus Mathias",
            "Rodrigo Benenson",
            "Marco Pedersoli",
            "Luc Van Gool"
        ],
        "references": [
            280608596,
            261310266,
            261306330,
            319770820,
            290258609,
            281327886,
            266298783,
            262397182,
            262205050,
            261465317
        ]
    },
    {
        "id": 265051004,
        "title": "Unconstrained Face Recognition: Identifying a Person of Interest From a Media Collection",
        "abstract": "As face recognition applications progress from constrained sensing and cooperative subjects scenarios (e.g., driver\u2019s license and passport photos) to unconstrained scenarios with uncooperative subjects (e.g., video surveillance), new challenges are encountered. These challenges are due to variations in ambient illumination, image resolution, background clutter, facial pose, expression, and occlusion. In forensic investigations where the goal is to identify a \u201cperson of interest,\u201d often based on low quality face images and videos, we need to utilize whatever source of information is available about the person. This could include one or more video tracks, multiple still images captured by bystanders (using, for example, their mobile phones), 3D face models, and verbal descriptions of the subject provided by witnesses. These verbal descriptions can be used to generate a face sketch and provide ancillary information about the person of interest (e.g., gender, race, and age). While traditional face matching methods take single media (i.e., a still face image, video track, or face sketch) as input, our work considers using the entire gamut of media collection as a probe to generate a single candidate list for the person of interest. We show that the proposed approach boosts the likelihood of correctly identifying the person of interest through the use of different fusion schemes, 3D face models, and incorporation of quality measures for fusion and video frame selection.",
        "date": "September 2014",
        "authors": [
            "Lacey Best-Rowden",
            "Hu Han",
            "Charles Otto",
            "Brendan Klare"
        ],
        "references": [
            303261846,
            283749931,
            262327040,
            312771295,
            301951817,
            286651051,
            269328754,
            269250424,
            265664891,
            263564119
        ]
    },
    {
        "id": 263697736,
        "title": "Unconstrained Face Recognition: Identifying a Person of Interest from a Media Collection",
        "abstract": "As face recognition applications progress from constrained sensing and cooperative subjects scenarios (e.g., driver\u2019s license and passport photos) to unconstrained scenarios with uncooperative subjects (e.g., video surveillance), new challenges are encountered. These challenges are due to variations in ambient illumination, image resolution, background clutter, facial pose, expression, and occlusion. In forensic investigations where the goal is to identify a \u201cperson of interest,\u201d often based on low quality face images and videos, we need to utilize whatever source of information is available about the person. This could include one or more video tracks, multiple still images captured by bystanders (using, for example, their mobile phones), 3D face models, and verbal descriptions of the subject provided by witnesses. These verbal descriptions can be used to generate a face sketch and provide ancillary information about the person of interest (e.g., gender, race, and age). While traditional face matching methods take single media (i.e., a still face image, video track, or face sketch) as input, our work considers using the entire gamut of media collection as a probe to generate a single candidate list for the person of interest. We show that the proposed approach boosts the likelihood of correctly identifying the person of interest through the use of different fusion schemes, 3D face models, and incorporation of quality measures for fusion and video frame selection.",
        "date": "March 2014",
        "authors": [
            "Lacey Best-Rowden",
            "Hu Han",
            "Charles Otto",
            "Brendan Klare"
        ],
        "references": [
            303261846,
            283749931,
            262327040,
            312771295,
            301951817,
            286651051,
            269328754,
            269250424,
            263564119,
            262168080
        ]
    },
    {
        "id": 261392568,
        "title": "The challenge of face recognition from digital point-and-shoot cameras",
        "abstract": "Inexpensive \u201cpoint-and-shoot\u201d camera technology has combined with social network technology to give the general population a motivation to use face recognition technology. Users expect a lot; they want to snap pictures, shoot videos, upload, and have their friends, family and acquaintances more-or-less automatically recognized. Despite the apparent simplicity of the problem, face recognition in this context is hard. Roughly speaking, failure rates in the 4 to 8 out of 10 range are common. In contrast, error rates drop to roughly 1 in 1,000 for well controlled imagery. To spur advancement in face and person recognition this paper introduces the Point-and-Shoot Face Recognition Challenge (PaSC). The challenge includes 9,376 still images of 293 people balanced with respect to distance to the camera, alternative sensors, frontal versus not-frontal views, and varying location. There are also 2,802 videos for 265 people: a subset of the 293. Verification results are presented for public baseline algorithms and a commercial algorithm for three cases: comparing still images to still images, videos to videos, and still images to videos.",
        "date": "September 2013",
        "authors": [
            "J. Ross Beveridge",
            "P. Jonathon Phillips",
            "David S. Bolme",
            "Bruce A. Draper"
        ],
        "references": [
            241266973,
            227036748,
            224605637,
            224266483,
            224068276,
            266341964,
            266328563,
            261421326,
            224254875,
            221292234
        ]
    },
    {
        "id": 286651051,
        "title": "A benchmark study of large-scale unconstrained face recognition",
        "abstract": "Many efforts have been made in recent years to tackle the unconstrained face recognition challenge. For the benchmark of this challenge, the Labeled Faces in theWild (LFW) database has been widely used. However, the standard LFW protocol is very limited, with only 3,000 genuine and 3,000 impostor matches for classification. Today a 97% accuracy can be achieved with this benchmark, remaining a very limited room for algorithm development. However, we argue that this accuracy may be too optimistic because the underlying false accept rate may still be high (e.g. 3%). Furthermore, performance evaluation at low FARs is not statistically sound by the standard protocol due to the limited number of impostor matches. Thereby we develop a new benchmark protocol to fully exploit all the 13,233 LFW face images for large-scale unconstrained face recognition evaluation under both verification and open-set identification scenarios, with a focus at low FARs. Based on the new benchmark, we evaluate 21 face recognition approaches by combining 3 kinds of features and 7 learning algorithms. The benchmark results show that the best algorithm achieves 41.66% verification rates at FAR=0.1%, and 18.07% open-set identification rates at rank 1 and FAR=1%. Accordingly we conclude that the large-scale unconstrained face recognition problem is still largely unresolved, thus further attention and effort is needed in developing effective feature representations and learning algorithms. We thereby release a benchmark tool to advance research in this field.",
        "date": "September 2014",
        "authors": [
            "Shengcai Liao",
            "Zhen Lei",
            "Dong Yi",
            "Stan Z Li"
        ],
        "references": [
            317083802,
            265051004,
            236593328,
            224396157,
            224164307,
            220182402,
            210341989,
            302991451,
            265464183,
            263564119
        ]
    },
    {
        "id": 280530237,
        "title": "Annotating Unconstrained Face Imagery: A Scalable Approach",
        "abstract": "As unconstrained face recognition datasets progress from containing faces that can be automatically detected by commodity face detectors to face imagery with full pose variations that must instead be manually localized, a significant amount of annotation effort is required for developing benchmark datasets. In this work we describe a systematic approach for annotating fully unconstrained face imagery using crowdsourced labor. For such data preparation, a cascade of crowdsourced tasks are performed, which begins with bounding box annotations on all faces contained in images and videos, followed by identification of the labelled person of interest in such imagery, and, finally, landmark annotation of key facial fiducial points. In order to allow such annotations to scale to large volumes of imagery, a software system architecture is provided which achieves a sustained rate of 30,000 annotations per hour (or 500 manual annotations per minute). While previous crowdsourcing guidance described in the literature generally involved multiple choice questions or text input, our tasks required annotators to provide geometric primitives (rectangles and points) in images. As such, algorithms are provided for combining multiple annotations of an image into a single result, and automatically measuring the quality of a given annotation. Finally, other guidance is provided for improving the accuracy and scalability of crowdsourced image annotation for face detection and recognition.",
        "date": "May 2015",
        "authors": [
            "Emma Taborsky",
            "Kristen Allen",
            "Austin Blanton",
            "Anil K. Jain"
        ],
        "references": [
            280530043,
            268988012,
            260350275,
            228897427,
            228520105,
            224223972,
            221430174,
            221361415,
            220875904,
            220519959
        ]
    },
    {
        "id": 266298783,
        "title": "FDDB: A Benchmark for Face Detection in Unconstrained Settings",
        "abstract": "Despite the maturity of face detection research, it re-mains difficult to compare different algorithms for face de-tection. This is partly due to the lack of common evaluation schemes. Also, existing data sets for evaluating face detec-tion algorithms do not capture some aspects of face appear-ances that are manifested in real-world scenarios. In this work, we address both of these issues. We present a new data set of face images with more faces and more accurate annotations for face regions than in previous data sets. We also propose two rigorous and precise methods for evaluat-ing the performance of face detection algorithms. We report results of several standard algorithms on the new bench-mark.",
        "date": "December 2010",
        "authors": [
            "Vidit Jain",
            "Erik Learned-Miller"
        ],
        "references": [
            228362107,
            221620223,
            221551729,
            221368819,
            221364881,
            221304792,
            221303855,
            221111859,
            220660094,
            220320307
        ]
    },
    {
        "id": 263564119,
        "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
        "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities, where each identity has an average of over a thousand samples. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.25% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 25%, closely approaching human-level performance.",
        "date": "September 2014",
        "authors": [
            "Yaniv Taigman",
            "Ming Yang",
            "Marc'Aurelio Ranzato",
            "Lior Wolf"
        ],
        "references": [
            261336897,
            224135964,
            215991023,
            45921303,
            4156225,
            2985446,
            290264922,
            269250424,
            267960550,
            266341964
        ]
    },
    {
        "id": 333874695,
        "title": "AI Ethics \u2013 Too Principled to Fail?",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Brent Mittelstadt"
        ],
        "references": [
            338983166,
            337565648,
            329192820,
            328595476,
            352867381,
            351401372,
            335579286,
            332811345,
            332741841,
            332085170
        ]
    },
    {
        "id": 333796160,
        "title": "Chapter 3 Platforms at Work: Automated Hiring Platforms and Other New Intermediaries in the Organization of Work",
        "abstract": "This chapter lays out a research agenda in the sociology of work for a type of data and organizational intermediary: work platforms. As an example, the authors employ a case study of the adoption of automated hiring platforms (AHPs) in which the authors distinguish between promises and existing practices. The authors draw on two main methods to do so: critical discourse analysis and affordance critique. The authors collected and examined a mix of trade, popular press, and corporate archives; 135 texts in total. The analysis reveals that work platforms offer five core affordances to management: (1) structured data fields optimized for capture and portability within organizations; (2) increased legibility of activity qua data captured inside and outside the workplace; (3) information asymmetry between labor and management; (4) an \u201cecosystem\u201d design that supports the development of limited-use applications for specific domains; and (5) the standardization of managerial techniques between workplaces. These combine to create a managerial frame for workers as fungible human capital, available on demand and easily ported between job tasks and organizations. While outlining the origin of platform studies within media and communication studies, the authors demonstrate the specific tools the sociology of work brings to the study of platforms within the workplace. The authors conclude by suggesting avenues for future sociological research not only on hiring platforms, but also on other work platforms such as those supporting automated scheduling and customer relationship management.",
        "date": "July 2019",
        "authors": [
            "Ifeoma Ajunwa",
            "Daniel Greene"
        ],
        "references": [
            323538785,
            316530561,
            333793582,
            330767930,
            327072196,
            323983704,
            319929548,
            318408751,
            315193868,
            315173186
        ]
    },
    {
        "id": 332828046,
        "title": "Data Is the New What? Popular Metaphors & Professional Ethics in Emerging Data Culture",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Luke Stark",
            "Anna Lauren Hoffmann"
        ],
        "references": [
            271525133,
            253095653,
            352867381,
            343492089,
            331489942,
            323325054,
            284320342,
            275263752,
            263597499
        ]
    },
    {
        "id": 332742200,
        "title": "Guidelines for Human-AI Interaction",
        "abstract": "Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.",
        "date": "April 2019",
        "authors": [
            "Saleema Amershi",
            "Kori Inkpen",
            "Jaime Teevan",
            "Ruth Kikin-Gil"
        ],
        "references": [
            324670825,
            320353627,
            312654128,
            304497424,
            322721511,
            322686862,
            316705910,
            314815288,
            308943463,
            305999024
        ]
    },
    {
        "id": 352867381,
        "title": "Values and Pragmatic Action: The Challenges of Introducing Ethical Intelligence in Technical Design Communities",
        "abstract": "relates to notions of power, trust in organizations, and trust in information and information systems as one",
        "date": "February 2009",
        "authors": [
            "No\u00ebmi Manders-Huits",
            "Michael Zimmer"
        ],
        "references": []
    },
    {
        "id": 344708255,
        "title": "Value Sensitive Design: Shaping Technology with Moral Imagination",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Batya Friedman",
            "David G Hendry"
        ],
        "references": []
    },
    {
        "id": 335579286,
        "title": "The global landscape of AI ethics guidelines",
        "abstract": "In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be \u2018ethical\u2019, there is debate about both what constitutes \u2018ethical AI\u2019 and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.",
        "date": "September 2019",
        "authors": [
            "Anna Jobin",
            "Marcello Ienca",
            "Effy Vayena"
        ],
        "references": [
            343265106,
            338983166,
            338271524,
            337947166,
            334378492,
            333874695,
            329192820,
            328773479,
            328491510,
            328292317
        ]
    },
    {
        "id": 333942616,
        "title": "Conceptualizing Care in the Everyday Work Practices of Machine Learning Developers",
        "abstract": "In this provocation, I investigate machine learning (ML) developers' accounts, which highlight situated, ongoing improvisational work practices that strive to appropriately match datasets, algorithms/modelling techniques, and domain questions. In conceptualizing ML developers' work as practices of care, I provide a case to more closely examine relationships which emerge between local innovations and global regimes of scientific formalization and standardization within sociotechnical systems (in this case, for example, between applied ML projects and scholarly algorithm development). In discussing these local/global relations, this provocation also brings two concepts from the ML field itself to bear (concept drift and transfer learning), productively challenging how we conceptualize everyday technical work ? and the configurative practices of care it includes.",
        "date": "June 2019",
        "authors": [
            "Christine T. Wolf"
        ],
        "references": [
            254074269,
            241888510,
            228723141,
            350274568,
            332335512,
            331680953,
            270677988,
            233823602
        ]
    },
    {
        "id": 332771223,
        "title": "Translation, Tracks & Data: an Algorithmic Bias Effort in Practice",
        "abstract": "Potential negative outcomes of machine learning and algorithmic bias have gained deserved attention. However, there are still relatively few standard processes to assess and address algorithmic biases in industry practice. Practical tools that integrate into engineers' workflows are needed. As a case study, we present two tooling efforts to create tools for teams in practice to address algorithmic bias. Both intend to increase understanding of data, models, and outcome measurement decisions. We describe the development of 1) a prototype checklist based on existing literature frameworks; and 2) dashboarding for quantitatively assessing outcomes at scale. We share both technical and organizational lessons learned on checklist perceptions, data challenges and interpretation pitfalls.",
        "date": "April 2019",
        "authors": [
            "Henriette Cramer",
            "Jean Garcia-Gathright",
            "Sravana Reddy",
            "Aaron Springer"
        ],
        "references": [
            327199584,
            324673201,
            324668146,
            317996556,
            317202908,
            303363022,
            239798558,
            2515436
        ]
    },
    {
        "id": 332741841,
        "title": "Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?",
        "abstract": "The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners' needs.",
        "date": "April 2019",
        "authors": [
            "Kenneth Holstein",
            "Jennifer Wortman Vaughan",
            "Hal Daum\u00e9",
            "Miro Dudik"
        ],
        "references": [
            334843768,
            330264946,
            328789894,
            328105065,
            326391140,
            326304068,
            326068576,
            325557708,
            325448885,
            324670607
        ]
    },
    {
        "id": 330297860,
        "title": "Measuring and Mitigating Unintended Bias in Text Classification",
        "abstract": "We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",
        "date": "December 2018",
        "authors": [
            "Lucas Dixon",
            "John Li",
            "Jeffrey Sorensen",
            "Nithum Thain"
        ],
        "references": [
            308327297,
            306093490,
            305615978,
            318740171,
            318129802,
            318128810,
            315873012,
            308980568,
            308610093,
            269416801
        ]
    },
    {
        "id": 326048577,
        "title": "IEEE P7003\u2122 standard for algorithmic bias considerations: work in progress paper",
        "abstract": "The IEEE P7003 Standard for Algorithmic Bias Considerations is one of eleven IEEE ethics related standards currently under development as part of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. The purpose of the IEEE P7003 standard is to provide individuals or organizations creating algorithmic systems with development framework to avoid unintended, unjustified and inappropriately differential outcomes for users. In this paper, we present the scope and structure of the IEEE P7003 draft standard, and the methodology of the development process.",
        "date": "May 2018",
        "authors": [
            "Ansgar Roald Koene",
            "Liz Dowthwaite",
            "Suchana Seth"
        ],
        "references": [
            317573589
        ]
    },
    {
        "id": 266859800,
        "title": "Qualitative content analysis - theoretical foundation, basic procedures and software solution",
        "abstract": "Qualitative content analysis: theoretical foundation, basic procedures and software solution Mayring, Philipp Erstver\u00f6ffentlichung / Primary Publication Monographie / monograph Empfohlene Zitierung / Suggested Citation: Mayring, Philipp : Qualitative content analysis: theoretical foundation, basic procedures and software solution. Klagenfurt, 2014. URN: http://nbn-resolving.de/urn:nbn:de:0168-ssoar-395173 Nutzungsbedingungen: Dieser Text wird unter einer CC BY-NC-ND Lizenz (Namensnennung-Nicht-kommerziell-Keine Bearbeitung) zur Verf\u00fcgung gestellt. N\u00e4here Ausk\u00fcnfte zu den CC-Lizenzen finden Sie hier: http://creativecommons.org/licenses/ Terms of use: This document is made available under a CC BY-NC-ND Licence (Attribution Non Comercial-NoDerivatives). For more Information see: http://creativecommons.org/licenses/",
        "date": "January 2014",
        "authors": [
            "Philipp Mayring"
        ],
        "references": [
            336209946,
            347561982,
            324333785,
            321507152,
            320705787,
            317953655,
            314860947,
            313658140,
            313390246,
            310513404
        ]
    },
    {
        "id": 325981161,
        "title": "The end of media logics? On algorithms and agency",
        "abstract": "We argue that algorithms are an outcome rather than a replacement of media logics, and ultimately, we advance this argument by connecting human agency to media logics. This theoretical contribution builds on the notion that technology, particularly algorithms are non-neutral, arguing for a stronger focus on the agency that goes into designing and programming them. We reflect on the limits of algorithmic agency and lay out the role of algorithms and agency for the dimensions and elements of network media logic. The article concludes with addressing questions of power, discussing algorithmic agency from both meso and macro perspectives.",
        "date": "June 2018",
        "authors": [
            "Ulrike Klinger",
            "Jakob Svensson"
        ],
        "references": [
            321371928,
            317792421,
            310916151,
            309101946,
            307840661,
            302979999,
            297664844,
            291601944,
            289460839,
            281748849
        ]
    },
    {
        "id": 317996556,
        "title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Alexandra Olteanu",
            "Carlos Castillo",
            "Fernando Diaz",
            "Emre Kiciman"
        ],
        "references": []
    },
    {
        "id": 286813583,
        "title": "Similarity Comparisons for Interactive Fine-Grained Categorization",
        "abstract": "Current human-in-the-loop fine-grained visual categorization systems depend on a predefined vocabulary of attributes and parts, usually determined by experts. In this work, we move away from that expert-driven and attribute-centric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metrics in a unified framework. At test time, users are asked to judge relative similarity between a query image and various sets of images, these general queries do not require expert-defined terminology and are applicable to other domains and basic-level categories, enabling a flexible, efficient, and scalable system for fine-grained categorization with humans in the loop. Our system outperforms existing state-of-the-art systems for relevance feedback-based image retrieval as well as interactive classification, resulting in a reduction of up to 43% in the average number of questions needed to correctly classify an image.",
        "date": "June 2014",
        "authors": [
            "Catherine Wah",
            "Grant Van Horn",
            "Steve Branson",
            "Subhransu Maji"
        ],
        "references": [
            267550510,
            262409300,
            261392079,
            233784966,
            227943302,
            224135964,
            221597784,
            221363513,
            221304729,
            221111053
        ]
    },
    {
        "id": 270960227,
        "title": "A data-frame theory of sensemaking",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Gary Klein",
            "J K Phillips",
            "E L Rall",
            "Deborah A. Peluso"
        ],
        "references": []
    },
    {
        "id": 262358421,
        "title": "How annotation styles influence content and preferences",
        "abstract": "Photo-tagging web sites provide several methods to annotate photographs. In this paper, we study how people use and respond to three different annotation styles: single-word tags, multi-word tags, and comments. We find significant differences in how annotation styles influence the objectivity, descriptiveness, and interestingness of annotations. Although single-word and multi-word tags are not normally differentiated, users prefer multi-word tags for their combination of descriptiveness and succinctness. We also discover that producers and consumers assess annotation styles differently in terms of ease of use, support for different user goals, and amount of effort required, demonstrating that allowing multiple modes of annotation is generally beneficial, as is considering both tag production and consumption.",
        "date": "May 2013",
        "authors": [
            "Justin Cheng",
            "Dan Cosley"
        ],
        "references": [
            246699633,
            224281022,
            221666649,
            221515578,
            221515012,
            221514874,
            221514372,
            221514135,
            221141155,
            221053629
        ]
    },
    {
        "id": 338375532,
        "title": "Data Scientist: The Engineer of the Future",
        "abstract": "Although our capabilities to store and process data have been increasing exponentially since the 1960-ties, suddenly many organizations realize that survival is not possible without exploiting available data intelligently. Out of the blue, \"Big Data\" has become a topic in board-level discussions. The abundance of data will change many jobs across all industries. Moreover, also scientific research is becoming more",
        "date": "January 2014",
        "authors": [
            "Wil Van der Aalst"
        ],
        "references": [
            49826525,
            312596137,
            310514248,
            295397112,
            275289997,
            242411475,
            232279315,
            227458458,
            221936069
        ]
    },
    {
        "id": 326861053,
        "title": "Data care and its politics: designing for local collective data management as a neglected thing",
        "abstract": "In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask \"How to design for data care?\" and \"How to account for the politics of data care in design?\" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.",
        "date": "August 2018",
        "authors": [
            "Karen Baker",
            "Helena Karasti"
        ],
        "references": [
            322270377,
            318758068,
            305727731,
            301389350,
            287729349,
            330332254,
            313196220,
            294444692,
            288374498,
            278712204
        ]
    },
    {
        "id": 326321359,
        "title": "Data infrastructure literacy",
        "abstract": "A recent report from the UN makes the case for \u201cglobal data literacy\u201d in order to realise the opportunities afforded by the \u201cdata revolution\u201d. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed \u2013 which we call \u201cdata infrastructure literacy\u201d. We illustrate this notion with examples of \u201cinventive data practice\u201d from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",
        "date": "July 2018",
        "authors": [
            "Jonathan Gray",
            "Carolin Gerlitz",
            "Liliana Bounegru"
        ],
        "references": [
            330334596,
            324950656,
            323475610,
            322369349,
            320237117,
            319149921,
            346624651,
            346447137,
            338764317,
            325104017
        ]
    },
    {
        "id": 325827444,
        "title": "Transparent to whom? No algorithmic accountability without a critical audience",
        "abstract": "Big data and data science transform organizational decision-making. We increasingly defer decisions to algorithms because machines have earned a reputation of outperforming us. As algorithms become embedded within organizations, they become more influential and increasingly opaque. Those who create algorithms may make arbitrary decisions in all stages of the \u2018data value chain\u2019, yet these subjectivities are obscured from view. Algorithms come to reflect the biases of their creators, can reinforce established ways of thinking, and may favour some political orientations over others. This is a cause for concern and calls for more transparency in the development, implementation, and use of algorithms in public- and private-sector organizations. We argue that one elementary \u2013 yet key \u2013 question remains largely undiscussed. If transparency is a primary concern, then to whom should algorithms be transparent? We consider algorithms as socio-technical assemblages and conclude that without a critical audience, algorithms cannot be held accountable.",
        "date": "June 2018",
        "authors": [
            "Jakko Kemper",
            "Daan Kolkman"
        ],
        "references": [
            345824756,
            319127327,
            318162417,
            316973825,
            316440840,
            346624964,
            330326510,
            329933689,
            317779236,
            317352268
        ]
    },
    {
        "id": 324069323,
        "title": "Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",
        "abstract": "Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (\"non-experts\"). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.",
        "date": "June 2018",
        "authors": [
            "Qian Yang",
            "Jina Suh",
            "Nan-Chen Chen",
            "Gonzalo Ramos"
        ],
        "references": [
            314828392,
            310953254,
            305683740,
            302352981,
            302074431,
            318800125,
            318494167,
            316653858,
            301822555,
            300917975
        ]
    },
    {
        "id": 324016889,
        "title": "Investigating How Experienced UX Designers Effectively Work with Machine Learning",
        "abstract": "Machine learning (ML) plays an increasingly important role in improving a user\u2019s experience. However, most UX practitioners face challenges in understanding ML\u2019s capabilities or envisioning what it might be. We interviewed 13 designers who had many years of experience designing the UX of ML-enhanced products and services. We probed them to characterize their practices. They shared they do not view themselves as ML experts, nor do they think learning more about ML would make them better designers. Instead, our participants appeared to be the most successful when they engaged in ongoing collaboration with data scientists to help envision what to make and when they embraced a data-centric culture. We discuss the implications of these findings in terms of UX education and as opportunities for additional design research in support of UX designers working with ML.",
        "date": "June 2018",
        "authors": [
            "Qian Yang",
            "Alex Scuito",
            "John Zimmerman",
            "Jodi Forlizzi"
        ],
        "references": [
            322643908,
            302352981,
            302074431,
            296836038,
            259823341,
            321740083,
            318494167,
            315843595,
            314808126,
            261959722
        ]
    },
    {
        "id": 336388239,
        "title": "Designing for Appropriation",
        "abstract": "",
        "date": "September 2007",
        "authors": [
            "Alan Dix"
        ],
        "references": [
            241800937,
            221518925,
            221441503,
            221441240,
            220944812,
            200026266,
            2870354,
            2543671,
            2409265,
            221514831
        ]
    },
    {
        "id": 335273310,
        "title": "Socialising Big Data: From concept to practice Socialising Big Data: From concept to practice",
        "abstract": "The working paper is a report on an ESRC-funded project, Socialising Big Data, that sought to address problematic conceptions of Big Data in popular discourse such as the 'data deluge' and the tendency to reduce the term to definitions such as the oft-cited '3 Vs'. Instead, building on how social scientists have conceived of things, methods and data as having social and cultural lives, the project sought to identify the normative, political and technical imperatives and choices that come to shape Big Data at various moments in its social lives. Recognising that Big Data involves distributed practices across a range of fields, the project experimented with collaboratories as a method for bringing together and engaging with practitioners across three different domains-genomics, national statistics and waste management. In this way it explored how relations between data are also simultaneously relations between people and that it is through such relations that a shared literacy and social framework for Big Data can be forged.",
        "date": "February 2015",
        "authors": [
            "Evelyn Ruppert",
            "Penny Harvey",
            "Celia Lury",
            "Adrian Mackenzie"
        ],
        "references": [
            268408093,
            258192453,
            254724925,
            254296668,
            235223193,
            221824516,
            220383586,
            51676200,
            33042435,
            305389454
        ]
    },
    {
        "id": 327565927,
        "title": "Open data integration",
        "abstract": "Open data plays a major role in supporting both governmental and organizational transparency. Many organizations are adopting Open Data Principles promising to make their open data complete, primary, and timely. These properties make this data tremendously valuable to data scientists. However, scientists generally do not have a priori knowledge about what data is available (its schema or content). Nevertheless, they want to be able to use open data and integrate it with other public or private data they are studying. Traditionally, data integration is done using a framework called query discovery where the main task is to discover a query (or transformation) that translates data from one form into another. The goal is to find the right operators to join, nest, group, link, and twist data into a desired form. We introduce a new paradigm for thinking about integration where the focus is on data discovery, but highly efficient internet-scale discovery that is driven by data analysis needs. We describe a research agenda and recent progress in developing scalable data-analysis or query-aware data discovery algorithms that provide high recall and accuracy over massive data repositories.",
        "date": "August 2018",
        "authors": [
            "Ren\u00e9e J. Miller"
        ],
        "references": [
            323375920,
            318845137,
            317352352,
            305662530,
            305446011,
            305333003,
            301878905,
            300337800,
            290997365,
            289382034
        ]
    },
    {
        "id": 326499262,
        "title": "Data Diff: Interpretable, Executable Summaries of Changes in Distributions for Data Wrangling",
        "abstract": "Many analyses in data science are not one-off projects, but are repeated over multiple data samples, such as once per month, once per quarter, and so on. For example, if a data scientist performs an analysis in 2017 that saves a significant amount of money, then she will likely to be asked to perform the same analysis on data from 2018. But more data analyses means more effort spent in data wrangling. We introduce the data diff problem, which attempts to turn this problem into an opportunity. Comparing the repeated data samples against each other, inconsistencies may be indicative of underlying issues in data quality. By analogy to text \\textttdiff, the data diff problem is to find a \"patch\", that is, transformation in a specified domain-specific language, that transforms the data samples so that they are identically distributed. We present a prototype tool for data diff that formalizes the problem as a bipartite matching problem, calibrating its parameters using a bootstrap procedure. The tool is evaluated quantitatively and through a case study on an open government data set.",
        "date": "July 2018",
        "authors": [
            "Charles Sutton",
            "Timothy Hobson",
            "James Geddes",
            "Rich Caruana"
        ],
        "references": [
            319769912,
            307896705,
            257364389,
            2622942,
            320890860,
            283620965,
            272825857,
            268687536,
            267006596,
            266657663
        ]
    },
    {
        "id": 325330277,
        "title": "Bias on the web",
        "abstract": "OUR INHERENT HUMAN tendency of favoring one thing or opinion over another is reflected in every aspect of our lives, creating both latent and overt biases toward everything we see, hear, and do. Any remedy for bias must start with awareness that bias exists; for example, most mature societies raise awareness of social bias through affirmative-action programs, and, while awareness alone does not completely alleviate the problem, it helps guide us toward a solution. Bias on the Web reflects both societal and internal biases within ourselves, emerging in subtler ways. This article aims to increase awareness of the potential effects imposed on us all through bias present in Web use and content. We must thus consider and account for it in the design of Web systems that truly address people's needs.",
        "date": "May 2018",
        "authors": [
            "Ricardo Baeza-Yates"
        ],
        "references": [
            323650280,
            316973825,
            306885285,
            305615978,
            299867310,
            282891179,
            272195361,
            317996556,
            298811552,
            290521809
        ]
    },
    {
        "id": 324641488,
        "title": "Detecting Biased Statements in Wikipedia",
        "abstract": "Quality in Wikipedia is enforced through a set of editing policies and guidelines recommended for Wikipedia editors. Neutral point of view (NPOV) is one of the main principles in Wikipedia, which ensures that for controversial information all possible points of view are represented proportionally. Furthermore, language used in Wikipedia should be neutral and not opinionated. However, due to the large number of Wikipedia articles and its operating principle based on a voluntary basis of Wikipedia editors; quality assurances and Wikipedia guidelines cannot always be enforced. Currently, there are more than 40,000 articles, which are flagged with NPOV or similar quality tags. Furthermore, these represent only the portion of articles for which such quality issues are explicitly flagged by the Wikipedia editors, however, the real number may be higher considering that only a small percentage of articles are of good quality or featured as categorized by Wikipedia. In this work, we focus on the case of language bias at the sentence level in Wikipedia. Language bias is a hard problem, as it represents a subjective task and usually the linguistic cues are subtle and can be determined only through its context. We propose a supervised classification approach, which relies on an automatically created lexicon of bias words, and other syntactical and semantic characteristics of biased statements. We experimentally evaluate our approach on a dataset consisting of biased and unbiased statements, and show that we are able to detect biased statements with an accuracy of 74%. Furthermore, we show that competitors that determine bias words are not suitable for detecting biased statements, which we outperform with a relative improvement of over 20%.",
        "date": "April 2018",
        "authors": [
            "Christoph Hube",
            "Besnik Fetahu"
        ],
        "references": [
            317788264,
            271696228,
            257882504,
            246699633,
            337957026,
            313011545,
            302889970,
            271448049,
            270877818,
            262357682
        ]
    },
    {
        "id": 322209540,
        "title": "Cognitive Biases in Crowdsourcing",
        "abstract": "Crowdsourcing has become a popular paradigm in data curation, annotation and evaluation for many artificial intelligence and information retrieval applications. Considerable efforts have gone into devising effective quality control mechanisms that identify or discourage cheat submissions in an attempt to improve the quality of noisy crowd judgments. Besides purposeful cheating, there is another source of noise that is often alluded to but insufficiently studied: Cognitive biases. This paper investigates the prevalence and effect size of a range of common cognitive biases on a standard relevance judgment task. Our experiments are based on three sizable publicly available document collections and note significant detrimental effects on annotation quality, system ranking and the performance of derived rankers when task design does not account for such biases.",
        "date": "February 2018",
        "authors": [
            "Carsten Eickhoff"
        ],
        "references": [
            315380496,
            310823413,
            310441625,
            305442609,
            287263937,
            285582633,
            316858590,
            309476347,
            309471612,
            305997939
        ]
    },
    {
        "id": 319632874,
        "title": "Modus Operandi of Crowd Workers: The Invisible Role of Microtask Work Environments",
        "abstract": "The ubiquity of the Internet and the widespread proliferation of electronic devices has resulted in flourishing microtask crowdsourcing marketplaces, such as Amazon MTurk. An aspect that has remained largely invisible in microtask crowdsourcing is that of work environments; defined as the hardware and software affordances at the disposal of crowd workers which are used to complete microtasks on crowdsourcing platforms. In this paper, we reveal the significant role of work environments in the shaping of crowd work. First, through a pilot study surveying the good and bad experiences workers had with UI elements in crowd work, we revealed the typical issues workers face. Based on these findings, we then deployed over 100 distinct microtasks on CrowdFlower, addressing workers in India and USA in two identical batches. These tasks emulate the good and bad UI element designs that characterize crowdsourcing microtasks. We recorded hardware specifics such as CPU speed and device type, apart from software specifics including the browsers used to complete tasks, operating systems on the device, and other properties that define the work environments of crowd workers. Our findings indicate that crowd workers are embedded in a variety of work environments which influence the quality of work produced. To confirm and validate our data-driven findings we then carried out semi-structured interviews with a sample of Indian and American crowd workers from this platform. Depending on the design of UI elements in microtasks, we found that some work environments support crowd workers more than others. Based on our overall findings resulting from all the three studies, we introduce ModOp, a tool that helps to design crowdsourcing microtasks that are suitable for diverse crowd work environments. We empirically show that the use of ModOp results in reducing the cognitive load of workers, thereby improving their user experience without affecting the accuracy or task completion time.",
        "date": "September 2017",
        "authors": [
            "Ujwal Gadiraju",
            "Alessandro Checco",
            "Neha Gupta",
            "Gianluca Demartini"
        ],
        "references": [
            319280264,
            317989672,
            316005378,
            312635816,
            311491263,
            311488768,
            318559877,
            317633386,
            316712308,
            316653776
        ]
    },
    {
        "id": 319280264,
        "title": "Using Worker Self-Assessments for Competence-Based Pre-Selection in Crowdsourcing Microtasks",
        "abstract": "Paid crowdsourcing platforms have evolved into remarkable marketplaces where requesters can tap into human intelligence to serve a multitude of purposes, and the workforce can benefit through monetary returns for investing their efforts. In this work, we focus on individual crowd worker competencies. By drawing from self-assessment theories in psychology, we show that crowd workers often lack awareness about their true level of competence. Due to this, although workers intend to maintain a high reputation, they tend to participate in tasks that are beyond their competence. We reveal the diversity of individual worker competencies, and make a case for competence-based pre-selection in crowdsourcing marketplaces. We show the implications of flawed self-assessments on real-world microtasks, and propose a novel worker pre-selection method that considers accuracy of worker self-assessments. We evaluated our method in a sentiment analysis task and observed an improvement in the accuracy by over 15%, when compared to traditional performance-based worker pre-selection. Similarly, our proposed method resulted in an improvement in accuracy of nearly 6% in an image validation task. Our results show that requesters in crowdsourcing platforms can benefit by considering worker self-assessments in addition to their performance for pre-selection.",
        "date": "August 2017",
        "authors": [
            "Ujwal Gadiraju",
            "Besnik Fetahu",
            "Ricardo Kawase",
            "Patrick Siehndel"
        ],
        "references": [
            311491263,
            311488660,
            348864528,
            325671628,
            314800413,
            314105258,
            301935794,
            301935700,
            299857815,
            293486698
        ]
    },
    {
        "id": 317989672,
        "title": "Clarity is a Worthwhile Quality: On the Role of Task Clarity in Microtask Crowdsourcing",
        "abstract": "Workers of microtask crowdsourcing marketplaces strive to find a balance between the need for monetary income and the need for high reputation. Such balance is often threatened by poorly formulated tasks, as workers attempt their execution despite a sub-optimal understanding of the work to be done. In this paper we highlight the role of clarity as a characterising property of tasks in crowdsourcing. We surveyed 100 workers of the CrowdFlower platform to verify the presence of issues with task clarity in crowdsourcing marketplaces, reveal how crowd workers deal with such issues, and motivate the need for mechanisms that can predict and measure task clarity. Next, we propose a novel model for task clarity based on the goal and role clarity constructs. We sampled 7.1K tasks from the Amazon mTurk marketplace, and acquired labels for task clarity from crowd workers. We show that task clarity is coherently perceived by crowd workers, and is affected by the type of the task. We then propose a set of features to capture task clarity, and use the acquired labels to train and validate a supervised machine learning model for task clarity prediction. Finally, we perform a long-term analysis of the evolution of task clarity on Amazon mTurk, and show that clarity is not a property suitable for temporal characterisation.",
        "date": "July 2017",
        "authors": [
            "Ujwal Gadiraju",
            "Jie Yang",
            "Alessandro Bozzon"
        ],
        "references": [
            315976923,
            311491263,
            307174448,
            296638565,
            291832989,
            282344415,
            316117762,
            306157209,
            285651830,
            284339660
        ]
    },
    {
        "id": 333555828,
        "title": "Language in the News: Discourse and Ideology in the Press",
        "abstract": "",
        "date": "October 2013",
        "authors": [
            "Roger Fowler"
        ],
        "references": []
    },
    {
        "id": 333191247,
        "title": "The pronouns of power and solidarity",
        "abstract": "",
        "date": "January 2012",
        "authors": [
            "Roger Brown",
            "Albert Gilman"
        ],
        "references": []
    },
    {
        "id": 328953368,
        "title": "Neural Based Statement Classification for Biased Language",
        "abstract": "Biased language commonly occurs around topics which are of controversial nature, thus, stirring disagreement between the different involved parties of a discussion. This is due to the fact that for language and its use, specifically, the understanding and use of phrases, the stances are cohesive within the particular groups. However, such cohesiveness does not hold across groups. In collaborative environments or environments where impartial language is desired (e.g. Wikipedia, news media), statements and the language therein should represent equally the involved parties and be neutrally phrased. Biased language is introduced through the presence of inflammatory words or phrases, or statements that may be incorrect or one-sided, thus violating such consensus. In this work, we focus on the specific case of phrasing bias, which may be introduced through specific inflammatory words or phrases in a statement. For this purpose, we propose an approach that relies on a recurrent neural networks in order to capture the inter-dependencies between words in a phrase that introduced bias. We perform a thorough experimental evaluation, where we show the advantages of a neural based approach over competitors that rely on word lexicons and other hand-crafted features in detecting biased language. We are able to distinguish biased statements with a precision of P=0.92, thus significantly outperforming baseline models with an improvement of over 30%. Finally, we release the largest corpus of statements annotated for biased language.",
        "date": "November 2018",
        "authors": [
            "Christoph Hube",
            "Besnik Fetahu"
        ],
        "references": [
            324641488,
            322590929,
            305334401,
            305194489,
            284576917,
            265252627,
            262877889,
            246699633,
            241750096,
            13853244
        ]
    },
    {
        "id": 326221106,
        "title": "SimilarHITs: Revealing the Role of Task Similarity in Microtask Crowdsourcing",
        "abstract": "Workers in microtask crowdsourcing systems typically consume different types of tasks. Task consumption is driven by the self-selection of workers in the most popular platforms such as Amazon Mechanical Turk and CrowdFlower. Workers typically complete tasks one after another in a chain. Prior works have revealed the impact of ordering tasks while considering aspects such as task complexity. However, little is understood about the benefits of considering task similarity in microtask chains. In this paper, we investigate the role of task similarity in microtask crowdsourcing and how it affects market dynamics. We identified different dimensions that affect the perception of task similarity among workers, and propose a supervised machine learning model to predict the overall task similarity of a task pair. Leveraging task similarity, we studied the effects of similarity on worker retention, satisfaction, boredom and fatigue. We reveal the impact of chaining tasks according to their similarity on worker accuracy and their task completion time. Our findings enrich the current understanding of crowd work and bear important implications on structuring workflow.",
        "date": "July 2018",
        "authors": [
            "Alan Aipe",
            "Ujwal Gadiraju"
        ],
        "references": [
            317989672,
            312524788,
            311491263,
            311488660,
            307174448,
            291832989,
            290107095,
            266660860,
            255969848,
            247753031
        ]
    },
    {
        "id": 344575773,
        "title": "HOW MANY ADULTS IDENTIFY AS TRANSGENDER IN THE UNITED STATES?",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Andrew Ryan Flores",
            "Jody L Herman",
            "Gary J Gates",
            "Taylor N T Brown"
        ],
        "references": [
            257781039,
            227575267,
            31355253,
            277347441,
            274133938,
            259418658,
            227659014,
            51807555
        ]
    },
    {
        "id": 309089022,
        "title": "Baking Gender Into Social Media Design: How Platforms Shape Categories for Users and Advertisers",
        "abstract": "In recent years, several popular social media platforms have launched freeform custom gender fields. This decision reconstitutes gender categories beyond an oppressive binary only permitting \u201cmales\u201d and \u201cfemales.\u201d In this work, we uncover many different user-facing gender category design strategies within the social media ecosystem, ranging from custom gender options (on Facebook, Google+, and Pinterest) to the absence of gender fields entirely (on Twitter and LinkedIn). To explore how gender is baked into platform design, this article investigates the 10 most popular English-speaking social media platforms by performing recorded walkthroughs from two different subject positions: (1) a new user registering an account, and (2) a new advertiser creating an ad. We explore several different spaces in social media software where designers commonly program gender\u2014sign-up pages, profile pages, and advertising portals\u2014to consider (1) how gender is made durable through social media design, and (2) the shifting composition of the category of gender within the social media ecosystem more broadly. Through this investigation, we question how these categorizations attribute meaning to gender as they materialize in different software spaces, along with the recursive implications for society. Ultimately, our analysis reveals how social media platforms act as intermediaries within the larger ecosystem of advertising and web analytics companies. We argue that this intermediary role entrusts social media platforms with a considerable degree of control over the generation of broader categorization systems, which can be wielded to shape the perceived needs and desires of both users and advertising clients.",
        "date": "October 2016",
        "authors": [
            "Rena Bivens",
            "Oliver L. Haimson"
        ],
        "references": [
            288448003,
            281692999,
            344901639,
            341950644,
            316631031,
            315909314,
            288671167,
            283985962,
            281562833,
            279700465
        ]
    },
    {
        "id": 305109810,
        "title": "Face Recognition Across Gender Transformation Using SVM Classifier",
        "abstract": "The face recognition of medically altered faces under gender transformation is performed. Medical alteration is the gender transformation performed by undergoing hormone replacement therapy (HRT). Face images that are undergoing HRT based gender transformation over a period of several months to three years are taken. By altering the balance of sex hormones HRT achieves gender transformation. HRT causes changes in the physical appearance of the body and face. Changes in the face due to HRT affect the face recognition system. Existing system uses a face recognition system and the studies reveal that the Periocular region is invariant to the changes due to HRT. So a Periocular based face recognition system is developed. Initially perform face and eye detection then alignment and cropping is performed. Face images are represented using patch based local binary patterns. Face recognition is performed by using the binary classifier SVM Classifier.",
        "date": "December 2016",
        "authors": [
            "Archana Vijayan",
            "Shyma Kareem",
            "Jubilant Kizhakkethottam"
        ],
        "references": [
            319770820,
            281327886,
            280907262,
            261419704,
            257091784,
            248843194,
            247672815,
            240224936,
            235390810,
            234358328
        ]
    },
    {
        "id": 304710082,
        "title": "Misgendering and Its Moral Contestability",
        "abstract": "In this article, I consider the harms inflicted upon transgender persons through \u201cmisgendering,\u201d that is, such deployments of gender terms that diminish transgender persons' self-respect, limit the discursive resources at their disposal to define their own gender, and cause them microaggressive psychological harms. Such deployments are morally contestable, that is, they can be challenged on ethical or political grounds. Two characterizations of \u201cwoman\u201d proposed in the feminist literature are critiqued from this perspective. When we consider what would happen to transgender women upon the broad implementation of these characterizations within transgender women's social context, we discover that they suffer from two defects: they either exclude at least some transgender women, or else they implicitly foster hierarchies among women, marginalizing transgender women in particular. In conclusion, I claim that the moral contestability of gender-term deployments acts as a stimulus to regularly consider the provisionality and revisability of our deployments of the term \u201cwoman.\u201d",
        "date": "July 2016",
        "authors": [
            "Stephanie Kapusta"
        ],
        "references": [
            343226307,
            338265838,
            333886761,
            323321496,
            323321185,
            320181448,
            303756922,
            298682194,
            291807960,
            290912012
        ]
    },
    {
        "id": 303563452,
        "title": "Robust transgender face recognition: Approach based on appearance and therapy factors",
        "abstract": "Transgender face recognition is gaining increasing attention in the face recognition community because of its potential in real life applications. Despite extensive progress in traditional face recognition domain, it is very challenging to recognize faces under transgender setting. The gender transformation results in significant face variations, both in shape and texture gradually over time. This introduces additional complexities to existing face recognition algorithms to achieve a reliable performance. In this paper, we present a novel framework that incorporates appearance factor and a transformation factor caused due to Hormone Replacement Therapy (HRT) for recognition. To this extent, we employ the Hidden Factor Analysis (HFA) to jointly model a face under therapy as a linear combination of appearance and transformation factors. This is based on the intuition that the appearance factor captures the features that are unaffected by the therapy and transformation factor captures the feature changes due to therapy. Extensive experiments carried out on publicly available HRT transgender face database shows the efficacy of the proposed scheme with a recognition accuracy of 82.36%.",
        "date": "February 2016",
        "authors": [
            "Vijay Kumar",
            "R. Raghavendra",
            "Anoop M. Namboodiri",
            "Christoph Busch"
        ],
        "references": [
            288079045,
            282285130,
            261088427,
            224148498,
            319770396,
            280218290,
            262391565,
            261419704,
            256011788,
            224309574
        ]
    },
    {
        "id": 318107214,
        "title": "The Authority of \"Fair\" in Machine Learning",
        "abstract": "In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.",
        "date": "June 2017",
        "authors": [
            "Michael Skirpan",
            "Micha Gorelick"
        ],
        "references": [
            317389541,
            309289028,
            308327297,
            305615978,
            301935610,
            301843071,
            276083030,
            272422876,
            346918354,
            346624964
        ]
    },
    {
        "id": 316712454,
        "title": "Intersectional HCI: Engaging Identity through Gender, Race, and Class",
        "abstract": "Understanding users becomes increasingly complicated when we grapple with various overlapping attributes of an individual's identity. In this paper we introduce intersectionality as a framework for engaging with the complexity of users' \"and authors\" \"identities\", and situating these identities in relation to their contextual surroundings. We conducted a meta-review of identity representation in the CHI proceedings, collecting a corpus of 140 manuscripts on gender, ethnicity, race, class, and sexuality published between 1982-2016. Drawing on this corpus, we analyze how identity is constructed and represented in CHI research to examine intersectionality in a human-computer interaction (HCI) context. We find that previous identity-focused research tends to analyze one facet of identity at a time. Further, research on ethnicity and race lags behind research on gender and socio-economic class. We conclude this paper with recommendations for incorporating intersectionality in HCI research broadly, encouraging clear reporting of context and demographic information, inclusion of author disclosures, and deeper engagement with identity complexities.",
        "date": "May 2017",
        "authors": [
            "Ari Schlesinger",
            "W. Keith Edwards",
            "Rebecca E. Grinter"
        ],
        "references": [
            302074121,
            301935818,
            301932845,
            301931746,
            301931080,
            300730089,
            300727717,
            300725936,
            299824834,
            293624478
        ]
    },
    {
        "id": 316706136,
        "title": "Agency in Assistive Technology Adoption: Visual Impairment and Smartphone Use in Bangalore",
        "abstract": "Studies on technology adoption typically assume that a user's perception of usability and usefulness of technology are central to its adoption. Specifically, in the case of accessibility and assistive technology, research has traditionally focused on the artifact rather than the individual, arguing that individual technologies fail or succeed based on their usability and fit for their users. Using a mixed-methods field study of smartphone adoption by 81 people with visual impairments in Bangalore, India, we argue that these positions are dated in the case of accessibility where a non-homogeneous population must adapt to technologies built for sighted people. We found that many users switch to smartphones despite their awareness of significant usability challenges with smartphones. We propose a nuanced understanding of perceived usefulness and actual usage based on need-related social and economic functions, which is an important step toward rethinking technology adoption for people with disabilities.",
        "date": "May 2017",
        "authors": [
            "Joyojeet Pal",
            "Anandhi Viswanathan",
            "Priyank Chandra",
            "Anisha Nazareth"
        ],
        "references": [
            324222556,
            301932935,
            301429274,
            300725781,
            290475072,
            283765943,
            280712096,
            280315490,
            277679294,
            273490600
        ]
    },
    {
        "id": 308659406,
        "title": "Methodologies of misgendering: Recommendations for reducing cisgenderism in psychological research",
        "abstract": "",
        "date": "May 2014",
        "authors": [
            "Y. G. Ansara",
            "Peter Hegarty"
        ],
        "references": [
            316442164,
            265810503,
            263256121,
            258187225,
            226958099,
            51113650,
            294537450,
            287093046,
            283362830,
            266402808
        ]
    },
    {
        "id": 302073924,
        "title": "Does Technology Have Race?",
        "abstract": "This paper started as a response to the \"Black Lives Matter\" campaign in the USA, and emerged as a critique of race more generally in technology design. This paper provides case studies of how technologies are often less usable by persons of color, and contextualizes this in light of intersectionalist theory. Finally, it discusses how the HCI community can ameliorate the situation, and our obligation to do so in light of the ACM code of ethics.",
        "date": "May 2016",
        "authors": [
            "David Hankerson",
            "Andrea R. Marshall",
            "Jennifer Booker",
            "Houda El Mimouni"
        ],
        "references": [
            305426244,
            300714599,
            279850473,
            279499369,
            262273595,
            262215641,
            260272828,
            257391805,
            249689679,
            230800325
        ]
    },
    {
        "id": 273599032,
        "title": "What Is a Flag For? Social Media Reporting Tools and the Vocabulary of Complaint",
        "abstract": "The flag is now a common mechanism for reporting offensive content to an online platform, and is used widely across most popular social media sites. It serves both as a solution to the problem of curating massive collections of user-generated content and as a rhetorical justification for platform owners when they decide to remove content. Flags are becoming a ubiquitous mechanism of governance\u2014yet their meaning is anything but straightforward. In practice, the interactions between users, flags, algorithms, content moderators, and platforms are complex and highly strategic. Significantly, flags are asked to bear a great deal of weight, arbitrating both the relationship between users and platforms, and the negotiation around contentious public issues. In this essay, we unpack the working of the flag, consider alternatives that give greater emphasis to public deliberation, and consider the implications for online public discourse of this now commonplace yet rarely studied sociotechnical mechanism.",
        "date": "July 2014",
        "authors": [
            "Kate Crawford",
            "Tarleton Gillespie"
        ],
        "references": [
            263568442,
            263566377,
            259934157,
            344707755,
            322618098,
            286719525,
            283985962,
            279814489,
            265035948,
            259823367
        ]
    },
    {
        "id": 228164835,
        "title": "Employing E-Health: The Impact of Electronic Health Records on the Workplace",
        "abstract": "Electronic Health Record (EHR) systems may soon become a fixture in most medical settings. President Obama\u2019s 2009 stimulus legislation includes $19 billion to promote their implementation. The sophisticated features and efficiencies of EHR systems have the potential to improve health outcomes and enhance patient welfare considerably. However, this emerging technology also poses significant challenges and risks, not the least of which are its workplace impacts. This article provides a first of its kind analysis of the ramifications of EHR systems for workers and employers. The potential effects of health information computerization on the workplace are numerous. Employers may obtain and process EHRs for purposes of fitness for duty determinations, reasonable accommodations, workers\u2019 compensation, and payment of medical claims. Digitized records could enable employers to obtain unprecedented amounts of information in response to lawful requests and thus intensify workers\u2019 concerns about privacy and discrimination. At the same time, employers may find EHRs to be cumbersome and difficult to interpret and, if they store health information electronically, may worry about security breaches. EHR systems could also affect employers\u2019 insurance costs, impact discovery in litigation, and profoundly affect the work habits of health care providers. This article argues that these concerns can best be addressed by specific changes to the ADA, the HIPAA Privacy and Security Rules, and parallel state laws as well as by technological advances and appropriate federal oversight. As the country transitions to computerization in the medical field, proactive steps must be taken to protect stakeholders in all settings, including the American workplace.",
        "date": "January 2010",
        "authors": [
            "Sharona Hoffman"
        ],
        "references": [
            228164723,
            8072860,
            7447238,
            6950831,
            228207916,
            43199664,
            24440629,
            23953759,
            8994282,
            5248011
        ]
    },
    {
        "id": 314469713,
        "title": "Interview on the Black Box Society",
        "abstract": "Hidden algorithms drive decisions at major Silicon Valley and Wall Street firms. Thanks to automation, those firms can approve credit, rank websites, and make myriad other decisions instantaneously. But what are the costs of their methods? And what exactly are they doing with their digital profiles of us? Leaks, whistleblowers, and legal disputes have shed new light on corporate surveillance and the automated judgments it enables. Self-serving and reckless behavior is surprisingly common, and easy to hide in code protected by legal and real secrecy. Even after billions of dollars of fines have been levied, underfunded regulators may have only scratched the surface of troublingly monopolistic and exploitative practices. Drawing on the work of social scientists, attorneys, and technologists, The Black Box Society offers a bold new account of the political economy of big data. Data-driven corporations play an ever larger role in determining opportunity and risk. But they depend on automated judgments that may be wrong, biased, or destructive. Their black boxes endanger all of us. Faulty data, invalid assumptions, and defective models can\u2019t be corrected when they are hidden. Frank Pasquale exposes how powerful interests abuse secrecy for profit and explains ways to rein them in. Demanding transparency is only the first step. An intelligible society would assure that key decisions of its most important firms are fair, nondiscriminatory, and open to criticism. Silicon Valley and Wall Street need to accept as much accountability as they impose on others. In this interview with Lawrence Joseph, Frank Pasquale describes the aims and methods of the book.",
        "date": "January 2014",
        "authors": [
            "Lawrence Joseph",
            "Frank Pasquale"
        ],
        "references": []
    },
    {
        "id": 295556936,
        "title": "The programmed prospect before us",
        "abstract": "",
        "date": "April 2014",
        "authors": [
            "R. Skidelsky"
        ],
        "references": []
    },
    {
        "id": 258825673,
        "title": "Increasing employee participation in corporate wellness programs",
        "abstract": "",
        "date": "September 2013",
        "authors": [
            "Barb Hendrickson"
        ],
        "references": []
    },
    {
        "id": 246950863,
        "title": "Nickel and Dimed: On (Not) Getting By in America",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "Barbara Ehrenreich"
        ],
        "references": []
    },
    {
        "id": 235356809,
        "title": "Discrimination in Online Ad Delivery",
        "abstract": "A Google search for a person's name, such as \"Trevon Jones\", may yield a personalized ad for public records about Trevon that may be neutral, such as \"Looking for Trevon Jones?\", or may be suggestive of an arrest record, such as \"Trevon Jones, Arrested?\". This writing investigates the delivery of these kinds of ads by Google AdSense using a sample of racially associated names and finds statistically significant discrimination in ad delivery based on searches of 2184 racially associated personal names across two websites. First names, assigned at birth to more black or white babies, are found predictive of race (88% black, 96% white), and those assigned primarily to black babies, such as DeShawn, Darnell and Jermaine, generated ads suggestive of an arrest in 81 to 86 percent of name searches on one website and 92 to 95 percent on the other, while those assigned at birth primarily to whites, such as Geoffrey, Jill and Emma, generated more neutral copy: the word \"arrest\" appeared in 23 to 29 percent of name searches on one site and 0 to 60 percent on the other. On the more ad trafficked website, a black-identifying name was 25% more likely to get an ad suggestive of an arrest record. A few names did not follow these patterns. All ads return results for actual individuals and ads appear regardless of whether the name has an arrest record in the company's database. The company maintains Google received the same ad text for groups of last names (not first names), raising questions as to whether Google's technology exposes racial bias.",
        "date": "January 2013",
        "authors": [
            "Latanya Sweeney"
        ],
        "references": [
            225284618,
            265190335,
            249713489,
            243134278,
            239839762,
            236791900,
            229945109,
            51957869,
            36835936,
            24091871
        ]
    },
    {
        "id": 23149941,
        "title": "2007 employment decisions under the ADA Title I--survey update",
        "abstract": "",
        "date": "May 2008",
        "authors": [
            "Amy L Allbright"
        ],
        "references": []
    },
    {
        "id": 306224576,
        "title": "Governance by algorithms",
        "abstract": "p>Algorithms are increasingly often cited as one of the fundamental shaping devices of our daily, immersed-in-information existence. Their importance is acknowledged, their performance scrutinised in numerous contexts. Yet, a lot of what constitutes 'algorithms' beyond their broad definition as \u201cencoded procedures for transforming input data into a desired output, based on specified calculations\u201d (Gillespie, 2013) is often taken for granted. This article seeks to contribute to the discussion about 'what algorithms do' and in which ways they are artefacts of governance, providing two examples drawing from the internet and ICT realm: search engine queries and e-commerce websites\u2019 recommendations to customers. The question of the relationship between algorithms and rules is likely to occupy an increasingly central role in the study and the practice of internet governance, in terms of both institutions\u2019 regulation of algorithms, and algorithms\u2019 regulation of our society.</p",
        "date": "August 2013",
        "authors": [
            "Francesca Musiani",
            "Internet Policy Review"
        ],
        "references": [
            281562384,
            237108923,
            324385808,
            309043557,
            308953917,
            298883947,
            263108135,
            256056774,
            229068411,
            220062980
        ]
    },
    {
        "id": 346624964,
        "title": "The Black Box Society: The Secret Algorithms That Control Money and Information",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Frank Pasquale"
        ],
        "references": []
    },
    {
        "id": 346447137,
        "title": "Trust in Numbers: The Pursuit of Objectivity in Science and Public Life",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "Theodore M. Porter"
        ],
        "references": []
    },
    {
        "id": 344486679,
        "title": "Programmed Visions: Software and Memory",
        "abstract": "",
        "date": "April 2011",
        "authors": [
            "Wendy Hui Kyong Chun"
        ],
        "references": []
    },
    {
        "id": 336971981,
        "title": "Nine Algorithms That Changed the Future: The Ingenious Ideas That Drive Today's Computers",
        "abstract": "",
        "date": "December 2013",
        "authors": [
            "John MacCormick"
        ],
        "references": []
    },
    {
        "id": 298883947,
        "title": "Deliberative, Agonistic, and Algorithmic Audiences: Journalism's Vision of its Public in an Age of Audience Transparency",
        "abstract": "Building on earlier empirical work in newsrooms, this paper contends that a fundamental transformation has occurred in journalists' understanding of their audiences. A new level of responsiveness to the agenda of the audience is becoming built into the DNA of contemporary news work. This article argues, however, that this journalistic responsiveness to the \"agenda of the audience\" has multiple, often contradictory meanings. It traces these out through a critical-historical sketch of key moments in the journalism-audience relationship, including the public journalism movement, Independent Media Center (Indymedia), and Demand Media. These different visions of the audience are correlated to different images of democracy, and they have different sociological implications. The public journalism movement believed in a form of democracy that was conversational and deliberative; in contrast, traditional journalism embraced an aggregative understanding of democracy, while Indymedia's democratic vision could best be seen as agonistic in nature. Demand Media and similar ventures, this article concludes, may be presaging an image of public that can best be described as algorithmic. Understanding this algorithmic conception of the audience may be the first step into launching a broader inquiry into the sociology and politics of algorithms.",
        "date": "January 2011",
        "authors": [
            "C. W. Anderson"
        ],
        "references": [
            239835289,
            249735453,
            249689876,
            249666676,
            249006848,
            248988212,
            247405627,
            45380289
        ]
    },
    {
        "id": 297735227,
        "title": "Source Code",
        "abstract": "Source code is a set of human readable computer commands written in high-level programming languages. It also shares some of the parameters of the natural language concept. This chapter starts with a source code program and explains the principle and concept of source code; it also describes the importance and advantages of source code in the field of software. Furthermore, the chapter explains that source code is a non-executable computer program stored in source files and can be called a repository; it also focuses on the compiling process of source codes. The chapter describes the two conditions of source code\u2014open and closed\u2014and its contribution to the field of software licensing.",
        "date": "April 2008",
        "authors": [
            "Joasia Krysa",
            "Grzesiek Sedek"
        ],
        "references": []
    },
    {
        "id": 288780647,
        "title": "Coding places: Software practice in a South American city",
        "abstract": "An examination of software practice in Brazil that reveals both the globalization and the localization of software development. \u00a9 2012 Massachusetts Institute of Technology. All rights reserved.",
        "date": "January 2012",
        "authors": [
            "Y. Takhteyev"
        ],
        "references": [
            247734722,
            240953869,
            294542376,
            289454970,
            286655649,
            279398996,
            270146409,
            249743928,
            238356968,
            233364922
        ]
    },
    {
        "id": 285685562,
        "title": "Algorithm",
        "abstract": "This chapter starts with the brief description of the importance of the algorithm concept in the field of software studies. It also focuses on the crucial role and historic position of the algorithm in the computer science field. The chapter proves that in the area of computational linguistics, the algorithm is the sum of logic and control that has its origins in ancient mathematics. The chapter also analyzes the concept of the statement and its relationship with the algorithm. Throughout the chapter, the works of several computer scientists are reflected upon for a better understanding of the core theme.",
        "date": "April 2008",
        "authors": [
            "Andrew Goffey"
        ],
        "references": []
    },
    {
        "id": 346624651,
        "title": "The Taming of Chance",
        "abstract": "",
        "date": "August 1990",
        "authors": [
            "Ian Hacking"
        ],
        "references": []
    },
    {
        "id": 333555794,
        "title": "Primate Visions: Gender, Race, and Nature in the World of Modern Science",
        "abstract": "Haraway\u2019s discussions of how scientists have perceived the sexual nature of female primates opens a new chapter in feminist theory, raising unsettling questions about models of the family and of heterosexuality in primate research.",
        "date": "January 2013",
        "authors": [
            "Donna J. Haraway"
        ],
        "references": []
    },
    {
        "id": 321877775,
        "title": "On the Normal and the Pathological",
        "abstract": "by MICHEL FOUCAULT Everyone knows that in France there are few logicians but many historians of science; and that in the 'philosophical establishment' - whether teaching or research oriented - they have occupied a considerable position. But do we know precisely the importance that, in the course of these past fifteen or twenty years, up to the very frontiers of the establishment, a 'work' like that of Georges Canguilhem can have had for those very people who were separ\u00ad ated from, or challenged, the establishment? Yes, I know, there have been noisier theatres: psychoanalysis, Marxism, linguistics, ethnology. But let us not forget this fact which depends, as you will, on the sociology of French intellectual environments, the functioning of our university institutions or our system of cultural values: in all the political or scientific discussions of these strange sixty years past, the role of the 'philosophers' - I simply mean those who had received their university training in philosophy department- has been important: perhaps too important for the liking of certain people. And, directly or indirectly, all or almost all these philosophers have had to 'come to terms with' the teaching and books of Georges Canguilhem. From this, a paradox: this man, whose work is austere, intentionally and carefully limited to a particular domain in the history of science, which in any case does not pass for a spectacular discipline, has somehow found him\u00ad self present in discussions where he himself took care never to figure.",
        "date": "January 1978",
        "authors": [
            "Georges Canguilhem"
        ],
        "references": []
    },
    {
        "id": 311805038,
        "title": "A History of Charisma",
        "abstract": "This book traces the history of the word 'charisma', and the various meanings assigned to it, from its first century origins in Christian theology to its manifestations in twenty-first century politics and culture, while considering how much of the word's original religious meaning persists in the contemporary secular understanding.",
        "date": "September 2009",
        "authors": [
            "John Potts"
        ],
        "references": [
            322161050
        ]
    },
    {
        "id": 291776274,
        "title": "Knowledge Eclipse: Producing Sociomaterial Reconfigurations in the Hospitality Sector",
        "abstract": "Drawing on a field study of the travel site TripAdvisor, the authors explore how online reviewing, rating, and ranking mechanisms are overshadowing traditional configurations of knowledge in the hospitality sector by redistributing resources, shifting practices and habitats, and redefining what counts, who counts, and how. The authors suggest that such sociomaterial reconfigurations offer important insights into the broader issues associated with the role of social media in knowledge practices, and the ways in which expert valuation schemes are being eclipsed by ones grounded in user-generated content. They maintain that these different valuation schemes entail different kinds of work, producing different valuations of the real, and enacting different (singular and multiple) realities. As such, these reconfigurations of valuation raise not just important epistemological issues but also critical questions of ontology and accountability.",
        "date": "March 2013",
        "authors": [
            "Wanda J. Orlikowski",
            "Susan V. Scott"
        ],
        "references": []
    },
    {
        "id": 275995045,
        "title": "How Institutions Think.",
        "abstract": "",
        "date": "May 1988",
        "authors": [
            "Bruno Latour",
            "Mary Douglas"
        ],
        "references": []
    },
    {
        "id": 273089461,
        "title": "The Taming of Chance",
        "abstract": "",
        "date": "September 1992",
        "authors": [
            "Peter Guttorp",
            "Ian Hacking"
        ],
        "references": []
    },
    {
        "id": 272721982,
        "title": "Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "James C. McCann",
            "James C. Scott"
        ],
        "references": []
    },
    {
        "id": 269758508,
        "title": "Trust in Numbers: The Pursuit of Objectivity in Science and Public Life",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "Timothy L. Alborn",
            "Theodore M. Porter",
            "M. Norton Wise"
        ],
        "references": []
    },
    {
        "id": 262160717,
        "title": "Exploiting user disagreement for web search evaluation: An experimental approach",
        "abstract": "To express a more nuanced notion of relevance as compared to binary judgments, graded relevance levels can be used for the evaluation of search results. Especially in Web search, users strongly prefer top results over less relevant results, and yet they often disagree on which are the top results for a given information need. Whereas previous works have generally considered disagreement as a negative effect, this paper proposes a method to exploit this user disagreement by integrating it into the evaluation procedure. First, we present experiments that investigate the user disagreement. We argue that, with a high disagreement, lower relevance levels might need to be promoted more than in the case where there is global consensus on the top results. This is formalized by introducing the User Disagreement Model, resulting in a weighting of the relevance levels with a probabilistic interpretation. A validity analysis is given, and we explain how to integrate the model with well-established evaluation metrics. Finally, we discuss a specific application of the model, in the estimation of suitable weights for the combined relevance of Web search snippets and pages.",
        "date": "February 2014",
        "authors": [
            "Thomas Demeester",
            "Robin Aly",
            "Djoerd Hiemstra",
            "Dong Nguyen"
        ],
        "references": [
            268369553,
            262154478,
            255563504,
            267026585,
            262397237,
            239489329,
            227517579,
            222753431,
            222649957,
            221615367
        ]
    },
    {
        "id": 312893045,
        "title": "Discriminative batch mode active learning",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Y. Guo",
            "D. Schuurmans"
        ],
        "references": []
    },
    {
        "id": 303783362,
        "title": "Ranking annotators for crowdsourced labeling tasks",
        "abstract": "",
        "date": "January 2011",
        "authors": [
            "V.C. Raykar",
            "S. Yu"
        ],
        "references": [
            221654367
        ]
    },
    {
        "id": 291249011,
        "title": "Crowdsourcing annotations for visual object detection",
        "abstract": "A large number of images with ground truth object bounding boxes are critical for learning object detectors, which is a fundamental task in compute vision. In this paper, we study strategies to crowd-source bounding box annotations. The core challenge of building such a system is to effectively control the data quality with minimal cost. Our key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions. Thus quality control through additional verification tasks is more cost effective than consensus based algorithms. In particular, we present a system that consists of three simple sub-tasks - a drawing task, a quality verification task and a coverage verification task. Experimental results demonstrate that our system is scalable, accurate, and cost-effective. Copyright \u00a9 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
        "date": "January 2012",
        "authors": [
            "Hao Su",
            "J. Deng",
            "L. Fei-Fei"
        ],
        "references": [
            221110765
        ]
    },
    {
        "id": 290276373,
        "title": "Incorporating Diversity in Active Learning with Support Vector Machines",
        "abstract": "In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > I examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.",
        "date": "January 2003",
        "authors": [
            "K. Blinker"
        ],
        "references": [
            221619430,
            2636545
        ]
    },
    {
        "id": 269087905,
        "title": "Cheap and fast---but is it good?",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "Rion Snow",
            "Brendan O'Connor",
            "Daniel Jurafsky",
            "Andrew Y. Ng"
        ],
        "references": [
            251186613,
            230876766,
            228911333,
            228559081,
            221655828,
            221654367,
            221517483,
            221366753,
            220420019,
            220017637
        ]
    },
    {
        "id": 266797230,
        "title": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation",
        "abstract": "Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines.",
        "date": "August 2013",
        "authors": [
            "Trevor Cohn",
            "Lucia Specia"
        ],
        "references": [
            255572833,
            220873922,
            220520145,
            220320600,
            220286636,
            51914293,
            51050154,
            47735534,
            270878317,
            228849469
        ]
    },
    {
        "id": 262392516,
        "title": "The effect of threshold priming and need for cognition on relevance calibration and assessment",
        "abstract": "Human assessments of document relevance are needed for the construction of test collections, for ad-hoc evaluation, and for training text classifiers. Showing documents to assessors in different orderings, however, may lead to different assessment outcomes. We examine the effect that \\defineterm{threshold priming}, seeing varying degrees of relevant documents, has on people's calibration of relevance. Participants judged the relevance of a prologue of documents containing highly relevant, moderately relevant, or non-relevant ocuments, followed by a common epilogue of documents of mixed relevance. We observe that participants exposed to only non-relevant documents in the prologue assigned significantly higher average relevance scores to prologue and epilogue documents than participants exposed to moderately or highly relevant documents in the prologue. We also examine how \\defineterm{need for cognition}, an individual difference measure of the extent to which a person enjoys engaging in effortful cognitive activity, impacts relevance assessments. High need for cognition participants had a significantly higher level of agreement with expert assessors than low need for cognition participants did. Our findings indicate that assessors should be exposed to documents from multiple relevance levels early in the judging process, in order to calibrate their relevance thresholds in a balanced way, and that individual difference measures might be a useful way to screen assessors.",
        "date": "July 2013",
        "authors": [
            "Falk Scholer",
            "Diane Kelly",
            "Wan-Ching Wu",
            "Hanseul Stephanie Lee"
        ],
        "references": [
            309764430,
            232490414,
            232427348,
            229039373,
            227696546,
            227634540,
            221300660,
            221299097,
            220435547,
            220433462
        ]
    },
    {
        "id": 262242277,
        "title": "Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks",
        "abstract": "With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEMthat iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.",
        "date": "February 2012",
        "authors": [
            "Vikas C. Raykar",
            "Shipeng Yu"
        ],
        "references": [
            228897427,
            221654367,
            221654023,
            221619723,
            221619287,
            221345647,
            220837292,
            220320474,
            220320331,
            303783362
        ]
    },
    {
        "id": 262236825,
        "title": "Querying Discriminative and Representative Samples for Batch Mode Active Learning",
        "abstract": "Empirical risk minimization (ERM) provides a useful guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may not follow the source distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-the-art methods.",
        "date": "August 2013",
        "authors": [
            "Zheng Wang",
            "Jieping Ye"
        ],
        "references": [
            262318598,
            260323344,
            226658534,
            225138938,
            221995733,
            221653343,
            221619711,
            221619430,
            221497539,
            221362286
        ]
    },
    {
        "id": 321325134,
        "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?",
        "abstract": "The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch",
        "date": "June 2018",
        "authors": [
            "Kensho Hara",
            "Hirokatsu Kataoka",
            "Yutaka Satoh"
        ],
        "references": [
            317040382,
            322646478,
            322059525,
            320971540,
            320971251,
            320968042,
            319163983,
            317194083,
            317062224,
            311610155
        ]
    },
    {
        "id": 339561787,
        "title": "SlowFast Networks for Video Recognition",
        "abstract": "",
        "date": "October 2019",
        "authors": [
            "Christoph Feichtenhofer",
            "Haoqi Fan",
            "Jitendra Malik",
            "Kaiming He"
        ],
        "references": [
            335963787,
            328143596,
            324744695,
            321794462,
            321510540,
            321306779,
            320968277,
            311737185,
            306187421,
            305779667
        ]
    },
    {
        "id": 338512546,
        "title": "Learning Correspondence From the Cycle-Consistency of Time",
        "abstract": "",
        "date": "June 2019",
        "authors": [
            "Xiaolong Wang",
            "Allan Jabri",
            "Alexei Efros"
        ],
        "references": [
            329743795,
            324744874,
            322060135,
            321210884,
            320971795,
            316875253,
            316190696,
            315116078,
            312222729,
            304409658
        ]
    },
    {
        "id": 335302673,
        "title": "Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles",
        "abstract": "Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as Space-Time Cubic Puzzles to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing Space-Time Cubic Puzzles, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.",
        "date": "July 2019",
        "authors": [
            "Kim Dahun",
            "Donghyeon Cho",
            "Soo-Ok Kweon"
        ],
        "references": [
            329750748,
            321325134,
            317040382,
            315747848,
            307984819,
            305779667,
            277023095,
            265295439,
            240308775,
            233815759
        ]
    },
    {
        "id": 329745706,
        "title": "Learning and Using the Arrow of Time",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Donglai Wei",
            "Joseph Lim",
            "Andrew Zisserman",
            "William T Freeman"
        ],
        "references": [
            317040382,
            315747848,
            310611045,
            307984819,
            305779667,
            305196650,
            277023095,
            273761448,
            269935290,
            248964741
        ]
    },
    {
        "id": 328143578,
        "title": "Deep Clustering for Unsupervised Learning of Visual Features: 15th European Conference, Munich, Germany, September 8\u201314, 2018, Proceedings, Part XIV",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Mathilde Caron",
            "Piotr Bojanowski",
            "Armand Joulin",
            "Matthijs Douze"
        ],
        "references": [
            308295512,
            306885833,
            303137904,
            282580735,
            280329483,
            279839496,
            277023095,
            275974132,
            265295439,
            265022827
        ]
    },
    {
        "id": 323412769,
        "title": "Anticipating Visual Representations from Unlabeled Video",
        "abstract": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.",
        "date": "June 2016",
        "authors": [
            "Carl Vondrick",
            "Hamed Pirsiavash",
            "Antonio Torralba"
        ],
        "references": [
            277023095,
            308854803,
            304409339,
            286734160,
            286594601,
            286594520,
            286594463,
            279840550,
            275897284,
            262355103
        ]
    },
    {
        "id": 322058314,
        "title": "Unsupervised Representation Learning by Sorting Sequences",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Hsin-Ying Lee",
            "Jia-Bin Huang",
            "Maneesh Kumar Singh",
            "Ming-Hsuan Yang"
        ],
        "references": [
            315747848,
            311106922,
            310611045,
            308295512,
            307984819,
            305652274,
            303544998,
            303137904,
            301836281,
            284579529
        ]
    },
    {
        "id": 322057985,
        "title": "Representation Learning by Learning to Count",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Mehdi Noroozi",
            "Hamed Pirsiavash",
            "Paolo Favaro"
        ],
        "references": [
            320971847,
            311106922,
            308295512,
            308139028,
            284579529,
            277023095,
            276211157,
            275974132,
            273284676,
            268689412
        ]
    },
    {
        "id": 325002984,
        "title": "Learning Image Representations by Completing Damaged Jigsaw Puzzles",
        "abstract": "",
        "date": "March 2018",
        "authors": [
            "Kim Dahun",
            "Donghyeon Cho",
            "Donggeun Yoo",
            "Inso Kweon"
        ],
        "references": [
            316235051,
            314234312,
            311106922,
            308107013,
            305196650,
            284579529,
            283762292,
            277023095,
            275974132,
            265295439
        ]
    },
    {
        "id": 322975889,
        "title": "Learning Image Representations by Completing Damaged Jigsaw Puzzles",
        "abstract": "In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called \"Completing damaged jigsaw puzzles\" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.",
        "date": "February 2018",
        "authors": [
            "Kim Dahun",
            "Donghyeon Cho",
            "Donggeun Yoo",
            "Inso Kweon"
        ],
        "references": [
            314234312,
            311106922,
            308107013,
            305196650,
            284579529,
            283762292,
            277023095,
            265295439,
            264979485,
            244437609
        ]
    },
    {
        "id": 322060714,
        "title": "Multi-task Self-Supervised Visual Learning",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Carl Doersch",
            "Andrew Zisserman"
        ],
        "references": [
            319770123,
            315747848,
            313857509,
            310611045,
            309551169,
            308295512,
            304505393,
            304350732,
            303750237,
            277023095
        ]
    },
    {
        "id": 322058914,
        "title": "Transitive Invariance for Self-Supervised Visual Representation Learning",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Xiaolong Wang",
            "Kaiming He",
            "Abhinav Gupta"
        ],
        "references": [
            314234312,
            311106922,
            304505393,
            303755744,
            303137904,
            277023095,
            275974132,
            272423336,
            269935290,
            265295439
        ]
    },
    {
        "id": 320964865,
        "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Richard Zhang",
            "Phillip Isola",
            "Alexei Efros"
        ],
        "references": [
            308295512,
            303755744,
            286971991,
            284579529,
            279839496,
            277023095,
            275974132,
            265295439,
            264160686,
            239571798
        ]
    },
    {
        "id": 303993074,
        "title": "Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning",
        "abstract": "Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.",
        "date": "June 2016",
        "authors": [
            "Mehdi Sajjadi",
            "Mehran Javanmardi",
            "Tolga Tasdizen"
        ],
        "references": [
            305196650,
            303921584,
            319770626,
            319770183,
            319770168,
            307516327,
            306218037,
            304409339,
            303505510,
            301921832
        ]
    },
    {
        "id": 301845925,
        "title": "Auxiliary Deep Generative Models",
        "abstract": "Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%) datasets.",
        "date": "February 2016",
        "authors": [
            "Lars Maal\u00f8e",
            "Casper S\u00f8nderby",
            "S\u00f8ren Kaae S\u00f8nderby",
            "Ole Winther"
        ],
        "references": [
            279968088,
            319770221,
            319770163,
            301874314,
            301854820,
            283658966,
            281487457,
            277023018,
            272194743,
            269935079
        ]
    },
    {
        "id": 308964680,
        "title": "Temporal Ensembling for Semi-Supervised Learning",
        "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63% to 12.89% in CIFAR-10 with 4000 labels and from 18.44% to 6.83% in SVHN with 500 labels.",
        "date": "October 2016",
        "authors": [
            "Samuli Laine",
            "Timo Aila"
        ],
        "references": [
            308278012,
            308066352,
            303993074,
            301845925,
            284476548,
            279968088,
            279632719,
            269722198,
            319770272,
            319770264
        ]
    },
    {
        "id": 286784019,
        "title": "Ill-posed problems",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "V.Y. Arsenin",
            "A.N. Tikhonov"
        ],
        "references": []
    },
    {
        "id": 284476459,
        "title": "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks",
        "abstract": "In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).",
        "date": "November 2015",
        "authors": [
            "Jost Tobias Springenberg"
        ],
        "references": [
            280581078,
            279968088,
            269722198,
            269577174,
            263471626,
            263012109,
            237053970,
            236735729,
            228467601,
            228102719
        ]
    },
    {
        "id": 315882843,
        "title": "DeepPermNet: Visual Permutation Learning",
        "abstract": "We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",
        "date": "April 2017",
        "authors": [
            "Rodrigo Santa Cruz",
            "Basura Fernando",
            "Anoop Cherian",
            "Stephen Gould"
        ],
        "references": [
            315747848,
            319770168,
            312456545,
            311610802,
            308839814,
            308277657,
            308277343,
            308277283,
            308277081,
            304409339
        ]
    },
    {
        "id": 311411639,
        "title": "Action Recognition with Dynamic Image Networks",
        "abstract": "We introduce the concept of \"dynamic image\", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of `rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation is in the form of an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a CNN layer. This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance.",
        "date": "December 2016",
        "authors": [
            "Hakan Bilen",
            "Basura Fernando",
            "Efstratios Gavves",
            "Andrea Vedaldi"
        ],
        "references": [
            312217094,
            320968442,
            319770183,
            319770168,
            317062224,
            312831238,
            312152790,
            311610199,
            311610155,
            311344466
        ]
    },
    {
        "id": 310020983,
        "title": "Learning End-to-end Video Classification with Rank-Pooling",
        "abstract": "We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolu-tional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.",
        "date": "June 2016",
        "authors": [
            "Basura Fernando",
            "Stephen Gould"
        ],
        "references": [
            319770438,
            308034527,
            308859819,
            305867060,
            304843249,
            304417592,
            303158328,
            300408292,
            299487275,
            286594438
        ]
    },
    {
        "id": 305779667,
        "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.",
        "date": "October 2016",
        "authors": [
            "Limin Wang",
            "Yuanjun Xiong",
            "Zhe Wang",
            "Yu Qiao"
        ],
        "references": [
            319770438,
            322322434,
            313474743,
            312727767,
            312152790,
            311682086,
            311610658,
            311609274,
            311344466,
            310752533
        ]
    },
    {
        "id": 305006910,
        "title": "VideoLSTM Convolves, Attends and Flows for Action Recognition",
        "abstract": "We present a new architecture for end-to-end sequence learning of actions in video, we call VideoLSTM. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be used for action localization by relying on just the action class label. Experiments and comparisons on challenging datasets for action classification and localization support our claims.",
        "date": "July 2016",
        "authors": [
            "Zhenyang Li",
            "Efstratios Gavves",
            "Mihir Jain",
            "Cees Snoek"
        ],
        "references": [
            320386397,
            319770438,
            312152790,
            308860420,
            308594286,
            304409775,
            300408292,
            290214563,
            286794765,
            286594438
        ]
    },
    {
        "id": 309630116,
        "title": "Parallel distributed processing: explorations in the microstructure of cognition",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "G.E. Hinton",
            "T.J. Sejnowski"
        ],
        "references": []
    },
    {
        "id": 309551617,
        "title": "Operator Variational Inference",
        "abstract": "Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.",
        "date": "October 2016",
        "authors": [
            "Rajesh Ranganath",
            "Jaan Altosaar",
            "Dustin Tran",
            "David M. Blei"
        ],
        "references": [
            301847976,
            301845925,
            283762219,
            319770355,
            319770134,
            316323067,
            305974809,
            287408893,
            284476307,
            283658966
        ]
    },
    {
        "id": 322715582,
        "title": "The elements of statistical learning",
        "abstract": "",
        "date": "January 2009",
        "authors": [
            "Jerome H. Friedman",
            "J. Hastie",
            "R. Tibshirani"
        ],
        "references": []
    },
    {
        "id": 313058712,
        "title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization",
        "abstract": "",
        "date": "January 2008",
        "authors": [
            "X. Nguyen",
            "M. Wainwright",
            "M. Jordan"
        ],
        "references": []
    },
    {
        "id": 311668858,
        "title": "Adversarial Message Passing For Graphical Models",
        "abstract": "Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables. However, inference in implicit models or complex posterior distributions is hard. A popular tool for learning implicit models are generative adversarial networks (GANs) which learn parameters of generators by fooling discriminators. Typically, GANs are considered to be models themselves and are not understood in the context of inference. Current techniques rely on inefficient global discrimination of joint distributions to perform learning, or only consider discriminating a single output variable. We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs. We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations. This allows us to compose models and yields a unified inference and learning framework for adversarial learning. Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs, including components such as intractable likelihoods, non-differentiable models, simulators and generally cumbersome models. A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families, without access to explicit distributions. As a side-result, we discuss the link to likelihood maximization. These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications.",
        "date": "December 2016",
        "authors": [
            "Theofanis Karaletsos"
        ],
        "references": [
            309551617,
            309551292,
            309283650,
            309192180,
            308980793,
            308964873,
            308324937,
            303859083,
            303755744,
            303682017
        ]
    },
    {
        "id": 311514837,
        "title": "Improved generator objectives for GANs",
        "abstract": "We present a framework to understand GAN training as alternating density ratio estimation and approximate divergence minimization. This provides an interpretation for the mismatched GAN generator and discriminator objectives often used in practice, and explains the problem of poor sample diversity. We also derive a family of generator objectives that target arbitrary $f$-divergences without minimizing a lower bound, and use them to train generative image models that target either improved sample quality or greater sample diversity.",
        "date": "December 2016",
        "authors": [
            "Ben Poole",
            "Alexander A Alemi",
            "Jascha Sohl-Dickstein",
            "Anelia Angelova"
        ],
        "references": [
            309192180,
            308980793,
            308152499,
            305881127,
            303755744,
            269876178,
            263012109,
            221619415,
            319770355,
            319770144
        ]
    },
    {
        "id": 288713304,
        "title": "Visually Indicated Sounds",
        "abstract": "Materials make distinctive sounds when they are hit or scratched -- dirt makes a thud; ceramic makes a clink. These sounds reveal aspects of an object's material properties, as well as the force and motion of the physical interaction. In this paper, we introduce an algorithm that learns to synthesize sound from videos of people hitting objects with a drumstick. The algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We demonstrate that the sounds generated by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about the material properties in a scene.",
        "date": "December 2015",
        "authors": [
            "Andrew Hale Owens",
            "Phillip Isola",
            "Josh Mcdermott",
            "Antonio Torralba"
        ],
        "references": [
            319770438,
            308034527,
            319770328,
            319770183,
            306537726,
            304409339,
            300412509,
            286794765,
            286594438,
            286570994
        ]
    },
    {
        "id": 319770399,
        "title": "Approximate nearest neighbors: towards removing the curse of dimensionality",
        "abstract": "The nearest neighbor problem is the following: Given a set of n points P in some metric space X, preprocess P so as to efficiently answer queries which require finding the point in P closest to the query point q in X. We focus on the particularly interesting case of the d-dimensional Euclidean space where X = R-d under some l-p norm.",
        "date": "January 1998",
        "authors": [
            "Piotr Indyk",
            "Rajeev Motwani"
        ],
        "references": []
    },
    {
        "id": 313544906,
        "title": "Audio-based context recognition",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "A.J. Eronen",
            "V.T. Peltonen",
            "J.T. Tuomi",
            "Anssi Klapuri"
        ],
        "references": []
    },
    {
        "id": 308161116,
        "title": "Is object localization for free? - Weakly-supervised learning with convolutional neural networks",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Maxime Oquab",
            "Leon Bottou",
            "Ivan Laptev",
            "Josef Sivic"
        ],
        "references": [
            273158036,
            271772694,
            269935290,
            265687273,
            261121672,
            261100864,
            259441043,
            241770100,
            228602850,
            224135977
        ]
    },
    {
        "id": 300412509,
        "title": "Learning Image Representations Tied to Ego-Motion",
        "abstract": "",
        "date": "December 2015",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "references": [
            319770366,
            286931946,
            270283023,
            269935600,
            264979485,
            262257655,
            260610332,
            258140919,
            228095620,
            221346269
        ]
    },
    {
        "id": 284579516,
        "title": "Learning visual groups from co-occurrences in space and time",
        "abstract": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.",
        "date": "November 2015",
        "authors": [
            "Phillip Isola",
            "Daniel Zoran",
            "Dilip Krishnan",
            "Edward H. Adelson"
        ],
        "references": [
            286404956,
            279840416,
            279751103,
            277023095,
            276211157,
            272423336,
            268689412,
            264979485,
            262270555,
            235648646
        ]
    },
    {
        "id": 339558556,
        "title": "Visualization of Convolutional Neural Networks for Monocular Depth Estimation",
        "abstract": "Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and out-door scene datasets. The results provide several findings that help exploration of the above question.",
        "date": "October 2019",
        "authors": [
            "Junjie Hu",
            "Yan Zhang",
            "Takayuki Okatani"
        ],
        "references": [
            331591265,
            324387531,
            323550085,
            322976218,
            320014368,
            333691188,
            331802449,
            329743389,
            329740202,
            325638340
        ]
    },
    {
        "id": 329750745,
        "title": "Learning Depth from Monocular Videos Using Direct Methods",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Chaoyang Wang",
            "Jos\u00e9 Miguel Buenaposada",
            "Rui Zhu",
            "Simon Lucey"
        ],
        "references": [
            319135283,
            305994969,
            303750237,
            301880609,
            301880010,
            301292064,
            279262680,
            272845187,
            261989114,
            221430091
        ]
    },
    {
        "id": 324271713,
        "title": "Depth Perception",
        "abstract": "This chapter describes how people judge how far objects are from themselves. Comparing the images in the two eyes and the orientations of the two eyes when looking at the object of interest provides depth information that is based solely on the viewing geometry, but the resolution of such information is limited. Information acquired from cues such as occlusion, motion parallax, and height in the visual field is based on assumptions that need not always be true, such as that objects are isotropic, static, and resting on horizontal surfaces. By combining the many sources of information in a clever manner people obtain quite reliable judgments that are not too sensitive to violations of the assumptions of the individual sources of depth information.",
        "date": "January 2018",
        "authors": [
            "Eli Brenner",
            "Jeroen Smeets"
        ],
        "references": [
            280827204,
            262931251,
            258854459,
            252604403,
            236964257,
            236182923,
            235398346,
            225104566,
            51763461,
            51668822
        ]
    },
    {
        "id": 321574879,
        "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency",
        "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. By exploiting epipolar geometry constraints, we generate disparity images by training our networks with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",
        "date": "July 2017",
        "authors": [
            "Clement Godard",
            "Oisin Mac Aodha",
            "Jourdan Gabriel"
        ],
        "references": [
            319770233,
            319769813,
            303750237,
            301880609,
            301876860,
            301839500,
            299397682,
            286301692,
            285459125,
            284579051
        ]
    },
    {
        "id": 320968448,
        "title": "Unsupervised Learning of Depth and Ego-Motion from Video",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Tinghui Zhou",
            "Matthew Brown",
            "Noah Snavely",
            "David G. Lowe"
        ],
        "references": [
            314943348,
            311737230,
            311492571,
            311411647,
            303750237,
            301880609,
            301839500,
            301292064,
            281377452,
            275974132
        ]
    },
    {
        "id": 320968191,
        "title": "Semi-Supervised Deep Learning for Monocular Depth Map Prediction",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Yevhen Kuznietsov",
            "J\u00f6rg St\u00fcckler",
            "Bastian Leibe"
        ],
        "references": [
            304409658,
            303750237,
            301880609,
            301876860,
            301292064,
            268747978,
            265295439,
            261989114,
            220659394,
            215616968
        ]
    },
    {
        "id": 320915709,
        "title": "Feature Visualization",
        "abstract": "",
        "date": "November 2017",
        "authors": [
            "Chris Olah",
            "Alexander Mordvintsev",
            "Ludwig Schubert"
        ],
        "references": [
            307560165,
            307822569
        ]
    },
    {
        "id": 319770337,
        "title": "Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue",
        "abstract": "A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known interview displacement, to reconstruct the source image; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset (without any further augmentation) gives com- parable performance to that of the state of art supervised methods for single view depth estimation.",
        "date": "March 2016",
        "authors": [
            "Ravi Garg",
            "Vijay Kumar BG",
            "Ian Reid"
        ],
        "references": []
    },
    {
        "id": 303367197,
        "title": "One-shot Learning with Memory-Augmented Neural Networks",
        "abstract": "Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",
        "date": "May 2016",
        "authors": [
            "Adam Santoro",
            "Sergey Bartunov",
            "Matthew Botvinick",
            "Daan Wierstra"
        ],
        "references": [
            292074166,
            228714103,
            321613968,
            319770272,
            286764893,
            272837232,
            272027131,
            267157056,
            262243796,
            248592315
        ]
    },
    {
        "id": 285764506,
        "title": "One shot learning of simple visual concepts",
        "abstract": "",
        "date": "January 2011",
        "authors": [
            "B.M. Lake",
            "R. Salakhutdinov",
            "J. Gross",
            "J.B. Tenenbaum"
        ],
        "references": []
    },
    {
        "id": 285648386,
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error.",
        "date": "December 2015",
        "authors": [
            "Christian Szegedy",
            "Vincent Vanhoucke",
            "Sergey Ioffe",
            "Jonathon Shlens"
        ],
        "references": [
            305196650,
            275279789,
            270454670,
            265787949,
            259212328,
            233730646,
            319770430,
            319770291,
            319770272,
            319770198
        ]
    },
    {
        "id": 284476538,
        "title": "Order Matters: Sequence to sequence for sets",
        "abstract": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.",
        "date": "November 2015",
        "authors": [
            "Oriol Vinyals",
            "Samy Bengio",
            "Manjunath Kudlur"
        ],
        "references": [
            319770438,
            319770160,
            265252627,
            262877889,
            228569700,
            13853244,
            319770465,
            289758666,
            281144792,
            275896989
        ]
    },
    {
        "id": 257672261,
        "title": "Image Classification with the Fisher Vector: Theory and Practice",
        "abstract": "A standard approach to describe an image for classification and retrieval purposes is to extract a set of local patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most common patch encoding strategy consists in quantizing the local descriptors into a finite set of prototypical elements. This leads to the popular Bag-of-Visual words representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch encoding strategy: we describe patches by their deviation from an \u201cuniversal\u201d generative Gaussian mixture model. This representation, which we call Fisher vector has many advantages: it is efficient to compute, it leads to excellent results even with efficient linear classifiers, and it can be compressed with a minimal loss of accuracy using product quantization. We report experimental results on five standard datasets\u2014PASCAL VOC 2007, Caltech 256, SUN 397, ILSVRC 2010 and ImageNet10K\u2014with up to 9M images and 10K classes, showing that the FV framework is a state-of-the-art patch encoding technique.",
        "date": "December 2013",
        "authors": [
            "Jorge S\u00e1nchez",
            "Florent Perronnin",
            "Thomas Mensink",
            "Jakob Verbeek"
        ],
        "references": [
            277292831,
            319770628,
            319770432,
            319770425,
            319770349,
            319770183,
            312973075,
            311344255,
            311175293,
            283949555
        ]
    },
    {
        "id": 286976996,
        "title": "Mid-level visual element discovery as discriminative mode seeking",
        "abstract": "Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical \"visual words\", but lower than full-blown semantic objects. Several approaches [5, 6, 12, 23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset.",
        "date": "January 2013",
        "authors": [
            "Carl Doersch",
            "A. Gupta",
            "A.A. Efros"
        ],
        "references": [
            241770100,
            215705221
        ]
    },
    {
        "id": 261306213,
        "title": "SUN attribute database: Discovering, annotating, and recognizing scene attributes",
        "abstract": "In this paper we present the first large-scale scene attribute database. First, we perform crowd-sourced human studies to find a taxonomy of 102 discriminative attributes. Next, we build the \u201cSUN attribute database\u201d on top of the diverse SUN categorical database. Our attribute database spans more than 700 categories and 14,000 images and has potential for use in high-level scene understanding and fine-grained scene recognition. We use our dataset to train attribute classifiers and evaluate how well these relatively simple classifiers can recognize a variety of attributes related to materials, surface properties, lighting, functions and affordances, and spatial envelope properties.",
        "date": "June 2012",
        "authors": [
            "Genevieve Patterson",
            "J. Hays"
        ],
        "references": [
            228935216,
            227943302,
            224135964,
            221364579,
            221363513,
            221361415,
            221111179,
            310439564,
            267400375,
            262276816
        ]
    },
    {
        "id": 322059850,
        "title": "RoomNet: End-to-End Room Layout Estimation",
        "abstract": "",
        "date": "October 2017",
        "authors": [
            "Chen-Yu Lee",
            "Vijay Badrinarayanan",
            "Tomasz Malisiewicz",
            "Andrew Rabinovich"
        ],
        "references": [
            322749812,
            319770420,
            319770272,
            319770268,
            311610668,
            311609522,
            311609147,
            310476586,
            310122727,
            308846031
        ]
    },
    {
        "id": 315454938,
        "title": "RoomNet: End-to-End Room Layout Estimation",
        "abstract": "This paper focuses on the task of room layout estimation from a monocular RGB image. Prior works break the problem into two sub-tasks: semantic segmentation of floor, walls, ceiling to produce layout hypotheses, followed by an iterative optimization step to rank these hypotheses. In contrast, we adopt a more direct formulation of this problem as one of estimating an ordered set of room layout keypoints. The room layout and the corresponding segmentation is completely specified given the locations of these ordered keypoints. We predict the locations of the room layout keypoints using RoomNet, an end-to-end trainable encoder-decoder network. On the challenging benchmark datasets Hedau and LSUN, we achieve state-of-the-art performance along with 200x to 600x speedup compared to the most recent work. Additionally, we present optional extensions to the RoomNet architecture such as including recurrent computations and memory units to refine the keypoint locations under the same parametric capacity.",
        "date": "March 2017",
        "authors": [
            "Chen-Yu Lee",
            "Vijay Badrinarayanan",
            "Tomasz Malisiewicz",
            "Andrew Rabinovich"
        ],
        "references": [
            322749812,
            308277898,
            319770420,
            319770272,
            311611182,
            311610668,
            310476586,
            310122727,
            308846031,
            308842686
        ]
    },
    {
        "id": 329747447,
        "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Roberto Cipolla",
            "Yarin Gal",
            "Alex Kendall"
        ],
        "references": [
            315096611,
            311842448,
            311411551,
            310953253,
            302305068,
            301880609,
            301877556,
            283471087,
            282182156,
            281670744
        ]
    },
    {
        "id": 328955291,
        "title": "UberNet: Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory",
        "abstract": "In this work we train in an end-to-end manner a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture. Such a network can act like a swiss knife for vision tasks, we call it an UberNet to indicate its overarching nature. The main contribution of this work consists in handling challenges that emerge when scaling up to many tasks. We introduce techniques that facilitate (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. This allows us to train in an end-to-end manner a unified CNN architecture that jointly handles (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all tasks in 0.7 seconds on a GPU. Our system will be made publicly available.",
        "date": "July 2017",
        "authors": [
            "Iasonas Kokkinos"
        ],
        "references": [
            311411551,
            303822170,
            301880530,
            301841861,
            301841817,
            278413297,
            274718480,
            269722851,
            268689640,
            319770420
        ]
    },
    {
        "id": 322582564,
        "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Kazuma Hashimoto",
            "caiming xiong",
            "Yoshimasa Tsuruoka",
            "Richard Socher"
        ],
        "references": [
            306093833,
            306093778,
            306093553,
            266201822,
            264003485,
            257882504,
            221102748,
            220817026,
            13853244,
            322587302
        ]
    },
    {
        "id": 320964427,
        "title": "Fully-Adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Yongxi Lu",
            "Abhishek Kumar",
            "Shuangfei Zhai",
            "Yu Cheng"
        ],
        "references": [
            306023833,
            305196650,
            303822170,
            301841817,
            301838380,
            301835547,
            288023079,
            284788333,
            283040455,
            282182150
        ]
    },
    {
        "id": 321325808,
        "title": "Separating Self-Expression and Visual Content in Hashtag Supervision",
        "abstract": "The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be ambiguous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs by modeling the joint distribution of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging.",
        "date": "November 2017",
        "authors": [
            "Andreas Veit",
            "Maximilian Nickel",
            "Serge Belongie",
            "Laurens van der Maaten"
        ],
        "references": [
            318337409,
            311969727,
            322059733,
            322058545,
            320965066,
            320075439,
            319501432,
            311609097,
            311609041,
            311449622
        ]
    },
    {
        "id": 311969727,
        "title": "Learning Visual N-Grams from Web Data",
        "abstract": "Real-world image recognition systems need to recognize tens of thousands of classes that constitute a plethora of visual concepts. The traditional approach of annotating thousands of images per class for training is infeasible in such a scenario, prompting the use of webly supervised data. This paper explores the training of image-recognition systems on large numbers of images and associated user comments. In particular, we develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image. Our visual n-gram models are feed-forward convolutional networks trained using new loss functions that are inspired by n-gram models commonly used in language modeling. We demonstrate the merits of our models in phrase prediction, phrase-based image retrieval, relating images and captions, and zero-shot transfer.",
        "date": "December 2016",
        "authors": [
            "Ang Li",
            "Allan Jabri",
            "Armand Joulin",
            "Laurens van der Maaten"
        ],
        "references": [
            319770438,
            319770237,
            319770160,
            329975395,
            322058840,
            320965066,
            319770249,
            319770220,
            319770006,
            318741395
        ]
    },
    {
        "id": 329747533,
        "title": "Separating Self-Expression and Visual Content in Hashtag Supervision",
        "abstract": "",
        "date": "June 2018",
        "authors": [
            "Andreas Veit",
            "Maximilian Nickel",
            "Serge Belongie",
            "Laurens van der Maaten"
        ],
        "references": [
            318337409,
            264495409,
            236901458,
            221619287,
            221346005,
            221345089,
            221300423,
            45919126,
            322058545,
            320965066
        ]
    },
    {
        "id": 321347616,
        "title": "Visualisation and 'Diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure",
        "abstract": "We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.",
        "date": "November 2017",
        "authors": [
            "Dieuwke Hupkes",
            "Sara Veldhoen",
            "Willem Zuidema"
        ],
        "references": [
            306187579,
            306094026,
            304470296,
            319770401,
            317723278,
            306093560,
            305334391,
            301445865,
            265554383,
            256745020
        ]
    },
    {
        "id": 318741956,
        "title": "What do Neural Machine Translation Models Learn about Morphology?",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Yonatan Belinkov",
            "Nadir Durrani",
            "Fahim Dalvi",
            "Hassan Sajjad"
        ],
        "references": [
            318742237,
            318738968,
            311990243,
            306187579,
            319770465,
            311990676,
            311990242,
            306093835,
            306093774,
            306093632
        ]
    },
    {
        "id": 315748956,
        "title": "N-gram Language Modeling using Recurrent Neural Network Estimation",
        "abstract": "We investigate the effective memory depth of RNN models by using them for n-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the n-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM n-gram matches the LSTM LM performance for n=9 and slightly outperforms it for n=13. When allowing dependencies across sentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM n-gram smoothing also has the desirable property of improving with increasing n-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low n-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale. Building LSTM n-gram LMs may be appealing for some practical situations: the state in a n-gram LM can be succinctly represented with (n-1)*4 bytes storing the identity of the words in the context and batches of n-gram contexts can be processed in parallel. On the downside, the n-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.",
        "date": "March 2017",
        "authors": [
            "Ciprian Chelba",
            "Mohammad Norouzi",
            "Samy Bengio"
        ],
        "references": [
            315654958,
            302768576,
            301839500,
            259239818,
            221618573,
            311469848,
            301847993,
            286794765,
            239060385,
            222835593
        ]
    },
    {
        "id": 306187579,
        "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
        "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.",
        "date": "August 2016",
        "authors": [
            "Yossi Adi",
            "Einat Kermany",
            "Yonatan Belinkov",
            "Ofer Lavi"
        ],
        "references": [
            305334586,
            301843081,
            319770465,
            319770411,
            307955489,
            304037295,
            301818426,
            301408902,
            284788001,
            283531914
        ]
    },
    {
        "id": 283666865,
        "title": "Structures, Not Strings: Linguistics as Part of the Cognitive Sciences",
        "abstract": "There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what 'language' is. Sharpening the core question - what is language? - and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences. Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language.",
        "date": "November 2015",
        "authors": [
            "M.B.H. Everaert",
            "Marinus A C Huybregts",
            "Noam Chomsky",
            "Robert Berwick"
        ],
        "references": [
            290539777,
            345774309,
            323838874,
            306939383,
            304405348,
            297875191,
            292714221,
            292704721,
            291018478,
            290864751
        ]
    },
    {
        "id": 271944021,
        "title": "Evidence for automatic accessing of constructional meaning: Jabberwocky sentences prime associated verbs",
        "abstract": "A central question within psycholinguistics is where sentences get their meaning. While it has been shown that phrasal constructions are readily associated with specific meanings, it remains unclear whether this meaning is accessed automatically, in the sense of being accessed quickly, and without reflection or explicit instruction. In this study, participants performed a lexical decision task on individual target words which were preceded by abstract skeletal constructions devoid of any meaningful open-class items. For example, an instance of a ditransitive prime was, He daxed her the norp. Three target words corresponded to the hypothesised meaning of each construction; that is, semantically congruent words for the English ditransitive were give, handed, and transferred. We found significant priming effects for congruent over incongruent target words, both for associated targets (which occur regularly within the construction: e.g., give and handed), and to a lesser extent, for target words that are semantically related to the construction but which rarely occur in the construction (e.g., transferred for the ditransitive).",
        "date": "December 2013",
        "authors": [
            "Matt Johnson",
            "Adele E. Goldberg"
        ],
        "references": [
            285191274,
            341690965,
            319338658,
            280801356,
            280292399,
            272816446,
            272585837,
            271018659,
            271017830,
            263191316
        ]
    },
    {
        "id": 221486318,
        "title": "IRSTLM: An open source toolkit for handling large scale language models",
        "abstract": "",
        "date": "September 2008",
        "authors": [
            "Marcello Federico",
            "Nicola Bertoldi",
            "Mauro Cettolo"
        ],
        "references": [
            253423758,
            245589415,
            221489718,
            220874004,
            220685657,
            228877275,
            221013326,
            220875255,
            220515949,
            2928640
        ]
    },
    {
        "id": 14795144,
        "title": "Learning and Development in Neural Networks: the Importance of Starting Small",
        "abstract": "It is a striking fact that in humans the greatest learning occurs precisely at that point in time--childhood--when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in connectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure. Training fails in the case of networks which are fully formed and 'adultlike' in their capacity. Training succeeds only when networks begin with limited working memory and gradually 'mature' to the adult state. This result suggests that rather than being a limitation, developmental restrictions on resources may constitute a necessary prerequisite for mastering certain complex domains. Specifically, successful learning may depend on starting small.",
        "date": "August 1993",
        "authors": [
            "Jeffrey Elman"
        ],
        "references": [
            243767157,
            239022104,
            303206000,
            247677002,
            243786282,
            243743707,
            243706571,
            242646280,
            239022098,
            239022095
        ]
    },
    {
        "id": 311990865,
        "title": "Charagram: Embedding Words and Sentences via Character n-grams",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "John Wieting",
            "Mohit Bansal",
            "Kevin Gimpel",
            "Karen Livescu"
        ],
        "references": [
            306094026,
            305334586,
            301446251,
            301408978,
            306093909,
            306093774,
            306093489,
            301847993,
            301446578,
            301445967
        ]
    },
    {
        "id": 305334586,
        "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
        "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",
        "date": "February 2016",
        "authors": [
            "Felix Hill",
            "Kyunghyun Cho",
            "Anna Korhonen"
        ],
        "references": [
            305196650,
            301408978,
            354232727,
            319770465,
            319770439,
            304037295,
            301448971,
            301432319,
            301404438,
            288449278
        ]
    },
    {
        "id": 304408934,
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "abstract": "",
        "date": "December 2015",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li"
        ],
        "references": [
            319770438,
            319770160,
            329975395,
            319770439,
            319770291,
            319770272,
            319770249,
            312727767,
            312714920,
            308871635
        ]
    },
    {
        "id": 301408978,
        "title": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity",
        "abstract": "In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings. For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire. The annotations for both tasks leveraged crowdsourcing. The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs.",
        "date": "January 2014",
        "authors": [
            "Eneko Agirre",
            "Carmen Banea",
            "Claire Cardie",
            "Daniel Cer"
        ],
        "references": [
            313621629,
            301408891,
            301408888,
            301404867,
            297768191,
            329975395,
            301404969,
            301404948,
            288381610,
            278763355
        ]
    },
    {
        "id": 266201822,
        "title": "Natural Language Processing (Almost) from Scratch",
        "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
        "date": "February 2011",
        "authors": [
            "Ronan Collobert",
            "Jason Weston",
            "Leon Bottou",
            "Michael Karlen"
        ],
        "references": [
            303137904,
            269087977,
            262408350,
            289827888,
            288905957,
            284761081,
            284266320,
            277299370,
            262406121,
            250754760
        ]
    },
    {
        "id": 262484733,
        "title": "A SICK cure for the evaluation of compositional distributional semantic models.",
        "abstract": "",
        "date": "May 2014",
        "authors": [
            "Marco Marelli",
            "Stefano Menini",
            "Marco Baroni",
            "Luisa Bentivogli"
        ],
        "references": [
            264003485,
            262174841,
            251507221,
            242146550,
            221366753,
            319394610,
            262367926,
            262235223,
            234820797,
            221012988
        ]
    },
    {
        "id": 312250769,
        "title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Shereen M. Oraby",
            "Vrindavan Harrison",
            "Lena Reed",
            "Ernesto Hernandez"
        ],
        "references": [
            319349800,
            305334464,
            280997312,
            278327496,
            272091377,
            305334391,
            303360625,
            284039049,
            283524147,
            277722713
        ]
    },
    {
        "id": 311990263,
        "title": "emoji2vec: Learning Emoji Representations from their Description",
        "abstract": "Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard. The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation.",
        "date": "January 2016",
        "authors": [
            "Ben Eisner",
            "Tim Rockt\u00e4schel",
            "Isabelle Augenstein",
            "Matko Bosnjak"
        ],
        "references": [
            306396532,
            306284202,
            304128217,
            303520355,
            301446107,
            284576917,
            319770369,
            306093838,
            301408902,
            285895924
        ]
    },
    {
        "id": 307512566,
        "title": "Emotion Analysis as a Regression Problem \u2014 Dimensional Models and Their Implications on Emotion Representation and Metrical Evaluation",
        "abstract": "Emotion analysis (EA) and sentiment analysis are closely related tasks differing in the psychological phenomenon they aim to catch. We address fine-grained models for EA which treat the computation of the emotional status of narrative documents as a regression rather than a classification problem, as performed by coarse-grained approaches. We introduce Ekman's Basic Emotions (BE) and Russell and Mehrabian's Valence-Arousal-Dominance (VAD) model\u2014two major schemes of emotion representation following opposing lines of psychological research, i.e., categorical and dimensional models\u2014and discuss problems when BEs are used in a regression approach. We present the first natural language system thoroughly evaluated for fine-grained emotion analysis using the VAD scheme. Although we only employ simple BOW features, we reach correlation values up until r = .65 with human annotations. Furthermore, we show that the prevailing evaluation methodology relying solely on Pearson's correlation coefficient r is deficient which leads us to the introduction of a complementary error-based metric. Due to the lack of comparable (VAD-based) systems, we, finally, introduce a novel method of mapping between VAD and BE emotion representations to create a reasonable basis for comparison. This enables us to evaluate VAD output against human BE judgments and, thus, allows for a more direct comparison with existing BE-based emotion analysis systems. Even with this, admittedly, error-prone transformation step our VAD-based system achieves state-of-the-art performance in three out of six emotion categories, out-performing all existing BE-based systems but one.",
        "date": "August 2016",
        "authors": [
            "Sven Buechel",
            "Udo Hahn"
        ],
        "references": [
            306093845,
            306093587,
            273959449,
            262174110,
            257405079,
            254746105,
            252117816,
            303802749,
            264387673,
            262145566
        ]
    },
    {
        "id": 305767979,
        "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
        "abstract": "http://aclweb.org/anthology/D/D16/D16-1104.pdf This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection.",
        "date": "November 2016",
        "authors": [
            "Aditya Joshi",
            "Kevin Patel",
            "Vaibhav Tripathi",
            "Pushpak Bhattacharyya"
        ],
        "references": [
            303360388,
            281492671,
            280997312,
            280997051,
            270877582,
            303360625,
            301845949,
            301409120,
            286184998,
            270878439
        ]
    },
    {
        "id": 305342002,
        "title": "SwissCheese at SemEval-2016 Task 4: Sentiment Classification Using an Ensemble of Convolutional Neural Networks with Distant Supervision",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Jan Deriu",
            "Maurice Gonzenbach",
            "Fatih Uzdilli",
            "Aurelien Lucchi"
        ],
        "references": [
            305334406,
            284576917,
            275886528,
            266560893,
            257882504,
            301533328,
            291044875,
            265052545,
            261475541,
            256663215
        ]
    },
    {
        "id": 305334406,
        "title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Preslav Nakov",
            "Alan Ritter",
            "Sara Rosenthal",
            "Fabrizio Sebastiani"
        ],
        "references": [
            305342002,
            305334621,
            305334619,
            305334618,
            305334602,
            305334600,
            305334623,
            305334622,
            305334620,
            305334601
        ]
    },
    {
        "id": 278654387,
        "title": "Distant Supervision for Emotion Classification with Discrete Binary Values",
        "abstract": "In this paper, we present an experiment to identify emotions in tweets. Unlike previous studies, which typically use the six basic emotion classes defined by Ekman, we classify emotions according to a set of eight basic bipolar emotions defined by Plutchik (Plutchik\u2019s \u201cwheel of emotions\u201d). This allows us to treat the inherently multi-class problem of emotion classification as a binary problem for four opposing emotion pairs. Our approach applies distant supervision, which has been shown to be an effective way to overcome the need for a large set of manually labeled data to produce accurate classifiers. We build on previous work by treating not only emoticons and hashtags but also emoji, which are increasingly used in social media, as an alternative for explicit, manual labels. Since these labels may be noisy, we first perform an experiment to investigate the correspondence among particular labels of different types assumed to be indicative of the same emotion. We then test and compare the accuracy of independent binary classifiers for each of Plutchik\u2019s four binary emotion pairs trained with different combinations of label types. Our best performing classifiers produce results between 75-91%, depending on the emotion pair; these classifiers can be combined to emulate a single multi-label classifier for Plutchik\u2019s eight emotions that achieves accuracies superior to those reported in previous multi-way classification studies.",
        "date": "March 2013",
        "authors": [
            "Jared Suttles",
            "Nancy Ide"
        ],
        "references": [
            269670995,
            267841873,
            258762213,
            256926649,
            255588604,
            255564129,
            254199702,
            312893386,
            307948199,
            284740464
        ]
    },
    {
        "id": 265382496,
        "title": "The (Un)Predictability of Emotional Hashtags in Twitter",
        "abstract": "Hashtags in Twitter posts may carry different semantic payloads. Their dual form (word and label) may serve to categorize the tweet, but may also add content to the message, or strengthen it. Some hashtags are related to emotions. In a study on emotional hashtags in Dutch Twitter posts we employ machine learning classifiers to test to what extent tweets that are stripped from their hashtag could be reassigned to this hashtag. About half of the 24 tested hashtags can be predicted with AUC scores of .80 or higher. However, when we apply the three best-performing classifiers to unseen tweets that do not carry the hashtag but might have carried it according to human annotators, the classifiers manage to attain a precision-at-250 of .7 for only two of the hashtags. We observe that some hashtags are predictable from their tweets, and strengthen the emotion already expressed in the tweets. Other hashtags are added to messages that do not predict them, presumably to provide emotional information that was not yet in the tweet.",
        "date": "April 2014",
        "authors": [
            "Florian Kunneman",
            "Christine Liebrecht",
            "Antal Van den Bosch"
        ],
        "references": [
            284043217,
            278654387,
            267841873,
            265804086,
            265382497,
            258762213,
            255729692,
            254199702,
            247935218,
            245032586
        ]
    },
    {
        "id": 259261310,
        "title": "Evaluation Datasets for Twitter Sentiment Analysis. A survey and a new dataset, the STS-Gold",
        "abstract": "Sentiment analysis over Twitter offers organisations and indi-viduals a fast and effective way to monitor the publics' feelings towards them and their competitors. To assess the performance of sentiment analysis methods over Twitter a small set of evaluation datasets have been released in the last few years. In this paper we present an overview of eight publicly available and manually annotated evaluation datasets for Twitter sentiment analysis. Based on this review, we show that a common limitation of most of these datasets, when assessing sentiment analysis at target (entity) level, is the lack of distinctive sentiment annotations among the tweets and the entities contained in them. For example, the tweet \"I love iPhone, but I hate iPad\" can be annotated with a mixed sentiment label, but the entity iPhone within this tweet should be anno-tated with a positive sentiment label. Aiming to overcome this limitation, and to complement current evaluation datasets, we present STS-Gold, a new evaluation dataset where tweets and targets (entities) are annotated individually and therefore may present different sentiment labels. This paper also provides a comparative study of the various datasets along several dimensions including: total number of tweets, vocabulary size and sparsity. We also investigate the pair-wise correlation among these dimensions as well as their correlations to the sentiment classification performance on different datasets.",
        "date": "December 2013",
        "authors": [
            "Hassan Saif",
            "Miriam Fernandez",
            "Yulan He",
            "Harith Alani"
        ],
        "references": [
            301408884,
            276291409,
            268133308,
            262399495,
            262220655,
            262154706,
            257915874,
            265945496,
            262235493,
            262234581
        ]
    },
    {
        "id": 332551012,
        "title": "High Accuracy Rule-based Question Classification using Question Syntax and Semantics",
        "abstract": "We present in this paper a purely rule-based system for Question Classification which we divide into two parts: The first is the extraction of relevant words from a question by use of its structure,and the second is the classification of questions based on rules that associate these words toConcepts. We achieve an accuracy of 97.2%, close to a 6 point improvement over the previous State of the Art of 91.6%. Additionally, we believe that machine learning algorithms can be applied on top of this method to further improve accuracy. Question Classification API available at: http://www.harishmadabushi.com/research/questionclassification/question-classification-api-documentation/",
        "date": "December 2016",
        "authors": [
            "Harish Tayyar Madabushi",
            "Mark G. Lee"
        ],
        "references": [
            305334631,
            303553351,
            272091377,
            221614151,
            221481178,
            305067205,
            303761649,
            300415050,
            265052545,
            230854746
        ]
    },
    {
        "id": 316591288,
        "title": "The representational geometry of word meanings acquired by neural machine translation models",
        "abstract": "This work is the first comprehensive analysis of the properties of word embeddings learned by neural machine translation (NMT) models trained on bilingual texts. We show the word representations of NMT models outperform those learned from monolingual text by established algorithms such as Skipgram and CBOW on tasks that require knowledge of semantic similarity and/or lexical\u2013syntactic role. These effects hold when translating from English to French and English to German, and we argue that the desirable properties of NMT word embeddings should emerge largely independently of the source and target languages. Further, we apply a recently-proposed heuristic method for training NMT models with very large vocabularies, and show that this vocabulary expansion method results in minimal degradation of embedding quality. This allows us to make a large vocabulary of NMT embeddings available for future research and applications. Overall, our analyses indicate that NMT embeddings should be used in applications that require word concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.",
        "date": "June 2017",
        "authors": [
            "Felix Hill",
            "Kyunghyun Cho",
            "S\u00e9bastien Jean",
            "Y. Bengio"
        ],
        "references": [
            284576917,
            281458937,
            275330072,
            270877878,
            319770465,
            307955489,
            306146231,
            289758666,
            285932285,
            270878439
        ]
    },
    {
        "id": 313481320,
        "title": "Deep Learning with Dynamic Computation Graphs",
        "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.",
        "date": "February 2017",
        "authors": [
            "Moshe Looks",
            "Marcello Herreshoff",
            "DeLesley Hutchins",
            "Peter Norvig"
        ],
        "references": [
            306094026,
            302569301,
            301847798,
            301840001,
            301839500,
            320736303,
            319770411,
            319770111,
            305388779,
            305334592
        ]
    },
    {
        "id": 311458944,
        "title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning",
        "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as \"the\" and \"of\". Other words that may seem visual can often be predicted reliably just from the language model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following \"talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
        "date": "December 2016",
        "authors": [
            "Jiasen Lu",
            "Caiming Xiong",
            "Devi Parikh",
            "Richard Socher"
        ],
        "references": [
            319770438,
            319770160,
            329975395,
            319770465,
            319770249,
            319770231,
            319770006,
            311610253,
            311609195,
            308964930
        ]
    },
    {
        "id": 310329039,
        "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference",
        "abstract": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.",
        "date": "November 2016",
        "authors": [
            "Biswajit Paria",
            "K. M. Annervaz",
            "Ambedkar Dukkipati",
            "Ankush Chatterjee"
        ],
        "references": [
            306094026,
            301839500,
            288713397,
            320736303,
            319770439,
            312416396,
            311990523,
            305334592,
            303840186,
            301880883
        ]
    },
    {
        "id": 308361705,
        "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference",
        "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. With the availability of large annotated data, neural network models have recently advanced the field significantly. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.3% on the standard benchmark, the Stanford Natural Language Inference dataset. This result is achieved first through our enhanced sequential encoding model, which outperforms the previous best model that employs more complicated network architectures, suggesting that the potential of sequential LSTM-based models have not been fully explored yet in previous work. We further show that by explicitly considering recursive architectures, we achieve additional improvement. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added to an already very strong system.",
        "date": "September 2016",
        "authors": [
            "Qian Chen",
            "Xiaodan Zhu",
            "Zhen-Hua Ling",
            "Si Wei"
        ],
        "references": [
            309461067,
            306094026,
            320736303,
            318739728,
            312416396,
            311990523,
            305388779,
            305334400,
            303840186,
            303681701
        ]
    },
    {
        "id": 233010375,
        "title": "Accurate RMM-Based Approximations for the CDF of the Normal Distribution",
        "abstract": "A variation of the RMM error distribution, used to model the exponential distribution, has recently been applied to derive a three-parameter approximation for the standard normal CDF, with a maximum absolute error of order (10). In this short communication, a simple modification enhances the accuracy to the order of (10). Another RMM-based approximation, based on the original RMM error distribution, achieves an absolute maximum error of (10). The simplicity of the new non-polynomial approximations qualifies them to be conveniently integrated into stochastic optimization models (like inventory models), or to be used in applications. That modeling of the exponential distribution via the RMM model could produce such highly accurate approximation for the standard normal CDF seems to lend further validity to the RMM model.",
        "date": "March 2005",
        "authors": [
            "Haim Shore"
        ],
        "references": [
            311653017,
            233918439,
            233918401,
            233156609,
            324426835,
            284073012,
            269024002,
            230663741
        ]
    },
    {
        "id": 324350486,
        "title": "The Art of Simulation.",
        "abstract": "",
        "date": "January 1964",
        "authors": [
            "S. Vajda",
            "K. D. Tocher"
        ],
        "references": []
    },
    {
        "id": 271760337,
        "title": "A Simpler Logistic Approximation to the Normal Tail Probability and its Inverse",
        "abstract": "This paper proposes a logistic approximation to the normal tail probability and its inverse with greater simplicity and covenience. Its accuracy is quite sufficient for many practical cases.",
        "date": "January 1990",
        "authors": [
            "Jinn-Tyan Lin"
        ],
        "references": [
            271760208,
            271686558
        ]
    },
    {
        "id": 271760208,
        "title": "Approximating the Cumulative Normal Distribution and Its Inverse",
        "abstract": "This note proposes an approximation to the cumulative normal distribution and its inverse which, according to circumstances, is simpler, more accurate, or more convenient than existing formulae.",
        "date": "January 1978",
        "authors": [
            "Hugo C. Hamaker"
        ],
        "references": [
            270365572
        ]
    },
    {
        "id": 271686558,
        "title": "Approximating the Normal Tail Probability and Its Inverse for Use on a Pocket Calculator",
        "abstract": "This paper proposes a simple approximation to the normal tail probability and its inverse. Its accuracy is quite sufficient for many practical cases.",
        "date": "January 1989",
        "authors": [
            "Jinn-Tyan Lin"
        ],
        "references": [
            271760208,
            270365572
        ]
    },
    {
        "id": 270365572,
        "title": "Approximations to the Cumulative Normal Function and its Inverse for Use on a Pocket Calculator",
        "abstract": "",
        "date": "January 1977",
        "authors": [
            "E. Page"
        ],
        "references": [
            243386634,
            242503838,
            242490966,
            240016571,
            220424897
        ]
    },
    {
        "id": 269024002,
        "title": "Calculating Normal Probabilities",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "Richard J. Bagby"
        ],
        "references": []
    },
    {
        "id": 268299777,
        "title": "A Note on Approximating the Normal Distribution Function",
        "abstract": "In this paper, we propose a one-term-to-calculate approximation to the normal cumulative distribution function. Our approximation has a maximum absolute error of .00197323. We compare our approximation to the exact one.",
        "date": "January 2008",
        "authors": [
            "K M Aludaat",
            "Moh'D Taleb Alodat"
        ],
        "references": [
            23742014,
            324323467,
            279404188,
            230663741
        ]
    },
    {
        "id": 239652405,
        "title": "Analytic Theory Of Continued Fractions",
        "abstract": "",
        "date": "January 1967",
        "authors": [
            "H. S. Wall"
        ],
        "references": []
    },
    {
        "id": 222675750,
        "title": "A uniform approximation to the right normal tail integral",
        "abstract": "Two simple formulas for approximation of the standard normal right tail probabilities and the method of computing approximations of higher order were presented. One of the approximations relied on two simple numerical constants and had absolute and relative errors of 0.00071 and 0.023 respectively. The other approximation used four numerical constants and had absolute and relative errors of 0.000019 and 0.005 respectively. The approximation gave at least two significant digits for probabilities over the entire range 0 \u2264 Z < \u221e.",
        "date": "April 2002",
        "authors": [
            "Wlodzimierz Bryc"
        ],
        "references": [
            242790458,
            324932789,
            324392055,
            274652649,
            271760337,
            271760208,
            271686558,
            270365572,
            270237180,
            269502062
        ]
    },
    {
        "id": 309424668,
        "title": "Surprisal-Driven Zoneout",
        "abstract": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states \\textit{zoneout} (maintain their previous value rather than updating), when the \\textit{suprisal} (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.32 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.",
        "date": "October 2016",
        "authors": [
            "Kamil Rocki",
            "Tomasz Kornuta"
        ],
        "references": [
            306376809,
            305186382,
            304226007,
            303821573,
            272195367,
            243712413,
            307861335,
            279864537,
            238651290,
            221345823
        ]
    },
    {
        "id": 301878902,
        "title": "Recurrent Batch Normalization",
        "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.",
        "date": "March 2016",
        "authors": [
            "Tim Cooijmans",
            "Nicolas Ballas",
            "C\u00e9sar Laurent",
            "Caglar Gulcehre"
        ],
        "references": [
            319770179,
            301872874,
            286513561,
            284476210,
            277603270,
            273005574,
            285270893,
            282691590,
            278048272,
            274645044
        ]
    },
    {
        "id": 301840001,
        "title": "Recurrent Dropout without Memory Loss",
        "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.",
        "date": "March 2016",
        "authors": [
            "Stanislau Semeniuta",
            "Aliaksei Severyn",
            "Erhardt Barth"
        ],
        "references": [
            308849574,
            301445897,
            301404520,
            283471647,
            319770465,
            308859819,
            307955489,
            291412150,
            291044875,
            281312208
        ]
    },
    {
        "id": 300412439,
        "title": "RnnDrop: a novel dropout for RNNs in ASR",
        "abstract": "",
        "date": "December 2015",
        "authors": [
            "Taesup Moon",
            "Heeyoul \"Henry\" Choi",
            "Hoshik Lee",
            "Inchul Song"
        ],
        "references": [
            272194766,
            267515250,
            311469848,
            302469019,
            287034172,
            286794765,
            265469170,
            265178583,
            262030045,
            261500539
        ]
    },
    {
        "id": 319769994,
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.",
        "date": "December 2016",
        "authors": [
            "Yarin Gal",
            "Zoubin Ghahramani"
        ],
        "references": []
    },
    {
        "id": 307861335,
        "title": "Hierarchical Multiscale Recurrent Neural Networks",
        "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.",
        "date": "September 2016",
        "authors": [
            "Junyoung Chung",
            "Sungjin Ahn",
            "Y. Bengio"
        ],
        "references": [
            319770160,
            307747289,
            306376809,
            305186382,
            304226007,
            303821573,
            303367197,
            302569301,
            301879329,
            301878902
        ]
    },
    {
        "id": 284476548,
        "title": "All you need is a good init",
        "abstract": "Layer-sequential unit-variance (LSUV) initialization -- a simple method for weight initialization for deep net learning -- is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "date": "May 2016",
        "authors": [
            "Dmytro Mishkin",
            "Jiri Matas"
        ],
        "references": [
            305196650,
            303137904,
            319770418,
            319770387,
            319770272,
            319770257,
            319770191,
            319769909,
            304372567,
            286512696
        ]
    },
    {
        "id": 220873867,
        "title": "Learning Word Vectors for Sentiment Analysis",
        "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
        "date": "January 2011",
        "authors": [
            "Andrew L. Maas",
            "Raymond Daly",
            "Peter T. Pham",
            "Dan MEI Huang"
        ],
        "references": [
            221620547,
            221618573,
            344680100,
            303802749,
            262175901,
            243134278,
            237971573,
            225818196,
            221618817,
            221604065
        ]
    },
    {
        "id": 301872762,
        "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
        "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",
        "date": "February 2016",
        "authors": [
            "Tim Salimans",
            "Diederik P. Kingma"
        ],
        "references": [
            284579529,
            284476548,
            267338836,
            265908778,
            236736831,
            235639035,
            229328831,
            215616968,
            13853244,
            2722445
        ]
    },
    {
        "id": 287249598,
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
        "abstract": "A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.",
        "date": "December 2015",
        "authors": [
            "Yarin Gal"
        ],
        "references": [
            308849574,
            284562792,
            277022910,
            262877889,
            258247515,
            228102719,
            220017637,
            13853244,
            289758666,
            286794765
        ]
    },
    {
        "id": 287034172,
        "title": "Fast dropout training",
        "abstract": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.",
        "date": "January 2013",
        "authors": [
            "Sida Wang",
            "C.D. Manning"
        ],
        "references": [
            220873692,
            225767457,
            3114053
        ]
    },
    {
        "id": 272832134,
        "title": "The Strength of Weak Learnability",
        "abstract": "This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error \u2208.",
        "date": "June 1990",
        "authors": [
            "Robert E Schapire"
        ],
        "references": [
            221497249,
            346063577,
            248079943,
            243779752,
            243528739,
            234793335,
            228057769,
            223660924,
            222633363,
            221996154
        ]
    },
    {
        "id": 281764031,
        "title": "Learning with marginalized corrupted features",
        "abstract": "The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples - which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distributionessentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.",
        "date": "January 2013",
        "authors": [
            "Laurens van der Maaten",
            "Minmin Chen",
            "Stephen Tyree",
            "Kilian Weinberger"
        ],
        "references": [
            319770395,
            313763061,
            313601183,
            306218037,
            282707871,
            277802802,
            265748773,
            262410247,
            239295682,
            236736798
        ]
    },
    {
        "id": 246546737,
        "title": "Dropout Training as Adaptive Regularization",
        "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learner, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.",
        "date": "July 2013",
        "authors": [
            "Stefan Wager",
            "Sida Wang",
            "Percy Liang"
        ],
        "references": [
            281764031,
            260428601,
            237619703,
            228915801,
            319770418,
            286553394,
            262204398,
            245665568,
            245664723,
            230801973
        ]
    },
    {
        "id": 235639035,
        "title": "Maxout Networks",
        "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
        "date": "February 2013",
        "authors": [
            "Ian Goodfellow",
            "David Warde-Farley",
            "Mehdi Mirza",
            "Aaron Courville"
        ],
        "references": [
            234131114,
            234131062,
            228467601,
            228102719,
            306218037,
            303256841,
            267960550,
            266031774,
            265748773,
            234131103
        ]
    },
    {
        "id": 227716504,
        "title": "Marginalized Denoising Autoencoders for Domain Adaptation",
        "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.",
        "date": "June 2012",
        "authors": [
            "Minmin Chen",
            "Zhixiang Xu",
            "Kilian Weinberger",
            "Fei Sha"
        ],
        "references": [
            234574749,
            224579211,
            307881957,
            305386599,
            303256841,
            272623654,
            248512106,
            229091480,
            228715647,
            223117716
        ]
    },
    {
        "id": 41040261,
        "title": "Sex, mixability, and modularity",
        "abstract": "The assumption that different genetic elements can make separate contributions to the same quantitative trait was originally made in order to reconcile biometry and Mendelism and ever since has been used in population genetics, specifically for the trait of fitness. Here we show that sex is responsible for the existence of separate genetic effects on fitness and, more generally, for the existence of a hierarchy of genetic evolutionary modules. Using the tools developed in the process, we also demonstrate that in terms of their fitness effects, separation and fusion of genes are associated with the increase and decrease of the recombination rate between them, respectively. Implications for sex and evolution theory are discussed.",
        "date": "January 2010",
        "authors": [
            "Adi Livnat",
            "Christos Papadimitriou",
            "Nicholas Pippenger",
            "Marcus W Feldman"
        ],
        "references": [
            274358132,
            273144939,
            272161442,
            270970552,
            262687696,
            247937265,
            247585224,
            234227829,
            225397062,
            216633076
        ]
    },
    {
        "id": 221345874,
        "title": "Nightmare at test time: robust learning by feature deletion",
        "abstract": "When constructing a classifler from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifler. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness speciflcally tailored to the classi- flcation task at hand. In this work, we in- troduce a new algorithm for avoiding single feature over-weighting by analyzing robust- ness using a game theoretic formalization. We develop classiflers which are optimally re- silient to deletion of features in a minimax sense, and show how to construct such clas- siflers using quadratic programming. We il- lustrate the applicability of our methods on spam flltering and handwritten digit recogni- tion tasks, where feature deletion is indeed a realistic noise model.",
        "date": "January 2006",
        "authors": [
            "Amir Globerson",
            "Sam T. Roweis"
        ],
        "references": [
            228615617,
            221619995,
            221617803,
            220815408,
            216792818,
            2948417,
            313173689,
            246355507,
            221996831,
            220320996
        ]
    },
    {
        "id": 216792879,
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "date": "December 1989",
        "authors": [
            "Yann Lecun",
            "Bernhard E. Boser",
            "John Denker",
            "Don Henderson"
        ],
        "references": [
            265896254,
            221619244,
            216792893,
            216792889,
            285058764,
            262360039,
            234830992,
            232585117,
            220660121,
            216792890
        ]
    },
    {
        "id": 278641739,
        "title": "Deep Read",
        "abstract": "",
        "date": "January 1999",
        "authors": [
            "Lynette Hirschman",
            "Marc Light",
            "Eric Breck",
            "John Burger"
        ],
        "references": [
            220825492,
            324317090,
            245749548,
            229101051,
            222340596,
            220825532,
            2662390,
            2646814,
            2551551,
            2480686
        ]
    },
    {
        "id": 301405017,
        "title": "Freebase QA: Information Extraction or Semantic Parsing?",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Xuchen Yao",
            "Jonathan Berant",
            "Benjamin Van Durme"
        ],
        "references": [
            221466796,
            221022617,
            51942217,
            289614552,
            282162224,
            270878664,
            270878429,
            270877947,
            262425032,
            228333647
        ]
    },
    {
        "id": 301404448,
        "title": "Machine Comprehension with Discourse Relations",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Karthik Narasimhan",
            "Regina Barzilay"
        ],
        "references": [
            272091377,
            270878554,
            262245231,
            229060015,
            220746875,
            301404907,
            286965813,
            284753918,
            270877739,
            238669513
        ]
    },
    {
        "id": 297123726,
        "title": "Updating Quasi-Newton Matrices with Limited Storage",
        "abstract": "",
        "date": "January 1980",
        "authors": [
            "J NOCEDAL"
        ],
        "references": [
            247023292,
            227872018,
            227163403,
            226746153,
            31248148
        ]
    },
    {
        "id": 286965813,
        "title": "MCTest: A challenge dataset for the open-domain machine comprehension of text",
        "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
        "date": "January 2013",
        "authors": [
            "M. Richardson",
            "C.J.C. Burges",
            "Erin Renshaw"
        ],
        "references": [
            224214742,
            221366753,
            2888359,
            2568471,
            231930841,
            221346310,
            221112397,
            220873595,
            220597379
        ]
    },
    {
        "id": 282162224,
        "title": "Semantic parsing on freebase from question-answer pairs",
        "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.",
        "date": "January 2013",
        "authors": [
            "J. Berant",
            "A. Chou",
            "R. Frostig",
            "P. Liang"
        ],
        "references": [
            267557990,
            266596637,
            220698997,
            51942217,
            279835246,
            262359249,
            234774008,
            221013098,
            221013062,
            220874622
        ]
    },
    {
        "id": 278656150,
        "title": "QA4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation",
        "abstract": "This paper describes the methodology for testing the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. This was the attempt of the QA4MRE challenge which was run as a Lab at CLEF 2011\u20132013. The traditional QA task was replaced by a new Machine Reading task, whose intention was to ask questions that required a deep knowledge of individual short texts and in which systems were required to choose one answer, by analysing the corresponding test document in conjunction with background text collections provided by the organization. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer\u2019s disease, and Entrance Exams. This paper describes their motivation, their goals, their methodology for preparing the data sets, their background collections, their metrics used for the evaluation, and the lessons learned along these three years.",
        "date": "September 2013",
        "authors": [
            "Anselmo Pe\u00f1as",
            "Eduard Hovy",
            "Pamela Forner",
            "\u00c1lvaro Rodrigo"
        ],
        "references": [
            267787093,
            227299448,
            221366753,
            221159740,
            221159652,
            220873174,
            220605292,
            290016859,
            290016854,
            221159589
        ]
    },
    {
        "id": 275504235,
        "title": "Frame-Semantic Parsing",
        "abstract": "Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model that takes lexical targets (i.e., content words and phrases) in their sentential contexts and predicts frame-semantic structures. Given a target in context, the first stage disambiguates it to a semantic frame. This model uses latent variables and semi-supervised learning to improve frame disambiguation for targets unseen at training time. The second stage finds the target's locally expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm collectively predicts all the arguments of a frame at once in order to respect declaratively stated linguistic constraints, resulting in qualitatively better structures than naive local predictors. Both components are feature-based and discriminatively trained on a small set of annotated frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We have released our frame-semantic parser as open-source software.",
        "date": "March 2014",
        "authors": [
            "Dipanjan Das",
            "Desai Chen",
            "Andr\u00e9 F. T. Martins",
            "Nathan Schneider"
        ],
        "references": [
            284652014,
            271452328,
            269077319,
            269033404,
            262205036,
            262161871,
            245460942,
            242582900,
            238675708,
            238367825
        ]
    },
    {
        "id": 256199468,
        "title": "Computing Lexical Contrast",
        "abstract": "Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually-created lexicons focus on opposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such as antipodals, complementaries, and gradable. However, existing lexicons often do not classify opposites into the different kinds. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm freezing}. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, $A$ and $B$, are contrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and $C$ are strongly related and $B$ and $D$ are strongly related. (For example, there exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm tropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm cold}.) We will call this the contrast hypothesis. We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we flesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods.",
        "date": "August 2013",
        "authors": [
            "Saif M. Mohammad",
            "Bonnie J. Dorr",
            "Graeme Hirst",
            "Peter David Turney"
        ],
        "references": [
            257547187,
            255665099,
            255663572,
            249745008,
            276228055,
            273268263,
            269851677,
            269447724,
            247484509,
            243769307
        ]
    },
    {
        "id": 220813820,
        "title": "Turing Test Considered Harmful.",
        "abstract": "Passing the Turing Test is not a sensible goal for Artificial Intelligence. Adherence to Turing&apos;s vision from 1950 is now actively harmful to our field. We review problems with Turing&apos;s idea, and suggest that, ironically, the very cognitive science that he tried to create must reject his research goal. 1",
        "date": "January 1995",
        "authors": [
            "Patrick Hayes",
            "Kenneth M. Ford"
        ],
        "references": [
            248987574,
            231912320,
            224982542,
            220688799
        ]
    },
    {
        "id": 220355269,
        "title": "Similarity of Semantic Relations",
        "abstract": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.",
        "date": "September 2006",
        "authors": [
            "Peter David Turney"
        ],
        "references": [
            251191738,
            248832100,
            243783152,
            321274467,
            285680823,
            285341127,
            273068944,
            269953350,
            243786557,
            243785612
        ]
    },
    {
        "id": 1895883,
        "title": "A Collection of Definitions of Intelligence",
        "abstract": "This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.",
        "date": "July 2007",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "references": [
            315899989,
            289963628,
            289963230,
            274808425,
            273227053,
            273108975,
            268745264,
            266211081,
            259981911,
            247916356
        ]
    },
    {
        "id": 329975850,
        "title": "Parsing Algebraic Word Problems into Equations",
        "abstract": "This paper formalizes the problem of solving multi-sentence algebraic word problems as that of generating and scoring equation trees. We use integer linear programming to generate equation trees and score their likelihood by learning local and global discriminative models. These models are trained on a small set of word problems and their answers, without any manual annotation, in order to choose the equation that best matches the problem text. We refer to the overall system as Alges. We compare Alges with previous work and show that it covers the full gamut of arithmetic operations whereas Hosseini et al. (2014) only handle addition and subtraction. In addition, Alges overcomes the brittleness of the Kushman et al. (2014) approach on single-equation problems, yielding a 15% to 50% reduction in error.",
        "date": "December 2015",
        "authors": [
            "Rik Koncel-Kedziorski",
            "Hannaneh Hajishirzi",
            "Ashish Sabharwal",
            "Oren Etzioni"
        ],
        "references": [
            301445864,
            290429478,
            232275981,
            221405119,
            221346130,
            220875019,
            200044364,
            329979260,
            301446025,
            301445947
        ]
    },
    {
        "id": 318494451,
        "title": "Computing Machinery and Intelligence",
        "abstract": "",
        "date": "January 1950",
        "authors": [
            "Alan M Turing"
        ],
        "references": []
    },
    {
        "id": 301446025,
        "title": "Solving Geometry Problems: Combining Text and Diagram Interpretation",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Minjoon Seo",
            "Hannaneh Hajishirzi",
            "Ali Farhadi",
            "Oren Etzioni"
        ],
        "references": [
            329974049,
            270878645,
            270878221,
            221605406,
            221405119,
            221346130,
            221303952,
            220875019,
            2850230,
            1946602
        ]
    },
    {
        "id": 301445855,
        "title": "Exploring Markov Logic Networks for Question Answering",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Tushar Khot",
            "Niranjan Balasubramanian",
            "Eric Gribkoff",
            "Ashish Sabharwal"
        ],
        "references": [
            269403748,
            255949552,
            221404108,
            220875209,
            220813165,
            51888036,
            2407807,
            2333743,
            289408852,
            279177379
        ]
    },
    {
        "id": 281666782,
        "title": "Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence",
        "abstract": "Ernest Davis and Gary Marcus share their views on the significance of commonsense reasoning and commonsense knowledge in artificial intelligence (AI). Many intelligent tasks, such as understanding texts, computer vision, planning, and scientific reasoning require the real-world knowledge and reasoning abilities. Techniques for implementing commonsense include logical analysis, handcrafting large knowledge bases, Web mining, and crowdsourcing. Intelligent machines need not replicate human cognition directly, but a better understanding of human commonsense is needed to perform such activities.",
        "date": "August 2015",
        "authors": [
            "Ernest Davis",
            "Gary Marcus"
        ],
        "references": [
            313242708,
            263699257,
            260289837,
            242382667,
            241623566,
            229912999,
            228750894,
            224673242,
            222310158,
            221606177
        ]
    },
    {
        "id": 313170906,
        "title": "Word association norms, mutual information and lexicography",
        "abstract": "",
        "date": "January 1991",
        "authors": [
            "Kenneth Church",
            "Patrick Hanks"
        ],
        "references": [
            237066988,
            234823075,
            313201001,
            288967781,
            261344688,
            247090257,
            245098881,
            242635142,
            242575724,
            242401706
        ]
    },
    {
        "id": 242554375,
        "title": "A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora",
        "abstract": "The availability of multi-word units (MWUs) in NLP lexica has important applications: enhances parsing precision, helps on attachment decision and enables more natural interaction of non-specialists users with information retrieval engines, among other applications. Most statistical approaches to MWUs extraction from corpora measure the association between two words, define thresholds for deciding which bigrams may be elected as possible units and use complex linguistic filters and language specific morpho-syntactic rules for filtering those units. In this paper we present: \u2022 A new algorithm (LocalMaxs) for extracting complex units made up of 2 or more adjacent words (n-grams, with n ! 2). \u2022 A new measure of \"glue\" or association between the words of any size n-gram. \u2022 An exhaustive comparison of our association measure with other known measures (Loglike, \" 2 , etc.). \u2022 A new normalization, fair dispersion point normalisation, for current statistical measures (Loglike, \" 2 , etc.) that enhances the precision and recall of the MWUs extracted by these measures.",
        "date": "January 1999",
        "authors": [
            "Joaquim Ferreira da Silva",
            "Gabriel Pereira Lopes",
            "Quinta da Torre",
            "Monte da Caparica"
        ],
        "references": [
            228717410,
            224737607,
            221299314,
            2477641,
            284400357,
            246785286,
            239665762,
            233967787,
            216814398
        ]
    },
    {
        "id": 238123710,
        "title": "Understanding Inverse Document Frequency: On Theoretical Arguments for IDF",
        "abstract": "The term-weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon's Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in the traditional probabilistic model of information retrieval.",
        "date": "October 2004",
        "authors": [
            "Stephen E. Robertson"
        ],
        "references": [
            247954937,
            321599148,
            314722919,
            301690162,
            301222420,
            290882608,
            247256783,
            242392196,
            238718217,
            238696293
        ]
    },
    {
        "id": 221299409,
        "title": "Using term informativeness for named entity detection",
        "abstract": "Informal communication (e-mail, bulletin boards) poses a difficult learning environment because traditional grammatical and lexical information are noisy. Other information is necessary for tasks such as named entity detection. How topic-centric, or informative, a word is can be valuable information. It is well known that informative words are best modeled by \"heavy-tailed\" distributions, such as mixture models. However, informativeness scores do not take full advantage of this fact. We introduce a new informativeness score that directly utilizes mixture model likelihood to identify informative words. We use the task of extracting restaurant names from bulletin board posts as a way to determine effectiveness. We find that our \"mixture score\" is weakly effective alone and highly effective when combined with Inverse Document Frequency. We compare against other informativeness criteria and find that only Residual IDF is competitive against our combined IDF/Mixture score.",
        "date": "August 2005",
        "authors": [
            "Jason D. M. Rennie",
            "Tommi Jaakkola"
        ],
        "references": [
            228057571,
            285278428,
            279838209,
            259952295,
            247931326,
            242591539,
            240325312,
            229777987,
            221995817,
            220698791
        ]
    },
    {
        "id": 221226686,
        "title": "TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams",
        "abstract": "In this paper, we propose a new term weighting scheme called Term Frequency - Inverse Corpus Frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods.",
        "date": "December 2006",
        "authors": [
            "Joel W. Reed",
            "yu Jiao",
            "Thomas E Potok",
            "Brian A. Klump"
        ],
        "references": [
            303206384,
            284528451,
            270352296,
            268646691,
            253071264,
            242708293,
            242637644,
            242499411,
            234789803,
            224105274
        ]
    },
    {
        "id": 221101542,
        "title": "Conundrums in Unsupervised Keyphrase Extraction: Making Sense of the State-of-the-Art.",
        "abstract": "State-of-the-art approaches for unsupervised keyphrase extraction are typically evaluated on a single dataset with a single parameter setting. Consequently, it is unclear how effective these approaches are on a new dataset from a different domain, and how sensitive they are to changes in parameter settings. To gain a better understanding of state-of-the-art unsupervised keyphrase extraction algorithms, we conduct a systematic evaluation and analysis of these algorithms on a variety of standard evaluation datasets.",
        "date": "January 2010",
        "authors": [
            "Kazi Hasan",
            "Vincent Ng"
        ],
        "references": [
            221013196,
            221012819,
            220816799,
            220489680,
            220479857,
            221157838,
            220874919,
            220705766,
            215470759,
            204241484
        ]
    },
    {
        "id": 221024134,
        "title": "Unsupervised query segmentation using only query log",
        "abstract": "We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries. We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model. The segments discovered by our scheme help understand this underlying grammatical structure. We apply a statistical model based on Hoeffding's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries. Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information (PMI) baseline.",
        "date": "January 2011",
        "authors": [
            "Nikita Mishra",
            "Rishiraj Saha Roy",
            "Niloy Ganguly",
            "Srivatsan Laxman"
        ],
        "references": [
            220047322,
            221300357,
            221023099,
            221022860,
            221012689
        ]
    },
    {
        "id": 220907261,
        "title": "Text Categorization with All Substring Features",
        "abstract": "",
        "date": "April 2009",
        "authors": [
            "Daisuke Okanohara",
            "Jun'ichi Tsujii"
        ],
        "references": []
    },
    {
        "id": 305342143,
        "title": "Dynamic Entity Representation with Max-pooling Improves Machine Reading",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Sosuke Kobayashi",
            "Ran Tian",
            "Naoaki Okazaki",
            "Kentaro Inui"
        ],
        "references": [
            283659163,
            266201822,
            265252627,
            257882504,
            286965813,
            281487270,
            281145060,
            278048272,
            261475541,
            255173850
        ]
    },
    {
        "id": 284476550,
        "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering",
        "abstract": "Question answering tasks have shown remarkable progress with distributed vector representations. In this paper, we look into the recently proposed Facebook 20 tasks (FB20). Finding the answers for questions in FB20 requires complex reasoning. Because the previous work on FB20 consists of end-to-end models, it is unclear whether errors come from imperfect understanding of semantics or in certain steps of the reasoning. To address this issue, we propose two vector space models inspired by tensor product representation (TPR) to perform analysis, knowledge representation, and reasoning based on common-sense inference. We achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches due to the special two-dimensional relationships identified from this study. The exploration reported in this paper and our subsequent work on generalizing the current model to the TPR formalism suggest the feasibility of developing further reasoning models in tensor space with learning capabilities.",
        "date": "November 2015",
        "authors": [
            "Moontae Lee",
            "Xiaodong He",
            "Wen-tau Yih",
            "Jianfeng Gao"
        ],
        "references": [
            301404391,
            290527719,
            285507661,
            284576917,
            281262284,
            279310212,
            319770369,
            319769995,
            282162224,
            272522139
        ]
    },
    {
        "id": 283659163,
        "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
        "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",
        "date": "November 2015",
        "authors": [
            "Felix Hill",
            "Antoine Bordes",
            "Sumit Chopra",
            "Jason Weston"
        ],
        "references": [
            279310212,
            278965988,
            291735245,
            286965813,
            281487270,
            281145060,
            278048272,
            277959429,
            277959017,
            273157787
        ]
    },
    {
        "id": 279310212,
        "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
        "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.",
        "date": "June 2015",
        "authors": [
            "Ankit Kumar",
            "Ozan Irsoy",
            "Jonathan Su",
            "James Bradbury"
        ],
        "references": [
            284576917,
            273067823,
            319770465,
            319770439,
            319770249,
            312607081,
            291295837,
            289758666,
            284753918,
            284039049
        ]
    },
    {
        "id": 220479903,
        "title": "Adapting boosting for information retrieval measures",
        "abstract": "We present a new ranking algorithm that combines the strengths of two previous methods: boosted tree classification, and LambdaRank, which has been shown to be empirically optimal for a widely used information retrieval measure. Our algorithm is based on boosted regression trees, although the ideas apply to any weak learners, and it is significantly faster in both train and test phases than the state of the art, for comparable accuracy. We also show how to find the optimal linear combination for any two rankers, and we use this method to solve the line search problem exactly during boosting. In addition, we show that starting with a previously trained model, and boosting using its residuals, furnishes an effective technique for model adaptation, and we give significantly improved results for a particularly pressing problem in web search\u2014training rankers for markets for which only small amounts of labeled data are available, given a ranker trained on much more data from a larger market.",
        "date": "June 2010",
        "authors": [
            "Qiang Wu",
            "Christopher J. C. Burges",
            "Krysta Marie Svore",
            "Jianfeng Gao"
        ],
        "references": [
            228587777,
            228395611,
            228364426,
            221618752,
            221618023,
            282676993,
            280687718,
            248499008,
            238384249,
            222403339
        ]
    },
    {
        "id": 306093209,
        "title": "Text Understanding with the Attention Sum Reader Network",
        "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.",
        "date": "March 2016",
        "authors": [
            "Rudolf Kadlec",
            "Martin Schmid",
            "Ond\u0159ej Bajgar",
            "Jan Kleindienst"
        ],
        "references": [
            305342143,
            283659163,
            277603270,
            265252627,
            262877889,
            233730646,
            220605292,
            319770257,
            306094228,
            301404729
        ]
    },
    {
        "id": 269117118,
        "title": "Deep Learning for Answer Sentence Selection",
        "abstract": "Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.",
        "date": "December 2014",
        "authors": [
            "Lei Yu",
            "Karl Moritz Hermann",
            "Phil Blunsom",
            "Stephen Pulman"
        ],
        "references": [
            270878435,
            266747839,
            266452571,
            289964736,
            284753918,
            270878336,
            270877716,
            263163914,
            262367926,
            261725707
        ]
    },
    {
        "id": 256708662,
        "title": "Automatic Feature Engineering for Answer Selection and Extraction",
        "abstract": "",
        "date": "September 2013",
        "authors": [
            "Aliaksei Severyn"
        ],
        "references": [
            266888154,
            255989135,
            255989134,
            255989042,
            253240062,
            221615271,
            221112301,
            262330903,
            241185808,
            239614775
        ]
    },
    {
        "id": 221012718,
        "title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA.",
        "abstract": "This paper presents a syntax-driven ap- proach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than us- ing syntactic features to augment exist- ing statistical classifiers (as in previous work), we build on the idea that ques- tions and their (correct) answers relate to each other via loose but predictable syntac- tic transformations. We propose a prob- abilistic quasi-synchronous grammar, in- spired by one proposed for machine trans- lation (D. Smith and Eisner, 2006), and pa- rameterized by mixtures of a robust non- lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines.",
        "date": "January 2007",
        "authors": [
            "Mengqiu Wang",
            "Noah A. Smith",
            "Teruko Mitamura"
        ],
        "references": [
            230875890,
            228441278,
            225129645,
            245258486,
            244116183,
            230876410,
            228623354,
            226174686,
            221619104,
            221300315
        ]
    },
    {
        "id": 2888359,
        "title": "The TREC-8 Question Answering Track Evaluation",
        "abstract": "The TREC-8 Question Answering track was the first large-scale evaluation of systems that return answers, as opposed to lists of documents, in response to a question. As a first evaluation, it is important to examine the evaluation methodology itself to understand any limits on the conclusions that can be drawn from the evaluation and possibly to find ways to improve subsequent evaluations. This paper has two main goals: to describe in detail how the evaluation was implemented, and to examine the consequences of the methodology on the comparative performance of the systems participating in the evaluation. The examination uncovered no serious flaws in the methodology, supporting its continued use for question answering evaluation. Nonetheless, redefining the specific task to be performed so that it more closely matches an actual user task does appear warranted.",
        "date": "November 2000",
        "authors": [
            "Ellen M. Voorhees",
            "Dawn M. Tice"
        ],
        "references": [
            263890803,
            221037808,
            256182235,
            243635728,
            238729554,
            222649957,
            2821114,
            2428268
        ]
    },
    {
        "id": 289964736,
        "title": "Automatic feature engineering for answer selection and extraction",
        "abstract": "This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.",
        "date": "January 2013",
        "authors": [
            "Aliaksei Severyn",
            "Alessandro Moschitti"
        ],
        "references": [
            220597335,
            220444383,
            262330903
        ]
    },
    {
        "id": 270877716,
        "title": "Question Answering Using Enhanced Lexical Semantic Models",
        "abstract": "In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.",
        "date": "August 2013",
        "authors": [
            "Wen-tau Yih",
            "Ming-Wei Chang",
            "Christopher Meek",
            "Andrzej Pastusiak"
        ],
        "references": [
            221299880,
            2819226,
            284580562,
            255563493,
            230876660,
            228623354,
            222688606,
            221299987,
            221213308,
            220515904
        ]
    },
    {
        "id": 262416109,
        "title": "Distributed Representations of Sentences and Documents",
        "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
        "date": "May 2014",
        "authors": [
            "Quoc V. Le",
            "Tomas Mikolov"
        ],
        "references": [
            266201822,
            257069350,
            228452494,
            221345755,
            221101423,
            220873867,
            220873681,
            45904528,
            319770369,
            319770263
        ]
    },
    {
        "id": 241185808,
        "title": "Answer Extraction as Sequence Tagging with Tree Edit Distance",
        "abstract": "",
        "date": "June 2013",
        "authors": [
            "Xuchen Yao",
            "Benjamin Van Durme",
            "Chris Callison-Burch",
            "Peter Clark"
        ],
        "references": []
    },
    {
        "id": 230876730,
        "title": "Solving logic puzzles: From robust processing to precise semantics.",
        "abstract": "This paper presents intial work on a system that bridges from robust, broad-coverage natural lan- guage processing to precise semantics and auto- mated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admis- sion Test (LSAT) and the analytic section of the Graduate Record Exam (GRE). We highlight key challenges, and discuss the representations and per- formance of the prototype system.",
        "date": "January 2004",
        "authors": [
            "Iddo Lev",
            "Bill MacCartney",
            "Christopher D. Manning",
            "Roger Levy"
        ],
        "references": [
            319394064,
            319393958,
            266359291,
            248051952,
            242402280,
            242327627,
            237131883,
            230876660,
            228802773,
            226426478
        ]
    },
    {
        "id": 221605406,
        "title": "Learning Language Semantics from Ambiguous Supervision.",
        "abstract": "This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of nat- ural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a se- mantic parser that maps sentences into meaning represen- tations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.",
        "date": "January 2007",
        "authors": [
            "Rohit Kate",
            "Raymond J. Mooney"
        ],
        "references": [
            221607022,
            220874055,
            313347109,
            265426020,
            242430394,
            230876744,
            228333647,
            221995817,
            49256909,
            37419815
        ]
    },
    {
        "id": 221405119,
        "title": "Reasoning about RoboCup Soccer Narratives.",
        "abstract": "",
        "date": "January 2011",
        "authors": [
            "Hannaneh Hajishirzi",
            "Julia Hockenmaier",
            "Erik T. Mueller",
            "Eyal Amir"
        ],
        "references": [
            252255095,
            319394074,
            313591181,
            303968845,
            284332223,
            280940018,
            272161384,
            268040816,
            261862140,
            248291502
        ]
    },
    {
        "id": 221346130,
        "title": "Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences",
        "abstract": "This paper studies the problem of learning from ambiguous supervision, focusing on the task of learning semantic correspondences. A learning problem is said to be ambiguously supervised when, for a given training input, a set of output candidates is provided with no prior of which one is correct. We propose to tackle this problem by solving a related unambiguous task with a label ranking approach and show how and why this performs well on the original task, via the method of task-transfer. We apply it to learning to match natural language sentences to a structured representation of their meaning and empirically demonstrate that this competes with the state-of-the-art on two benchmarks. 1.",
        "date": "August 2010",
        "authors": [
            "Antoine Bordes",
            "Nicolas Usunier",
            "Jason Weston"
        ],
        "references": [
            221653970,
            221605406,
            221304391,
            232643929,
            225423726,
            221618213,
            221461015,
            221364352,
            221345559,
            221344607
        ]
    },
    {
        "id": 221012726,
        "title": "A Multi-Pass Sieve for Coreference Resolution.",
        "abstract": "Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference mod- els one at a time from highest to lowest preci- sion. Each tier builds on the previous tier's entity cluster output. Further, our model prop- agates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guar- antees that stronger features are given prece- dence over weaker ones and that each deci- sion is made using all of the information avail- able at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our ap- proach outperforms many state-of-the-art su- pervised and unsupervised models on several standard corpora. This suggests that sieve- based approaches could be applied to other NLP tasks.",
        "date": "January 2010",
        "authors": [
            "Karthik Raghunathan",
            "Heeyoung Lee",
            "Sudarshan Rangarajan",
            "Nate Chambers"
        ],
        "references": [
            230875890,
            228575059,
            221012792,
            243774058,
            232561923,
            228526786,
            223000072,
            221013163,
            221012925,
            220873792
        ]
    },
    {
        "id": 220875019,
        "title": "Learning Semantic Correspondences with Less Supervision.",
        "abstract": "A central problem in grounded language acqui- sition is learning the correspondences between a rich world state and a stream of text which refer- ences that world state. To deal with the high de- gree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty\u2014Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.",
        "date": "January 2009",
        "authors": [
            "Percy Liang",
            "Michael Jordan",
            "Dan Klein"
        ],
        "references": [
            230875890,
            221606594,
            221605406,
            221102262,
            221012604,
            230876744,
            221345559,
            221013231,
            221012890,
            221012864
        ]
    },
    {
        "id": 215470704,
        "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
        "abstract": "Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",
        "date": "January 2005",
        "authors": [
            "Jenny Rose Finkel",
            "Trond Grenager",
            "Christopher D. Manning"
        ],
        "references": [
            234825167,
            227276601,
            299015627,
            260453705,
            245123229,
            239582331,
            228597280,
            225228322,
            221669680,
            220873486
        ]
    },
    {
        "id": 200044364,
        "title": "Generating Typed Dependency Parses from Phrase Structure Parses",
        "abstract": "This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.",
        "date": "January 2006",
        "authors": [
            "Marie-Catherine de Marneffe",
            "Bill MacCartney",
            "Christopher D. Manning"
        ],
        "references": [
            228907970,
            221605763,
            220778246,
            220482445,
            260065124,
            247655966,
            244444395,
            230875924,
            225879575,
            220816649
        ]
    },
    {
        "id": 2488725,
        "title": "Weka: Practical Machine Learning Tools and Techniques with Java Implementations",
        "abstract": "Introduction The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform. Tools are provided for pre-processing data, feeding it into a variety of learning schemes, and analyzing the resulting classifiers and their performance. An important resource for navigating through Weka is its on-line documentation, which is automatically generated from the source. The primary learning methods in Weka are classifiers, and they induce a rule set or decision tree that models the data. Weka also",
        "date": "April 2002",
        "authors": [
            "Ian Witten",
            "Eibe Frank",
            "Len Trigg",
            "Mark Hall"
        ],
        "references": [
            228776646,
            226658534,
            226270700,
            225891365,
            222897842,
            221345775,
            220815890,
            220688794,
            216300797,
            33051717
        ]
    },
    {
        "id": 272091377,
        "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
        "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straight-forward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.",
        "date": "January 2014",
        "authors": [
            "Christopher D. Manning",
            "Mihai Surdeanu",
            "John Bauer",
            "Jenny Finkel"
        ],
        "references": [
            266596637,
            237140687,
            220691633,
            220017613,
            320958047,
            284039049,
            265809872,
            262218213,
            249688577,
            220017553
        ]
    },
    {
        "id": 262310014,
        "title": "Learning Constraints for Consistent Timeline Extraction",
        "abstract": "We present a distantly supervised system for extracting the temporal bounds of fluents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn't been born yet. Our system achieves a 36% error reduction over a pipelined baseline.",
        "date": "July 2012",
        "authors": [
            "David McClosky",
            "Christopher D. Manning"
        ],
        "references": [
            267803439,
            267711345,
            266870503,
            266596637,
            251574306,
            221901076,
            283925025,
            267026639,
            230876788,
            224285663
        ]
    },
    {
        "id": 242582900,
        "title": "FrameNet II: Extended Theory and Practice",
        "abstract": "",
        "date": "January 2010",
        "authors": [
            "Josef Ruppenhofer",
            "Michael Ellsworth",
            "Miriam R L Petruck",
            "Christopher R. Johnson"
        ],
        "references": [
            290987024,
            221352933,
            324365629,
            319394304,
            230876726,
            230876705,
            230875998,
            221629042,
            221605172,
            2926910
        ]
    },
    {
        "id": 228663165,
        "title": "A semi-automatic method for annotating a biomedical proposition bank",
        "abstract": "In this paper, we present a semi-automatic approach for annotating se-mantic information in biomedical texts. The information is used to construct a biomedical proposition bank called BioProp. Like PropBank in the newswire domain, BioProp contains annotations of predicate argument structures and seman-tic roles in a treebank schema. To con-struct BioProp, a semantic role labeling (SRL) system trained on PropBank is used to annotate BioProp. Incorrect tag-ging results are then corrected by human annotators. To suit the needs in the bio-medical domain, we modify the Prop-Bank annotation guidelines and charac-terize semantic roles as components of biological events. The method can sub-stantially reduce annotation efforts, and we introduce a measure of an upper bound for the saving of annotation efforts. Thus far, the method has been applied experimentally to a 4,389-sentence tree-bank corpus for the construction of Bio-Prop. Inter-annotator agreement meas-ured by kappa statistic reaches .95 for combined decision of role identification and classification when all argument la-bels are considered. In addition, we show that, when trained on BioProp, our bio-medical SRL system called BIOSMILE achieves an F-score of 87%.",
        "date": "August 2006",
        "authors": [
            "Wen-Chi Chou",
            "Richard Tzong",
            "Richard Tzong-Han Tsai",
            "Ying-Shan Su"
        ],
        "references": [
            238626641,
            228817488,
            228707015,
            228629735,
            321620224,
            313392375,
            312927331,
            271431029,
            246963053,
            228715705
        ]
    },
    {
        "id": 224890460,
        "title": "Overview of BioNLP shared task 2011",
        "abstract": "",
        "date": "January 2011",
        "authors": [
            "Jin-Dong Kim",
            "Sampo Pyysalo",
            "Tomoko Ohta",
            "Robert Bossy"
        ],
        "references": [
            262314971,
            262289825,
            253293712,
            251422214,
            238626641,
            262395017,
            262292820,
            256483062,
            256482997,
            246668580
        ]
    },
    {
        "id": 224890459,
        "title": "Overview of BioNLP'09 shared task on event extraction",
        "abstract": "",
        "date": "June 2009",
        "authors": [
            "Jin-Dong Kim",
            "Tomoko Ohta",
            "Sampo Pyysalo",
            "Yoshinobu Kano"
        ],
        "references": [
            238626641,
            234784595,
            234779589,
            228530416,
            221037541,
            220875032,
            246668580,
            238700871,
            221038282,
            220875180
        ]
    },
    {
        "id": 221605724,
        "title": "Toward an Architecture for Never-Ending Language Learning.",
        "abstract": "We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent. Copyright \u00a9 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
        "date": "January 2010",
        "authors": [
            "Andrew C. Carlson",
            "Justin Betteridge",
            "Bryan Kisiel",
            "Burr Settles"
        ],
        "references": [
            234783121,
            221603776,
            283429954,
            233823600,
            228942691,
            228342287,
            222891794,
            222488504,
            221618951,
            221603625
        ]
    },
    {
        "id": 221013034,
        "title": "Fast and Robust Joint Models for Biomedical Event Extraction",
        "abstract": "Extracting biomedical events from literature has attracted much recent attention. The best-performing systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-the-art. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP 2011 Infectious Diseases task.",
        "date": "January 2011",
        "authors": [
            "Sebastian Riedel",
            "Andrew Mccallum"
        ],
        "references": [
            277749806,
            262322915,
            237145993,
            234813699,
            224890583,
            224890460,
            328281543,
            268685107,
            261860244,
            228715705
        ]
    },
    {
        "id": 220873694,
        "title": "Jointly Identifying Temporal Relations with Markov Logic.",
        "abstract": "Recent work on temporal relation iden- tification has focused on three types of relations between events: temporal rela- tions between an event and a time expres- sion, between a pair of events and between an event and the document creation time. These types of relations have mostly been identified in isolation by event pairwise comparison. However, this approach ne- glects logical constraints between tempo- ral relations of different types that we be- lieve to be helpful. We therefore propose a Markov Logic model that jointly identifies relations of all three relation types simul- taneously. By evaluating our model on the TempEval data we show that this approach leads to about 2% higher accuracy for all three types of relations \u2014and to the best results for the task when compared to those of other machine learning based systems.",
        "date": "January 2009",
        "authors": [
            "Katsumasa Yoshikawa",
            "Sebastian Riedel",
            "Masayuki Asahara",
            "Yuji Matsumoto"
        ],
        "references": [
            279529494,
            269031423,
            230845412,
            228559081,
            228392419,
            328281543,
            265793418,
            228954927,
            221475236,
            221012925
        ]
    },
    {
        "id": 313621629,
        "title": "Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence",
        "abstract": "We present a simple, easy-to-replicate monolingual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources. Based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts, we propose a system that operates by finding such pairs. In two intrinsic evaluations on alignment test data, our system achieves F 1 scores of 88\u201392%, demonstrating 1\u20133% absolute improvement over the previous best system. Moreover, in two extrinsic evaluations our aligner outperforms existing aligners, and even a naive application of the aligner approaches state-of-the-art performance in each extrinsic task.",
        "date": "December 2014",
        "authors": [
            "A.M. Sultan",
            "Steven Bethard",
            "Tamara Sumner"
        ],
        "references": [
            261437246,
            234075845,
            221012732,
            220874570,
            262156969,
            251501498,
            241185808,
            241185806,
            241185801,
            239917401
        ]
    },
    {
        "id": 274078591,
        "title": "A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations.",
        "abstract": "In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.",
        "date": "January 2013",
        "authors": [
            "Shashank Srivastava",
            "Dirk Hovy",
            "Eduard Hovy"
        ],
        "references": [
            274078594,
            266201822,
            255603927,
            234812363,
            228368890,
            221497506,
            221345917,
            221112301,
            221996125,
            221345858
        ]
    },
    {
        "id": 270878554,
        "title": "A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing",
        "abstract": "Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be applied in practice. In this work, we develop a much faster model whose time complexity is linear in the number of sentences. Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers. To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF. In addition to efficiency, our parser also significantly outperforms the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.",
        "date": "June 2014",
        "authors": [
            "Vanessa Wei Feng",
            "Graeme Hirst"
        ],
        "references": [
            270878498,
            262245231,
            261861695,
            252978256,
            234795766,
            229060015,
            270877635,
            262406121,
            230876289,
            228057950
        ]
    },
    {
        "id": 221654624,
        "title": "Regularized multi--task learning",
        "abstract": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.",
        "date": "August 2004",
        "authors": [
            "Theodoros Evgeniou",
            "Massimiliano Pontil"
        ],
        "references": [
            244415797,
            227442388,
            227442149,
            221653883,
            260516388,
            243767164,
            236736850,
            227631760,
            222727703,
            220694713
        ]
    },
    {
        "id": 221653970,
        "title": "Training linear SVMs in linear time",
        "abstract": "Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.",
        "date": "January 2006",
        "authors": [
            "Thorsten Joachims"
        ],
        "references": [
            245092557,
            234786663,
            307881957,
            285278428,
            262333164,
            247686251,
            243767009,
            230873058,
            226628001,
            221996831
        ]
    },
    {
        "id": 221346169,
        "title": "Learning structural SVMs with latent variables",
        "abstract": "We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of applica- tion problems, with an optimization problem that can be solved efficiently using Concave- Convex Programming. The generality and performance of the approach is demonstrated through three applications including motif- finding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.",
        "date": "June 2009",
        "authors": [
            "Chun-Nam John Yu",
            "Thorsten Joachims"
        ],
        "references": [
            228636738,
            221497408,
            313611092,
            313606782,
            248510200,
            243767009,
            221619007,
            221618763,
            221618389,
            221363206
        ]
    },
    {
        "id": 221345875,
        "title": "Training structural SVMs when exact inference is intractable",
        "abstract": "While discriminative training (e.g., CRF, structural SVM) holds much promise for ma- chine translation, image segmentation, and clustering, the complex inference these ap- plications require make exact training in- tractable. This leads to a need for ap- proximate training methods. Unfortunately, knowledge about how to perform ecient and eective approximate training is limited. Fo- cusing on structural SVMs, we provide and explore algorithms for two dierent classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and over- generating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained struc- tural SVMs, focusing on fully connected pair- wise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergener- ating methods, are empirically robust rela- tive to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors.",
        "date": "January 2008",
        "authors": [
            "Thomas William Finley",
            "Thorsten Joachims"
        ],
        "references": [
            307881957,
            228715647,
            228576278,
            228057950,
            225803849,
            222686598,
            222430151,
            221619768,
            221619279,
            221618559
        ]
    },
    {
        "id": 221037807,
        "title": "Factoid Question Answering over Unstructured and Structured Web Content.",
        "abstract": "We describe our experience with two new, built- from-scratch, web-based question answering sys- tems applied to the TREC 2005 Main Question Answering task, which use complementary models of answering questions over both structured and unstructured content on the Web. Our approaches depart from previous question answering (QA) work in several ways. For unstructured content, we used a web-based system with novel features such as web snippet pattern matching and generic an- swer type matching using web counts. We also ex- perimented with a new, complementary question answering approach that uses information from the millions of tables and lists that abound on the web. This system attempts to answer factoid ques- tions by guessing relevant rows and fields in matching web tables and integrating the results. We believe a combination of the two approaches holds promise.",
        "date": "January 2005",
        "authors": [
            "Silviu Cucerzan",
            "Eugene Agichtein"
        ],
        "references": [
            221614801,
            220875225,
            4084975,
            314795177,
            240053209,
            237124054,
            220515731,
            3908411,
            2946940,
            2907621
        ]
    },
    {
        "id": 267433170,
        "title": "Domain and Function: A Dual-Space Model of Semantic Relations and Compositions",
        "abstract": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.",
        "date": "September 2013",
        "authors": [
            "Peter David Turney"
        ],
        "references": [
            265890831,
            262216486,
            247316414,
            243773375,
            319394956,
            305083440,
            292733066,
            285680823,
            262329884,
            248956668
        ]
    },
    {
        "id": 234827624,
        "title": "One distributional memory, many semantic spaces",
        "abstract": "We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks.",
        "date": "March 2009",
        "authors": [
            "Marco Baroni",
            "Alessandro Lenci"
        ],
        "references": [
            262161871,
            227090938,
            221013297,
            221012883,
            329650509,
            279395438,
            277298430,
            241144345,
            238727910,
            230875966
        ]
    },
    {
        "id": 221112660,
        "title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL",
        "abstract": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).",
        "date": "September 2001",
        "authors": [
            "Peter David Turney"
        ],
        "references": [
            313170906,
            243764166,
            319393820,
            313618594,
            303802749,
            269200023,
            256856526,
            246472275,
            245098892,
            242608481
        ]
    },
    {
        "id": 220873681,
        "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning.",
        "abstract": "If we take an existing supervised NLP sys- tem, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accu- racy of these baselines. We find further improvements by combining di erent word representations. You can download our word features, for o -the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/",
        "date": "January 2010",
        "authors": [
            "Joseph P. Turian",
            "Lev-Arie Ratinov",
            "Y. Bengio"
        ],
        "references": [
            301691717,
            255820377,
            239584593,
            230876750,
            230876271,
            230786023,
            228741173,
            228719369,
            305387533,
            233530330
        ]
    },
    {
        "id": 266439415,
        "title": "Multi-View Learning of Word Embeddings via CCA",
        "abstract": "Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model the context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate low dimensional context-specific word representations from unlabeled data. These representation features can then be used with any supervised learner. LR-MVL is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (NER) and chunking problems.",
        "date": "January 2011",
        "authors": [
            "Paramveer Dhillon",
            "Dean Foster",
            "Lyle H. Ungar"
        ],
        "references": [
            221345134,
            220873681,
            220355244,
            287702488,
            233530330,
            221345848,
            221345530,
            220874824,
            220873440,
            220873104
        ]
    },
    {
        "id": 230786023,
        "title": "Word Category Maps based on Emergent Features Created by ICA",
        "abstract": "In this paper, we assume that word co-occurrence statistics can be used to extract meaningful features, exhibiting syntactic and se- mantic behavior, from text data. Independent component analysis (ICA), an unsupervised statistical method, is applied to word usage statistics, calculated from a natural language corpora, to extract a number of fea- tures. With a self-organizing map (SOM), we will demonstrate that the extracted vector representation for words can further be applied to other tasks. It is also demonstrated, that the ICA-based encoding scheme is a good alternative to random projection (RP), a method commonly used in text analysis.",
        "date": "January 2004",
        "authors": [
            "Jaakko V\u00e4yrynen",
            "Timo Honkela"
        ],
        "references": [
            227090938,
            220597261,
            220017760,
            265919274,
            247482326,
            238679016,
            237067978,
            226434431,
            222489418,
            200773081
        ]
    },
    {
        "id": 221345058,
        "title": "Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space.",
        "abstract": "Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts. Concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient...",
        "date": "January 2000",
        "authors": [
            "Alberto Paccanaro",
            "Geoffrey E. Hinton"
        ],
        "references": []
    },
    {
        "id": 45904528,
        "title": "From Frequency to Meaning: Vector Space Models of Semantics",
        "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.",
        "date": "March 2010",
        "authors": [
            "Peter David Turney",
            "Patrick Pantel"
        ],
        "references": [
            317580935,
            313768163,
            324397609,
            314818211,
            312985702,
            312775005,
            308953917,
            308161218,
            305397619,
            305083440
        ]
    },
    {
        "id": 3296950,
        "title": "Learning distributed representations of concepts using Linear Relational Embedding",
        "abstract": "We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization",
        "date": "April 2001",
        "authors": [
            "Alberto Paccanaro",
            "G.E. Hinton"
        ],
        "references": [
            270960245,
            243698906,
            316753002,
            303802749,
            287334435,
            270364306,
            233130587,
            230876435,
            230875884,
            228604339
        ]
    },
    {
        "id": 222099363,
        "title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics",
        "abstract": "We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.",
        "date": "February 2012",
        "authors": [
            "Michael Gutmann",
            "Aapo Hyv\u00e4rinen"
        ],
        "references": [
            227121666,
            304109482,
            296924802,
            281053130,
            271512996,
            266036828,
            266005429,
            247131283,
            242358078,
            237067978
        ]
    },
    {
        "id": 5462889,
        "title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model",
        "abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.",
        "date": "May 2008",
        "authors": [
            "Y. Bengio",
            "Sofian Audry"
        ],
        "references": [
            244436420,
            312922200,
            312536675,
            309630116,
            296924802,
            281309188,
            268209561,
            265681840,
            243770214,
            243766097
        ]
    },
    {
        "id": 281458937,
        "title": "Multimodal Distributional Semantics",
        "abstract": "Distributional semantic models derive computational representations of word meaning from the patterns of co-occurrence of words in text. Such models have been a success story of computational linguistics, being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them. However, distributional models extract meaning information exclusively from text, which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge. We address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete \"visual words\" in images, so that the distributional representation of a word can be extended to also encompass its co-occurrence with the visual words of images it is associated with. We propose a flexible architecture to integrate text- and image-based distributional information, and we show in a set of empirical tests that our integrated model is superior to the purely text-based approach, and it provides somewhat complementary semantic information with respect to the latter.",
        "date": "November 2014",
        "authors": [
            "Elia Bruni",
            "Nam Khanh Tran",
            "Marco Baroni"
        ],
        "references": [
            262356435,
            262250358,
            235737186,
            319394707,
            310439564,
            264474244,
            250310896,
            247162040,
            241097083,
            239060666
        ]
    },
    {
        "id": 254464318,
        "title": "Large-scale learning of word relatedness with constraints",
        "abstract": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
        "date": "August 2012",
        "authors": [
            "Guy Halawi",
            "Gideon Dror",
            "Evgeniy Gabrilovich",
            "Yehuda Koren"
        ],
        "references": [
            303802749,
            266712070,
            247866275,
            242919995,
            237128020,
            236736791,
            234819712,
            234803640,
            230854795,
            221621957
        ]
    },
    {
        "id": 234826128,
        "title": "Unsupervised classification with dependency based word spaces",
        "abstract": "We present the results of clustering experiments with a number of different evaluation sets using dependency based word spaces. Contrary to previous results we found a clear advantage using a parsed corpus over word spaces constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13% in absolute terms of cluster purity.",
        "date": "March 2009",
        "authors": [
            "Klaus Rothenh\u00e4usler",
            "Hinrich Sch\u00fctze"
        ],
        "references": [
            266452571,
            234781408,
            228783371,
            228632901,
            228548010,
            221012932,
            220355312,
            313618594,
            277298430,
            234784820
        ]
    },
    {
        "id": 234131208,
        "title": "Zero-Shot Learning Through Cross-Modal Transfer",
        "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.",
        "date": "January 2013",
        "authors": [
            "Richard Socher",
            "Milind Ganjoo",
            "Hamsa Bastani",
            "Osbert Bastani"
        ],
        "references": [
            267265679,
            244436740,
            310439564,
            305386599,
            285764506,
            267554204,
            262201777,
            240308783,
            234118152,
            232650517
        ]
    },
    {
        "id": 265022827,
        "title": "Visualizing Higher-Layer Features of a Deep Network",
        "abstract": "Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualita-tive interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Auto-encoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architec-tures to understand more of how and why deep architectures work.",
        "date": "January 2009",
        "authors": [
            "Dumitru Erhan",
            "Y. Bengio",
            "Aaron Courville",
            "Pascal Vincent"
        ],
        "references": [
            303137904,
            236736855,
            224579211,
            221620343,
            221346269,
            311469666,
            304109482,
            248236057,
            245993331,
            221620295
        ]
    },
    {
        "id": 262238209,
        "title": "Duluth: measuring degrees of relational similarity with the gloss vector measure of semantic relatedness",
        "abstract": "This paper describes the Duluth systems that participated in Task 2 of SemEval-2012. These systems were unsupervised and relied on variations of the Gloss Vector measure found in the freely available software package WordNet:: Similarity. This method was moderately successful for the Class-Inclusion, Similar, Contrast, and Non-Attribute categories of semantic relations, but mimicked a random baseline for the other six categories.",
        "date": "June 2012",
        "authors": [
            "Ted Pedersen"
        ],
        "references": [
            233779111,
            228520439,
            200772975,
            2903260,
            2563220,
            303802749,
            232445877,
            230854746,
            228057706,
            220355277
        ]
    },
    {
        "id": 308797129,
        "title": "Distributional structure",
        "abstract": "",
        "date": "January 1954",
        "authors": [
            "ZS Harris"
        ],
        "references": []
    },
    {
        "id": 262566601,
        "title": "Data Mining: Practical Machine Learning Tools and Techniques (Third Edition)",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "Ian Witten",
            "Eibe Frank"
        ],
        "references": []
    },
    {
        "id": 262334182,
        "title": "BUAP: a first approximation to relational similarity measuring",
        "abstract": "We describe a system proposed for measuring the degree of relational similarity beetwen a pair of words at the Task #2 of Semeval 2012. The approach presented is based on a vectorial representation using the following features: i) the context surrounding the words with a windows size = 3, ii) knowledge extracted from WordNet to discover several semantic relationships, such as meronymy, hyponymy, hypernymy, and part-whole between pair of words, iii) the description of the pairs with their POS tag, morphological information (gender, person), and iv) the average number of words separating the two words in text.",
        "date": "June 2012",
        "authors": [
            "Tovar Vidal Mireya",
            "Jos\u00e9 A. Reyes",
            "Azucena Montes Rend\u00f3n",
            "Darnes Vilari\u00f1o"
        ],
        "references": [
            228520439,
            44233809,
            262367188,
            262285896,
            235323874,
            221995866
        ]
    },
    {
        "id": 262284913,
        "title": "UTD: determining relational similarity using lexical patterns",
        "abstract": "In this paper we present our approach for assigning degrees of relational similarity to pairs of words in the SemEval-2012 Task 2. To measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents. Patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation. Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%.",
        "date": "June 2012",
        "authors": [
            "Bryan Rink",
            "Sanda Harabagiu"
        ],
        "references": [
            228520439,
            221653970,
            220148033,
            23683709,
            1958402,
            1772519,
            232445877,
            220874560,
            220141968,
            3296591
        ]
    },
    {
        "id": 298666972,
        "title": "Statistical language modeling for speech recognition",
        "abstract": "In this article we discuss the theoretical basis for language modeling and their practical application to the problem of continues speech recognition. In the 1. section we discusse the process of speech recognition in the mathematical sense using well-known Bayes formula (see Formula 2 and Figure 1). We outline the importance of language model. The 2. section explains the basic principles of building a language model using elementary rules of probability theory. The description of trigram language model is given (see Formulas 3, 4 and 5). We face the problem of estimating probabilities from sparse data and present the solution called nonlinear back off procedure, given by Katz[4] (see Formulas 6, 7, 8, 10, 11, 12 and 13). In the 3. section we consider objective measure of language model quality. The measure is called perplexity (see Formulas 15 and 16). It allows us to judge a model separated from other components of recognizer. A meaningfull comparison can he made between perplexities of several models, all with respect to the same text and the same vocabulary. We describe a set of software tools designed to facilitate language modeling work. The tools are designed as modular building blocks, which enable us simple step-by-step manipulation of data streams. The model parameters are extracted from large amount of text. Text is a stream of words, interspersed with context cues: beginning-of-article marker, beginning-of-paragraph marker and begining-of-sentence marker. These markers are not a part of the application vocabulary. They are treated in a special way. The text is devided in two parts: training and testing corpus. The probability estimates are computed from training corpus and the judgement of the model is done using testing corpus (see Figure 2). Language models are defined relative to a particular vocabulary. We extracted the vocabulary from training corpus. First we build the word frequency list and the the top V words define the vocabulary. All words that are outside the given vocabulary are mapped into a single symbol called Out-Of-Vocabulary symbol. The training corpus is used for trigram frequency computation. Trigrams are then reduced with respect to the vocabulary (see Table 4). We report some frequency-of-frequency statistics for trigrams and bigrams (see Table 2). We build two types of language models. First half of the models were build with respect to all trigram and bigram counts and the second half of the models were build with respect to bigrams and trigrams with counts greater than I. Language models were designed with vocabularies of different size. Comparison between several models has been done (see Table 4 and Figures 5. 6 and 7). At the end we give some hues for future work.",
        "date": "January 1997",
        "authors": [
            "Mirjam Sepesy Mau\u010dec"
        ],
        "references": []
    },
    {
        "id": 284685610,
        "title": "On learning the past tenses of English verbs. Parallel distributed processing: explorations in the microstructure of cognition",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "D.E. Rumelhart",
            "James L Mcclelland"
        ],
        "references": []
    },
    {
        "id": 262201777,
        "title": "Improving word representations via global context and multiple word prototypes",
        "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.",
        "date": "July 2012",
        "authors": [
            "Eric H. Huang",
            "Richard Socher",
            "Christopher D. Manning",
            "Andrew Y. Ng"
        ],
        "references": [
            290992427,
            266439415,
            233756933,
            228452494,
            228441022,
            221618573,
            221300017,
            221023177,
            221013297,
            221012968
        ]
    },
    {
        "id": 255564430,
        "title": "The Microsoft Research Sentence Completion Challenge",
        "abstract": "Work on modeling semantics in text is progressing quickly, yet there are few existing public datasets which authors can use to measure and compare their systems. This work takes a step towards addressing this issue. We present the MSR Sentence Completion Challenge Data, which consists of 1,040 sentences, each of which has four impostor sentences, in which a single (fixed) word in the original sentence has been replaced by an impostor word with similar occurrence statistics. For each sentence the task is then to determine which of the five choices for that word is the correct one. This data was constructed from Project Gutenberg data. Seed sentences were selected from Sherlock Holmes novels, and then imposter words were suggested with the aid of a language model trained on over 500 19th century novels. The language model was used to compute 30 alternative words for a given low frequency word in a sentence, and human judges then picked the 4 best impostor words, based on a set of provided guidelines. Although the data presented here will not be changed, this is still a work in progress, and we plan to add similar datasets based on other sources. This technical report is a living document and will be updated appropriately as new datasets are constructed and new results on existing datasets (for example, using human subjects) are reported.",
        "date": "January 2011",
        "authors": [
            "Geoffrey Zweig",
            "Christopher J. C. Burges"
        ],
        "references": [
            319394064,
            221023177,
            220515594,
            2413241,
            303802749,
            220816735
        ]
    },
    {
        "id": 234828967,
        "title": "UTD: Classifying semantic relations by combining lexical and semantic resources",
        "abstract": "This paper describes our system for SemEval-2010 Task 8 on multi-way classification of semantic relations between nominals. First, the type of semantic relation is classified. Then a relation type-specific classifier determines the relation direction. Classification is performed using SVM classifiers and a number of features that capture the context, semantic role affiliation, and possible pre-existing relations of the nominals. This approach achieved an F1 score of 82.19% and an accuracy of 77.92%.",
        "date": "January 2010",
        "authors": [
            "Bryan Rink",
            "Sanda Harabagiu"
        ],
        "references": [
            234801319,
            222964023,
            220873390,
            220816876,
            37689455,
            2481407
        ]
    },
    {
        "id": 228881189,
        "title": "Integrating Logical Representations with Probabilistic Information using Markov Logic",
        "abstract": "First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning. This paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distribu-tional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic informa-tion on word meaning in context.",
        "date": "May 2012",
        "authors": [
            "Dan Garrette",
            "Katrin Erk",
            "Raymond Mooney"
        ],
        "references": [
            234802265,
            228770226,
            227090938,
            221366753,
            221013297,
            247752095,
            242367562,
            241335550,
            227375686,
            222450819
        ]
    },
    {
        "id": 221013013,
        "title": "Broad Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger",
        "abstract": "In this paper we approach word sense disambiguation and information extrac- tion as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet super- sense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes - person, location, organization, time, etc. - the tagger, as a by-product, returns extended named en- tity information. We cast the problem of supersense tagging as a sequential label- ing task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows consid- erable improvements over the best known \"first-sense\" baseline.",
        "date": "January 2006",
        "authors": [
            "Massimiliano Ciaramita",
            "Yasemin Altun"
        ],
        "references": [
            262348364,
            238671951,
            234789404,
            230876766,
            224890478,
            247487468,
            243787227,
            238735047,
            230854795,
            220874587
        ]
    },
    {
        "id": 220874934,
        "title": "Compositional Matrix-Space Models of Language.",
        "abstract": "We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.",
        "date": "January 2010",
        "authors": [
            "Sebastian Rudolph",
            "Eugenie Giesbrecht"
        ],
        "references": [
            266452571,
            234808686,
            323707969,
            246146039,
            245098881,
            242499684,
            240154491,
            235734026,
            234817041,
            230876238
        ]
    },
    {
        "id": 220817516,
        "title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables.",
        "abstract": "In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.",
        "date": "January 2010",
        "authors": [
            "Tetsuji Nakagawa",
            "Kentaro Inui",
            "Sadao Kurohashi"
        ],
        "references": [
            271451830,
            258293896,
            234814741,
            228643786,
            226779968,
            339502749,
            228529829,
            226174686,
            224890479,
            224362722
        ]
    },
    {
        "id": 275807826,
        "title": "Principles of Neurodynamics.",
        "abstract": "",
        "date": "May 1963",
        "authors": [
            "A. A. Mullin",
            "Frank Rosenblatt"
        ],
        "references": []
    },
    {
        "id": 233784963,
        "title": "Perceptrons",
        "abstract": "Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades. It marked a historical turn in artificial intelligence, and it is required reading for anyone who wants to understand the connectionist counterrevolution that is going on today. Artificial-intelligence research, which for a time concentrated on the programming of ton Neumann computers, is swinging back to the idea that intelligence might emerge from the activity of networks of neuronlike entities. Minsky and Papert's book was the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain. Now the new developments in mathematical tools, the recent interest of physicists in the theory of disordered matter, the new insights into and psychological models of how the brain works, and the evolution of fast computers that can simulate networks of automata have given Perceptrons new importance. Witnessing the swing of the intellectual pendulum, Minsky and Papert have added a new chapter in which they discuss the current state of parallel computers, review developments since the appearance of the 1972 edition, and identify new research directions related to connectionism. They note a central theoretical challenge facing connectionism: the challenge to reach a deeper understanding of how \"objects\" or \"agents\" with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called \"society theories of mind.\" Marvin L. Minsky is Donner Professor of Science in MIT's Electrical Engineering and Computer Science Department. Seymour A. Papert is Professor of Media Technology at MIT.",
        "date": "December 1969",
        "authors": [
            "Marvin L Minsky",
            "Seymour A Papert"
        ],
        "references": []
    },
    {
        "id": 221707483,
        "title": "A Family of Computationally Efficient and Simple Estimators for Unnormalized Statistical Models",
        "abstract": "We introduce a new family of estimators for unnormalized statistical models. Our family of estimators is parameterized by two nonlinear functions and uses a single sample from an auxiliary distribution, generalizing Maximum Likelihood Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we can estimate the partition function like any other parameter in the model. The estimation is done by optimizing an algebraically simple, well defined objective function, which allows for the use of dedicated optimization methods. We establish consistency of the estimator family and give an expression for the asymptotic covariance matrix, which enables us to further analyze the influence of the nonlinearities and the auxiliary density on estimation performance. Some estimators in our family are particularly stable for a wide range of auxiliary densities. Interestingly, a specific choice of the nonlinearity establishes a connection between density estimation and classification by nonlinear logistic regression. Finally, the optimal amount of auxiliary samples relative to the given amount of the data is considered from the perspective of computational efficiency.",
        "date": "March 2012",
        "authors": [
            "Miika Pihlaja",
            "Michael Gutmann",
            "Aapo Hyvarinen"
        ],
        "references": [
            221460970,
            220320709,
            216636373,
            242706958,
            237067978,
            221932989,
            220320841,
            200744578,
            49269499,
            44693606
        ]
    },
    {
        "id": 220691633,
        "title": "Natural Language Processing with Python",
        "abstract": "",
        "date": "January 2009",
        "authors": [
            "Steven Bird",
            "Ewan Klein",
            "Edward Loper"
        ],
        "references": [
            255672981,
            221574042,
            220486407,
            220398500,
            220355311,
            200772975,
            200179363,
            29737222,
            222551458,
            222524327
        ]
    },
    {
        "id": 221532132,
        "title": "Extracting Distributed Representations of Concepts and Relations from Positive and Negative Propositions.",
        "abstract": "Linear Relational Embedding (LRE) was introduced (Paccanaro and Hinton, 1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative infor- mation and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization.",
        "date": "January 2000",
        "authors": [
            "Alberto Paccanaro",
            "Geoffrey E. Hinton"
        ],
        "references": [
            2350768,
            303802749,
            270364306,
            246666948,
            230876435,
            200092187,
            200045221,
            24061680
        ]
    },
    {
        "id": 221492270,
        "title": "Neural network language models for conversational speech recognition",
        "abstract": "Recently there is growing interest in using neural networks for language modeling. In contrast to the well known backoff n- gram language models (LM), the neural network approach tries to limit problems from the data sparseness by performing the es- timation in a continuous space, allowing by these means smooth interpolations. Therefore this type of LM is interesting for tasks for which only a very limited amount of in-domain training data is available, such as the modeling of conversational speech. In this paper we analyze the generalization behavior of the neural network LM for in-domain training corpora vary- ing from 7M to over 21M words. In all cases, significant word error reductions were observed compared to a carefully tuned 4-gram backoff language model in a state of the art conversa- tional speech recognizer for the NIST rich transcription evalu- ations. We also apply ensemble learning methods and discuss their connections with LM interpolation.",
        "date": "October 2004",
        "authors": [
            "Holger Schwenk",
            "Jean-Luc Gauvain"
        ],
        "references": [
            221618573,
            4117044,
            2522500,
            2413241,
            230876151,
            4015073,
            2906929,
            2806398,
            2602251
        ]
    },
    {
        "id": 221491569,
        "title": "Self-organizing letter code-book for text-to-phoneme neural network model.",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "K\u00e5re Jean Jensen",
            "S\u00f8ren Kamaric Riis"
        ],
        "references": [
            237111620,
            220485679,
            14505537,
            274202844,
            247105445,
            242620672,
            239665722,
            222336468,
            215721451,
            200033861
        ]
    },
    {
        "id": 221489509,
        "title": "Building continuous space language models for transcribing european languages",
        "abstract": "Large vocabulary continuous speech recognizers for En- glish Broadcast News achieve today word error rates below 10%. An important factor for this succes is the availability of large amounts of acoustic and language modeling training data. In this paper the recognition of French Broadcast News and English and Spanish parliament speeches is addressed, tasks for which less resources are available. A neural network lan- guage model is applied that takes better advantage of the lim- ited amount of training data. This approach performs the esti- mation of the probabilities in a continuous space, allowing by this means smooth interpolations. Word error reduction of up to 0.9% absolute are reported with respect to a carefully tuned backoff language model trained on the same data.",
        "date": "September 2005",
        "authors": [
            "Holger Schwenk",
            "Jean-Luc Gauvain"
        ],
        "references": [
            250758918,
            221618573,
            221492270,
            221489057,
            4117044,
            2522500,
            2413241,
            262180799,
            2602251
        ]
    },
    {
        "id": 221489178,
        "title": "Can artificial neural networks learn language models?",
        "abstract": "Currently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods.",
        "date": "January 2000",
        "authors": [
            "Wei Xu",
            "Alexander I. Rudnicky"
        ],
        "references": [
            221481881,
            3652298,
            313753822,
            307174896,
            230876149,
            230875884,
            221995817,
            220875255,
            215721461,
            3618313
        ]
    },
    {
        "id": 228980010,
        "title": "Categorizing nine visual classes using local appearance descriptors",
        "abstract": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na\u00efve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for classifying nine semantic visual categories and comment on results obtained by Fergus et al using a different method on the same data set. We obtain excellent results as well for multi class categorization as for object detection. A thorough evaluation clearly demonstrates that our method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.",
        "date": "January 2004",
        "authors": [
            "Jutta Willamowski",
            "Damian Arregui",
            "Gabriela Csurka",
            "Christopher R Dance"
        ],
        "references": [
            221303855,
            3906193,
            285278428,
            281496262,
            249728327,
            240224936,
            234827916,
            221259229,
            220660356,
            220515519
        ]
    },
    {
        "id": 313522833,
        "title": "PCA-SIFT: A more distinctive representation for local image descriptors",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "Y. Ke",
            "R. Sukthankar"
        ],
        "references": []
    },
    {
        "id": 248488976,
        "title": "YEF Real-Time Object Detection",
        "abstract": "We present an improvement to the object detection scheme of Viola and Jones (5), using the example of face detec- tion. We present a new kind of visual features, which is based on sampling individual pixels within the examined sub-window, instead of comparing the sums of pixel val- ues in rectangular regions. Our features are faster and do not demand the preparation of an integral image, nor variance-normalization of each sub-window. The result is an improved system which detects faces in quarter-PAL im- ages in only 9 millisecond per image on a 2.4GHz pentium computer.",
        "date": "January 2005",
        "authors": [
            "Yotam Abramson",
            "Bruno Steux"
        ],
        "references": []
    },
    {
        "id": 242100776,
        "title": "Cr\u00e9ation de Vocabulaires Visuels Efficaces pour la Cat\u00e9gorisation d\u2019Images",
        "abstract": "Abstract We propose in this article an automatic method,for build- ing visual codebooks. Codebooks,are obtained by quantiz- ing local image descriptors and are used to automatically build discriminative representations of objects occuringin images. We describe an image categorization application based on the proposed approaches, providing results far above re- lated state of the art existing methods. Keywords image description, image categorization, machine learnin g",
        "date": "September 2015",
        "authors": [
            "Diane Larlus",
            "Gyuri Dork\u00f3",
            "Frederic Jurie"
        ],
        "references": []
    },
    {
        "id": 228715647,
        "title": "LIBSVM: A library for support vector machines",
        "abstract": "LIBSVM is a library for support vector machines (SVM). Its goal is to help users to easily use SVM as a tool. In this document, we present all its imple-mentation details. For the use of LIBSVM, the README file included in the package and the LIBSVM FAQ provide the information.",
        "date": "July 2007",
        "authors": [
            "Chih-Chung Chang",
            "Chih-Jen Lin"
        ],
        "references": [
            243776958,
            234786663,
            228573389,
            228381676,
            220597254,
            220499623,
            220320039,
            220319835,
            41781517,
            2880986
        ]
    },
    {
        "id": 285469376,
        "title": "Nominal, perceptual, and semantic codes in picture categorization",
        "abstract": "",
        "date": "January 1978",
        "authors": [
            "Smith E.E.",
            "Balzano G.J.",
            "J. Walker"
        ],
        "references": []
    },
    {
        "id": 228834755,
        "title": "Single-View Based Recognition of Faces Rotated in Depth",
        "abstract": "We present a method for recognizing objects (faces) on the basis of just one stored view, in spite of rotation in depth. The method is not based on the construction of a three-dimensional model for the object. Our recognition results represent a significant improvement over a previous system developed in our laboratory. We achieve this with the help of a simple assumption about the transformation of local feature vectors with rotation in depth. 1 Introduction It is known from biology that objects are seen under two aspects [UM]: The `Where' and the `What' aspect. `Where' means localization in space and perspective, `What' means classification and identification. In technical systems the last task often turns out to be the more difficult one which needs the highest accuracy (e.g. it is less difficult to find a face in an image than to recognize it). Thus, visual features are needed which are invariant under perspective changes or can be transformed. Invariant features seem attractive,...",
        "date": "June 1995",
        "authors": [
            "Thomas Maurer",
            "Christoph von der Malsburg"
        ],
        "references": [
            243782020,
            221114027
        ]
    },
    {
        "id": 220182212,
        "title": "An Experimental Comparison of Range Image Segmentation Algorithms",
        "abstract": "A methodology for evaluating range image segmentation algorithms is proposed. This methodology involves (a) a common set of 40 laser range finder images and 40 structured light scanner images that have manually specified ground truth and (b) a set of defined performance metrics for instances of correctly segmented, missed and noise regions, over- and under-segmentation, and accuracy of the recovered geometry. A tool is used to objectively compare a machine generated segmentation against the specified ground truth. Four research groups have contributed to evaluate their own algorithm for segmenting a range image into planar patches. Key words: experimental comparison of algorithms, range image segmentation, low level processing, performance evaluation In general, standardized segmentation error metrics are needed to help advance the stateof -the-art. No quantitative metrics are measured on standard test images in most of today's research environments. ---NSF Range Image Unde...",
        "date": "July 1996",
        "authors": [
            "Adam Hoover",
            "Gillian Jean-Baptiste",
            "Xiaoyi Jiang",
            "Patrick J. Flynn"
        ],
        "references": [
            262159193,
            316693754,
            281397742,
            263798565,
            257468570,
            248848344,
            243770483,
            243667454,
            239666204,
            234364339
        ]
    },
    {
        "id": 28763103,
        "title": "Face Recognition and Gender Determination",
        "abstract": "The system presented here is a specialized version of a general object recognition system. Images of faces are represented as graphs, labeled with topographical information and local templates. Different poses are represented by different graphs. New graphs of faces are generated by an elastic graph matching procedure comparing the new face with a set of precomputed graphs: the \"general face knowledge\". The final phase of the matching process can be used to generate composite images of faces and to determine certain features represented in the general face knowledge, such as gender or the presence of glasses or a beard. The graphs can be compared by a similarity function which makes the system efficient in recognizing faces.",
        "date": "June 1995",
        "authors": [
            "Laurenz Wiskott",
            "Jean-marc Fellous",
            "Norbert Kr\u00fcger",
            "Christoph von der Malsburg"
        ],
        "references": [
            228834755,
            224755110,
            224740138,
            220450968,
            220359285,
            256075302,
            240174908,
            227634536,
            222266367,
            19719670
        ]
    },
    {
        "id": 3684468,
        "title": "UNIPEN project of on-line data exchange and recognizer benchmarks",
        "abstract": "We report the status of the UNIPEN project of data exchange and recognizer benchmarks started two years ago at the initiative of the International Association of Pattern Recognition (Technical Committee 11). The purpose of the project is to propose and implement solutions to the growing need of handwriting samples for online handwriting recognizers used by pen-based computers. Researchers from several companies and universities have agreed on a data format, a platform of data exchange and a protocol for recognizer benchmarks. The online handwriting data of concern may include handprint and cursive from various alphabets (including Latin and Chinese), signatures and pen gestures. These data will be compiled and distributed by the Linguistic Data Consortium. The benchmarks will be arbitrated the US National Institute of Standards and Technologies. We give a brief introduction to the UNIPEN format. We explain the protocol of data exchange and benchmarks",
        "date": "November 1994",
        "authors": [
            "Isabelle Guyon",
            "Lambert Schomaker",
            "R\u00e9jean Plamondon",
            "Mark Liberman"
        ],
        "references": [
            208033632,
            247880840,
            243785298,
            243615377,
            242572658,
            224657044,
            3602506,
            3532137,
            2433793
        ]
    },
    {
        "id": 3637747,
        "title": "Feature-Based Face Recognition Using Mixture-Distance",
        "abstract": "We consider the problem of feature-based face recognition in the setting where only a single example of each face is available for training. The mixture-distance technique we introduce achieves a recognition rate of 95% on a database of 685 people in which each face is represented by 30 measured distances. This is currently the best recorded recognition rate for a feature-based system applied to a database of this size. By comparison, nearest neighbor search using Euclidean distance yields 84%. In our work a novel distance function is constructed based on local second order statistics as estimated by modeling the training data as a mixture of normal densities. We report on the results from mixtures of several sizes. We demonstrate that a flat mixture of mixtures performs as well as the best model and therefore represents an effective solution to the model selection problem. A mixture perspective is also taken for individual Gaussians to choose between first order (variance) and second order (covariance) models. Here an approximation to flat combination is proposed and seen to perform well in practice. Our results demonstrate that even in the absence of multiple training examples for each class, it is sometimes possible to infer from a statistical model of training data, a significantly improved distance function for use in pattern recognition",
        "date": "July 1996",
        "authors": [
            "I.J. Cox",
            "Joumana Ghosn",
            "Peter N. Yianilos"
        ],
        "references": [
            258972643,
            285090956,
            257827224,
            256484410,
            252068915,
            243673769,
            240177294,
            224112606,
            222772244,
            222637342
        ]
    },
    {
        "id": 3043085,
        "title": "Distortion Invariant Object Recognition in the Dynamic Link Architecture",
        "abstract": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons",
        "date": "April 1993",
        "authors": [
            "Martin Lades",
            "J.C. Vorbruggen",
            "Joachim M Buhmann",
            "Joerg Lange"
        ],
        "references": [
            294343027,
            265577005,
            244437525,
            313763919,
            312944963,
            284428377,
            282404685,
            257650162,
            247123095,
            245805381
        ]
    },
    {
        "id": 2782316,
        "title": "Benchmark Studies on Face Recognition",
        "abstract": "We describe herein FERET - a major R&D program aimed at advancing the state-of-the-art in automated face recognition. We address three complementary problems: (A) development of a representative data base set of facial images to train, test, and evaluate alternative face recognition schemes; (B) bench marking of both simple but well known algorithms, and of novel automated and integrated face recognition schemes; and (C) training and testing of human subjects to evaluate human performance using the same data base developed to test the automated face recognition component. The major results of our R&D program indicate that (i) future advances in automated face recognition are predicated on the development of hybrid recognition systems, (ii) that holistic (connectionist) methods outperform discrete (and direct) feature and correlation methods, (iii) that if the test beds are random and contextual cues are lacking human performance is quite poor, not consistent, and degrades rapidly when ...",
        "date": "November 1998",
        "authors": [
            "Srinivas Gutta",
            "Jeffrey Huang",
            "Dig Singh",
            "Imran Shah"
        ],
        "references": [
            312919299,
            223009662,
            3684520,
            2666669,
            2656414
        ]
    },
    {
        "id": 258515867,
        "title": "The FERET (face recognition technology) program",
        "abstract": "The mission of the Department of Defense (DoD) Counter-drug Technology Development Program Office's Face Recognition Technology (FERET) program is to develop automatic face recognition systems from the development of detection and recognition algorithms in the laboratory through their demonstration in a prototype real-time system. To achieve this objective, the program supports research in face recognition algorithms, the collection of a large database of facial images, independent testing and evaluation of face recognition algorithms, construction of a real-time demonstration systems, and the integration of algorithms into the demonstration systems. The FERET program has established baseline performance for face recognition. The Army Research Laboratory (ARL) has been the technical agent for the Advanced Research Projects Agency since 1993, managing development of the recognition algorithms, database collection, and algorithm testing. Currently ARL is managing the development of several prototype face recognition systems that will demonstrate complete real-time video face identification in an access control mission. This paper gives an overview of the FERET program, presents recent performance results of face recognition algorithms evaluated, and addresses the future direction of the program and applications for DoD and law enforcement.",
        "date": "February 1997",
        "authors": [
            "Patrick J. Rauss",
            "Jonathan Phillips",
            "Hyeonjoon Moon",
            "Syed A. Rizvi"
        ],
        "references": []
    },
    {
        "id": 256075302,
        "title": "Eigenfaces for Recognition",
        "abstract": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.",
        "date": "January 1991",
        "authors": [
            "Matthew Turk",
            "Alex Pentland"
        ],
        "references": [
            284683101,
            279851325,
            279851068,
            273995705,
            271618882,
            271559168,
            269996289,
            268586757,
            314593430,
            313614377
        ]
    },
    {
        "id": 252310255,
        "title": "Comparison of view-based face recognition algorithms on visible and infrared imagery",
        "abstract": "This paper presents initial results in a study comparing the effectiveness of visible and infra-red (IR) imagery for detecting and recognizing faces in areas where personnel identification is critical, (e.g., airports and secure buildings). We compare the effectiveness of visible versus IR imagery by running three face recognition algorithms on a database of images collected for this study. There are both IR and visible images for each person in the database collected using the same scenarios. We used three very different feature-extraction and decision-making algorithms for our study to insure that the comparisons would not depend on a particular processing technique. We also present recognition results when visible and infra-red decision metrics are fused. The recognition results show that both visible and IR imagery perform similarly across algorithms and that fusion of IR and visible imagery is a viable means of enhancing performance beyond that of either acting alone. We examine the relative importance of different regions of the face for recognition. We also discuss practical issues of implementation, along with plans for the next phase of the study, face detection in an uncontrolled environment. Preliminary face detection experiments are described.",
        "date": "February 1997",
        "authors": [
            "Joseph Wilder",
            "P. Jonathan Phillips",
            "Cunhong Jiang",
            "Stephen M. Wiener"
        ],
        "references": []
    },
    {
        "id": 221620547,
        "title": "Latent Dirichlet Allocation",
        "abstract": "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.",
        "date": "January 2001",
        "authors": [
            "David M. Blei",
            "Andrew Y. Ng",
            "Michael Jordan"
        ],
        "references": [
            334269133,
            314001725,
            303802749,
            301222514,
            277378042,
            265353559,
            262333164,
            245594601,
            244419624,
            243766399
        ]
    },
    {
        "id": 220660282,
        "title": "Saliency, Scale and Image Description",
        "abstract": "Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent. This paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliency; scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated.",
        "date": "November 2001",
        "authors": [
            "Timor Kadir",
            "Michael Brady"
        ],
        "references": [
            313606555,
            313201808,
            291179102,
            286632024,
            279235450,
            247445349,
            247430109,
            246498134,
            244427107,
            243655500
        ]
    },
    {
        "id": 4082304,
        "title": "Learning methods for generic object recognition with invariance to pose and lighting",
        "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",
        "date": "January 2004",
        "authors": [
            "Yann Lecun",
            "Fu Jie Huang",
            "L. Bottou"
        ],
        "references": [
            226359900,
            221620338,
            221364881,
            313562430,
            312468236,
            281496262,
            245080835,
            240224936,
            221363018,
            221304142
        ]
    },
    {
        "id": 4038433,
        "title": "Object recognition with informative features and linear classification",
        "abstract": "We show that efficient object recognition can be obtained by combining informative features with linear classification. The results demonstrate the superiority of informative class-specific features, as compared with generic type features such as wavelets, for the task of object recognition. We show that information rich features can reach optimal performance with simple linear separation rules, while generic feature based classifiers require more complex classification schemes. This is significant because efficient and optimal methods have been developed for spaces that allow linear separation. To compare different strategies for feature extraction, we trained and compared classifiers working in feature spaces of the same low dimensionality, using two feature types (image fragments vs. wavelets) and two classification rules (linear hyperplane and a Bayesian network). The results show that by maximizing the individual information of the features, it is possible to obtain efficient classification by a simple linear separating rule, as well as more efficient learning.",
        "date": "November 2003",
        "authors": [
            "Michel Vidal-Naquet",
            "Shimon Ullman"
        ],
        "references": [
            319877813,
            313440997,
            281496262,
            256075302,
            248848344,
            246047425,
            243763580,
            240224936,
            238740116,
            227201563
        ]
    },
    {
        "id": 3940582,
        "title": "Rapid Object Detection using a Boosted Cascade of Simple Features",
        "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.",
        "date": "February 2001",
        "authors": [
            "Paul Viola",
            "Michael Jeffrey Jones"
        ],
        "references": [
            221619409,
            319770176,
            281496262,
            261584732,
            245080835,
            235262728,
            234791241,
            227025232,
            225070191,
            222864111
        ]
    },
    {
        "id": 3906069,
        "title": "Indexing based on scale invariant interest points",
        "abstract": "This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, of affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images",
        "date": "February 2001",
        "authors": [
            "Krystian Mikolajczyk",
            "Cordelia Schmid"
        ],
        "references": [
            221363670,
            3854196,
            3854190,
            3191935,
            286632024,
            221305307,
            221304229,
            220659552,
            3816624,
            3192692
        ]
    },
    {
        "id": 2941307,
        "title": "Probabilistic Latent Semantic Indexing",
        "abstract": "Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain#speci#c synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing #LSI# by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and de#nes a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methodsaswell as over LSI. In particular, the combination of models with di#erent dimensionalities has proven to be advantageous. 1",
        "date": "April 2004",
        "authors": [
            "Thomas Hofmann"
        ],
        "references": [
            288940927,
            223616307,
            220814088,
            303802749,
            283617574,
            242362765,
            236025174,
            229100910,
            228057706,
            221995817
        ]
    },
    {
        "id": 2832818,
        "title": "Matching Words and Pictures",
        "abstract": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real #2003 Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David Blei and Michael Jordan.",
        "date": "May 2003",
        "authors": [
            "Kobus Barnard",
            "Pinar Duygulu",
            "David Forsyth",
            "David M. Blei"
        ],
        "references": [
            329648252,
            308468610,
            288869337,
            277297445,
            243771527,
            243766480,
            243449680,
            242575117,
            242530019,
            239744191
        ]
    },
    {
        "id": 221617821,
        "title": "Sparse Coding of Natural Images Using an Overcomplete Set of Limited Capacity Units.",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "Eizaburo Doi",
            "Michael S. Lewicki"
        ],
        "references": [
            243712413,
            228606103,
            16086010,
            283360188,
            281053130,
            246456885,
            232793509,
            220499975,
            13803866,
            12774896
        ]
    },
    {
        "id": 8655422,
        "title": "Spike Generator Limits Efficiency of Information Transfer in A Retinal Ganglion Cell",
        "abstract": "The quality of the signal a retinal ganglion cell transmits to the brain is important for preception because it sets the minimum detectable stimulus. The ganglion cell converts graded potentials into a spike train with a selective filter but in the process adds noise. To explore how efficiently information is transferred to spikes, we measured contrast detection threshold and increment threshold from graded potential and spike responses of brisk-transient ganglion cells. Intracellular responses to a spot flashed over the receptive field center of the cell were recorded in an intact mammalian retina maintained in vitro at 37 degrees C. Thresholds were measured in a single-interval forced-choice procedure with an ideal observer. The graded potential gave a detection threshold of 1.5% contrast, whereas spikes gave 3.8%. The graded potential also gave increment thresholds approximately twofold lower and carried approximately 60% more gray levels. Increment threshold \"dipped\" below the detection threshold at a low contrast (<5%) but increased rapidly at higher contrasts. The magnitude of the \"dipper\" for both graded potential and spikes could be predicted from a threshold nonlinearity in the responses. Depolarization of the cell by current injection reduced the detection threshold for spikes but also reduced the range of contrasts they can transmit. This suggests that contrast sensitivity and dynamic range are related in an essential trade-off.",
        "date": "April 2004",
        "authors": [
            "Narender K Dhingra",
            "Robert G Smith"
        ],
        "references": [
            284192255,
            29869136,
            248848344,
            246404602,
            246264673,
            246047412,
            245944551,
            245594671,
            244958953,
            232793509
        ]
    },
    {
        "id": 3302668,
        "title": "Optimal Linear Compression Under Unreliable Representation and Robust PCA Neural Models",
        "abstract": "In a typical linear data compression system the representation variables resulting from the coding operation are assumed totally reliable and therefore the solution in the mean-squared-error sense is an orthogonal projector to the so-called principal component subspace. When the representation variables are contaminated by additive noise which is uncorrelated with the signal, the problem is called noisy principal component analysis (NPCA) and the optimal MSE solution is not a trivial extension of PCA. We show that: the problem is not well defined unless we impose explicit or implicit constraints on either the coding or the decoding operator; orthogonality is not a property of the optimal solution under most constraints; and the signal components may or may not be reconstructed depending on the noise level. As the noise power increases, we observe rank reduction in the optimal solution under most reasonable constraints. In these cases it appears that it is preferable to omit the smaller signal components rather than attempting to reconstruct them. Finally, we show that standard Hebbian-type PCA learning algorithms are not optimally robust to noise, and propose a new Hebbian-type learning algorithm which is optimally robust in the NPCA sense",
        "date": "October 1999",
        "authors": [
            "Kostas Diamantaras",
            "Kurt Hornik",
            "Michael Gerassimos Strintzis"
        ],
        "references": [
            224740230,
            222464584,
            279958061,
            279938680,
            246910696,
            245069631,
            242554268,
            242395812,
            236222478,
            226965853
        ]
    },
    {
        "id": 220499975,
        "title": "What Does the Retina Know about Natural Scenes?",
        "abstract": "By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data.",
        "date": "March 1992",
        "authors": [
            "Joseph J. Atick",
            "A. Norman Redlich"
        ],
        "references": [
            220499594,
            17055952,
            246774921,
            243763408,
            243631439,
            227630801,
            23455570,
            19726562,
            18960620,
            18133451
        ]
    },
    {
        "id": 12774896,
        "title": "Information theory and neural coding",
        "abstract": "Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus-response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus-response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.",
        "date": "December 1999",
        "authors": [
            "Alexander Borst",
            "Fr\u00e9d\u00e9ric E Theunissen"
        ],
        "references": [
            285599550,
            253013456,
            51322938,
            21723813,
            21093228,
            20234320,
            16829152,
            15716862,
            15427971,
            15352688
        ]
    },
    {
        "id": 20925549,
        "title": "Topography of ganglion cells in human retina",
        "abstract": "We quantified the spatial distribution of presumed ganglion cells and displaced amacrine cells in unstained whole mounts of six young normal human retinas whose photoreceptor distributions had previously been characterized. Cells with large somata compared to their nuclei were considered ganglion cells; cells with small somata relative to their nuclei were considered displaced amacrine cells. Within the central area, ganglion cell densities reach 32,000--38,000 cells/mm\u00b2 in a horizontally oriented elliptical ring 0.4--2.0 mm from the foveal center. In peripheral retina, densities in nasal retina exceed those at corresponding eccentricities in temporal retina by more than 300%; superior exceeds inferior by 60%. Displaced amacrine cells represented 3% of the total cells in central retina and nearly 80% in the far periphery. A twofold range in the total number of ganglion cells (0.7 to 1.5 million) was largely explained by a similar range in ganglion cell density in different eyes. Cone and ganglion cell number were not correlated, and the overall cone: ganglion cell ratio ranged from 2.9 to 7.5 in different eyes. Peripheral cones and ganglion cells have different topographies, thus suggesting meridianal differences in convergence onto individual ganglion cells.",
        "date": "October 1990",
        "authors": [
            "Christine A Curcio",
            "Kimberly A. Allen"
        ],
        "references": [
            247083235,
            303328005,
            288853344,
            284653895,
            275113343,
            247839793,
            247660309,
            247107638,
            242864631,
            237133057
        ]
    },
    {
        "id": 313517478,
        "title": "Toeplitz and circulant matrices: A review",
        "abstract": "",
        "date": "January 2005",
        "authors": [
            "Robert M Gray"
        ],
        "references": []
    },
    {
        "id": 284144629,
        "title": "Psychophysical studies of monkey vision. III: Spatial luminanace contrast sensitivity of macaque and human observers",
        "abstract": "",
        "date": "January 1974",
        "authors": [
            "R.L. DeValois",
            "H. Morgan",
            "Donald Max Snodderly"
        ],
        "references": []
    },
    {
        "id": 246774921,
        "title": "Erratum to \u201cPsychophysical studies of monkey Vision-III. Spatial luminance contrast sensitivity tests of macaque and human observers\u201d [Vis. Res. 14 (1) (1974) 75\u201381]",
        "abstract": "The detectability of luminance modulated gratings of different spatial frequencies was determined at five different adaptation levels for three macaque monkeys and five normal human observers. The human and macaque observers gave results which were identical in form and similar in absolute values. Both species showed optimal contrast sensitivity in the middle spatial frequency range of about 3\u20135 c/deg with both low and high frequency attenuation, at high light levels. Contrast sensitivity to high frequencies dropped rapidly as adaptation levels were lowered, with a resulting shift in peak sensitivity to lower spatial frequencies. At the lowest adaptation level studied, neither macaque nor human observers showed any low frequency attenuation in the spatial luminance contrast sensitivity function.",
        "date": "January 1974",
        "authors": [
            "RL DEVALOIS",
            "Herman Morgan",
            "Donald Max Snodderly"
        ],
        "references": [
            284192255,
            236623778,
            284874311,
            232538822,
            18203818,
            18187741,
            18019428,
            17811118,
            17753320,
            17520201
        ]
    },
    {
        "id": 243763408,
        "title": "Towards a Theory of Early Visual Processing",
        "abstract": "We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a generalized redundancy subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise.",
        "date": "September 1990",
        "authors": [
            "Joseph J. Atick",
            "A. Norman Redlich"
        ],
        "references": [
            284192255,
            243712413,
            226212540,
            225288384,
            224949359,
            223889141,
            220500251,
            51738495,
            51470846,
            46381145
        ]
    },
    {
        "id": 232075035,
        "title": "Designing receptive fields for highest fidelity",
        "abstract": "We formulate a basic problem for neural encoding: the stimulus should be accurately represented in the neural responses. We use this criterion to design the optimal receptive fields of a model visual system. Since reconstruction fidelity is an ensemble average over signals and noise, the statistics of natural stimuli play a central role. We compare our results with those of similar studies which apply optimization principles based on information theory.",
        "date": "July 2009",
        "authors": [
            "Dan Ruderman"
        ],
        "references": [
            17055952,
            19726562
        ]
    },
    {
        "id": 230675458,
        "title": "The First Steps in Seeing",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "R W Rodieck"
        ],
        "references": []
    },
    {
        "id": 228083862,
        "title": "Digital Image Processing",
        "abstract": "",
        "date": "January 1977",
        "authors": [
            "R C Gonzalez",
            "R E Woods"
        ],
        "references": [
            49184555
        ]
    },
    {
        "id": 220660439,
        "title": "Learning Low-Level Vision",
        "abstract": "We describe a learning-based method for low-level vision problems\u2014estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA\u2014Vision by Image/Scene TrAining. We apply VISTA to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a \u201cblobs world\u201d, we show figure/ground discrimination, solution of the aperture problem, and filling-in arising from application of the same probabilistic machinery.",
        "date": "October 2000",
        "authors": [
            "William T. Freeman",
            "Egon C. Pasztor",
            "Owen Thomas Carmichael"
        ],
        "references": [
            291179102,
            274202844,
            271512887,
            263499383,
            261389896,
            253020079,
            246668889,
            243786502,
            243673833,
            242706958
        ]
    },
    {
        "id": 283360188,
        "title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1?",
        "abstract": "The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.",
        "date": "December 1997",
        "authors": [
            "B A Olshausen",
            "David J. Field"
        ],
        "references": []
    },
    {
        "id": 281309188,
        "title": "Products of experts",
        "abstract": "It is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing. This is a very efficient way to model high-dimensional data which simultaneously satisfies many different low-dimensional constraints. Each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints. Data vectors that satisfy this one constraint but violate other constraints will be ruled out by their low probability under the other expert models. Training a product of models appears difficult because, in addition to maximizing the probabilities that the individual models assign to the observed data, it is necessary to make the models disagree on unobserved regions of the data space: It is fine for one model to assign a high probability to an unobserved region as long as some other model assigns it a very low probability. Fortunately, if the individual models are tractable there is a fairly efficient way to train a product of models. This training algorithm suggests a biologically plausible way of learning neural population codes.",
        "date": "January 1999",
        "authors": [
            "Geoffrey E. Hinton"
        ],
        "references": []
    },
    {
        "id": 271512969,
        "title": "Probabilistic Inference Using Markov Chain Monte Carlo Methods",
        "abstract": "",
        "date": "January 1993",
        "authors": [
            "R M Neal"
        ],
        "references": []
    },
    {
        "id": 256424823,
        "title": "Bayesian Learning of Sparse Multiscale Image Representations",
        "abstract": "Multiscale representations of images have become a standard tool in image analysis. Such representations offer a number of advantages over fixed-scale methods, including the potential for improved performance in denoising, compression, and the ability to represent distinct but complementary information that exists at various scales. A variety of multiresolution transforms exist, including both orthogonal decompositions such as wavelets as well as non-orthogonal, overcomplete representations. Recently, techniques for finding adaptive, sparse representations have yielded state-of-the-art results when applied to traditional image processing problems. Attempts at developing multiscale versions of these so-called dictionary learning models have yielded modest but encouraging results. However, none of these techniques has sought to combine a rigorous statistical formulation of the multiscale dictionary learning problem and the ability to share atoms across scales. We present a model for multiscale dictionary learning that overcomes some of the drawbacks of previous approaches by first decomposing an input into a pyramid of distinct frequency bands using a recursive filtering scheme, after which we perform dictionary learning and sparse coding on the individual levels of the resulting pyramid. The associated image model allows us to utilize a single set of adapted dictionary atoms that is shared - and learned - across all scales in the model. The underlying statistical model of our proposed method is fully Bayesian and allows for efficient inference of parameters, including the level of additive noise for denoising applications. We apply the proposed model to several common image processing problems including non-Gaussian and non-stationary denoising of real-world color images.",
        "date": "August 2013",
        "authors": [
            "James Michael Hughes",
            "Daniel N. Rockmore",
            "Yang Wang"
        ],
        "references": [
            246532602,
            235067347,
            225092525,
            224242654,
            224078861,
            221620168,
            221618878,
            221346108,
            220848162,
            220385839
        ]
    },
    {
        "id": 247131283,
        "title": "Image Quality Assessment: From Error Visibility to Structural Similarity",
        "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
        "date": "September 2004",
        "authors": [
            "Zhou Wang",
            "Alan Bovik",
            "Hamid Rahim Sheikh",
            "Eero P. Simoncelli"
        ],
        "references": [
            15251410
        ]
    },
    {
        "id": 239582331,
        "title": "Stochastic Relaxation, Gibbs Distributions and the Bayesian Resoration of Images",
        "abstract": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.",
        "date": "June 1984",
        "authors": [
            "Stuart Geman",
            "DONALD GEMAN"
        ],
        "references": [
            243001976,
            224877198,
            3115960,
            324468987,
            316799237,
            291407018,
            268708773,
            268553359,
            265665306,
            243777740
        ]
    },
    {
        "id": 221619891,
        "title": "Learning Sparse Topographic Representations with Products of Student-t Distributions.",
        "abstract": "We propose a model for natural images in which the probability of an im- age is proportional to the product of the probabilities of some filter out- puts. We encourage the system to find sparse features by using a Student- t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the sys- tem learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the \"iterated Wiener filter\" for the pur- pose of denoising images.",
        "date": "January 2002",
        "authors": [
            "Max Welling",
            "Geoffrey E. Hinton",
            "Simon Osindero"
        ],
        "references": [
            279605441,
            3192689,
            2815339,
            2802767,
            12809163,
            11899871,
            11207765,
            2645146
        ]
    },
    {
        "id": 220182914,
        "title": "Geman, D.: Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. PAMI-6(6), 721-741",
        "abstract": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.",
        "date": "November 1984",
        "authors": [
            "Stuart Geman",
            "Donald Geman"
        ],
        "references": [
            260869405,
            224877198,
            6026283,
            3115960,
            3081033,
            312613930,
            312446607,
            305979404,
            291407018,
            290228295
        ]
    },
    {
        "id": 252457602,
        "title": "Higher-Order Boltzmann Machines",
        "abstract": "The Boltzmann machine is a nonlinear network of stochastic binary processing units that interact pairwise through symmetric connection strengths. In a third-order Boltzmann machine, triples of units interact through symmetric conjunctive interactions. The Boltzmann learning algorithm is generalized to higher-order interactions. The rate of learning for internal representations in a higher-order Boltzmann machine should be much faster than for a second-order Boltzmann machine based on pairwise interactions.",
        "date": "March 1987",
        "authors": [
            "Terrence J. Sejnowski"
        ],
        "references": [
            275807826,
            220182914,
            31643034
        ]
    },
    {
        "id": 224881745,
        "title": "GTM: The Generative Topographic Mapping",
        "abstract": "Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline. GTM: The Generative Topographic Mapping 2 1 Introduction Many data sets exhibit significant correlations between the variables. One way to capture such structure is to model the distribution of the data in term...",
        "date": "May 1997",
        "authors": [
            "Christopher M. Bishop",
            "Markus Svensen",
            "Christopher K. I. Williams"
        ],
        "references": [
            324629477,
            324628605,
            274439247,
            274202844,
            272745997,
            271673518,
            271673472,
            270643849,
            268066959,
            268065833
        ]
    },
    {
        "id": 221619544,
        "title": "Exponential Family Harmoniums with an Application to Information Retrieval.",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "Max Welling",
            "Michal Rosen-Zvi",
            "Geoffrey E. Hinton"
        ],
        "references": [
            239571798,
            235356630,
            229157667,
            304109482,
            303802749,
            253766770,
            245787763,
            234108836,
            228057706,
            223575778
        ]
    },
    {
        "id": 221111597,
        "title": "On the Spatial Statistics of Optical Flow",
        "abstract": "We develop a method for learning the spatial statistics of optical flow fields from a novel training database. Training flow fields are constructed using range images of natural scenes and 3D camera motions recovered from handheld and car-mounted video sequences. A detailed analysis of optical flow statistics in natural scenes is presented and machine learning methods are developed to learn a Markov random field model of optical flow. The prior probability of a flow field is formulated as a field-of-experts model that captures the higher order spatial statistics in overlapping patches and is trained using contrastive divergence. This new optical flow prior is compared with previous robust priors and is incorporated into a recent, accurate algorithm for dense optical flow computation. Experiments with natural and synthetic sequences illustrate how the learned optical flow prior quantitatively improves flow accuracy and how it captures the rich spatial structure found in natural scene motion.",
        "date": "January 2005",
        "authors": [
            "Stefan Roth",
            "Michael J Black"
        ],
        "references": [
            228728619,
            227132073,
            226117099,
            225716342,
            304109482,
            296924802,
            283243746,
            260920174,
            228583641,
            224730474
        ]
    },
    {
        "id": 220660459,
        "title": "Design and Use of Linear Models for Image Motion Analysis",
        "abstract": "Linear parameterized models of optical flow, particularly affine models, have become widespread in image motion analysis. The linear model coefficients are straightforward to estimate, and they provide reliable estimates of the optical flow of smooth surfaces. Here we explore the use of parameterized motion models that represent much more varied and complex motions. Our goals are threefold: to construct linear bases for complex motion phenomena; to estimate the coefficients of these linear models; and to recognize or classify image motions from the estimated coefficients. We consider two broad classes of motions: i) generic \u201cmotion features\u201d such as motion discontinuities and moving bars; and ii) non-rigid, object-specific, motions such as the motion of human mouths. For motion features we construct a basis of steerable flow fields that approximate the motion features. For object-specific motions we construct basis flow fields from example motions using principal component analysis. In both cases, the model coefficients can be estimated directly from spatiotemporal image derivatives with a robust, multi-resolution scheme. Finally, we show how these model coefficients can be use to detect and recognize specific motions such as occlusion boundaries and facial expressions.",
        "date": "February 2000",
        "authors": [
            "David J. Fleet",
            "Michael J Black",
            "Yaser Yacoob",
            "Allan D. Jepson"
        ],
        "references": [
            280794779,
            277283457,
            271424460,
            263499383,
            262233141,
            262163115,
            245328289,
            243784272,
            243667034,
            243638431
        ]
    },
    {
        "id": 220500224,
        "title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
        "abstract": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.",
        "date": "July 1998",
        "authors": [
            "Bernhard Sch\u00f6lkopf",
            "Alex J. Smola",
            "Klaus-Robert M\u00fcller"
        ],
        "references": [
            271637725,
            313869997,
            313763919,
            299740221,
            281510136,
            279384963,
            275090260,
            274610238,
            273748014,
            271513141
        ]
    },
    {
        "id": 220319920,
        "title": "Feature Extraction by Non-Parametric Mutual Information Maximization.",
        "abstract": "We present a method for learning discriminative feature transforms using as criterion the mutual information between class labels and transformed features. Instead of a commonly used mutual information measure based on Kullback-Leibler divergence, we use a quadratic divergence measure, which allows us to make an efficient non-parametric implementation and requires no prior assumptions about class densities. In addition to linear transforms, we also discuss nonlinear transforms that are implemented as radial basis function networks. Extensions to reduce the computational complexity are also presented, and a comparison to greedy feature selection is made.",
        "date": "January 2003",
        "authors": [
            "Kari Torkkola"
        ],
        "references": [
            244958444,
            239041607,
            238748609,
            237127798,
            230875979,
            225304623,
            224817392,
            224663328,
            223657690,
            222483952
        ]
    },
    {
        "id": 216792753,
        "title": "Loss Functions for Discriminative Training of Energy-Based Models",
        "abstract": "Probabilistic graphical models associate a prob- ability to each configuration of the relevant vari- ables. Energy-based models (EBM) associate an energy to those configurations, eliminating the need for proper normalization of probability dis- tributions. Making a decision (an inference) with an EBM consists in comparing the energies asso- ciated with various configurations of the variable to be predicted, and choosing the one with the smallest energy. Such systems must be trained discriminatively to associate low energies to the desired configurations and higher energies to un- desired configurations. A wide variety of loss function can be used for this purpose. We give sufficient conditions that a loss function should satisfy so that its minimization will cause the sys- tem to approach to desired behavior. We give many specific examples of suitable loss func- tions, and show an application to object recog- nition in images.",
        "date": "January 2005",
        "authors": [
            "Yann Lecun",
            "Fu Jie Huang"
        ],
        "references": [
            240039219,
            5576473,
            4082304,
            2985446,
            296924802,
            248208758,
            246735299,
            221491038,
            3080334,
            2899890
        ]
    },
    {
        "id": 44649467,
        "title": "Learning, Invariance, and Generalization in High-Order Neural Networks",
        "abstract": "High-order neural networks have been shown to have impressive computational, storage, and learning capabilities. This performance is because the order or structure of a high-order neural network can be tailored to the order or structure of a problem. Thus, a neural network designed for a particular class of problems becomes specialized but also very efficient in solving those problems. Furthermore, a priori knowledge, such as geometric invariances, can be encoded in high-order networks. Because this knowledge does not have to be learned, these networks are very efficient in solving problems that utilize this knowledge.",
        "date": "December 1987",
        "authors": [
            "C. Lee Giles",
            "Thomas Maxwell"
        ],
        "references": [
            252457602,
            269312567,
            262212575,
            256232217,
            243743707,
            242530350,
            234926064,
            234825620,
            230876435,
            222449051
        ]
    },
    {
        "id": 263917978,
        "title": "Context-sensitive coding, associative memory, and serial order in (speech) behavior",
        "abstract": "Defines the problem of serial order in noncreative behavior in much the same manner as K. S. Lashley, and examines several theories of serial order. Lashley's rejection of associative-chain theories of serial order is shown to apply to 1 particular theory, and to be invalid as applied to other associative theories. The most plausible theory is the context-sensitive associative theory, which assumes that serial order is encoded by means of associations between context-sensitive elementary motor responses. In speech, this means that a word, e.g., sleep, is assumed to be coded allophonically rather than phonemically. This theory handles the pronunciation of single words and even phrases in a certain sense. (19 ref.) (PsycINFO Database Record (c) 2012 APA, all rights reserved)",
        "date": "January 1969",
        "authors": [
            "Wayne A. Wickelgran"
        ],
        "references": []
    },
    {
        "id": 248457484,
        "title": "What was transformational grammar?. A review of: Noam Chomsky, The Logical Structure of Linguistic Theory* * Published by Plenum Press, New York, 1975. 573 pp.",
        "abstract": "",
        "date": "August 1979",
        "authors": [
            "Geoffrey Sampson"
        ],
        "references": [
            324625271,
            288879702,
            271019322,
            270019977,
            268994742,
            268622075,
            260180118,
            242430108,
            239537334,
            231926648
        ]
    },
    {
        "id": 230875900,
        "title": "Rules and Schemas in the Development and Use of the English past Tense",
        "abstract": "Consistent error patterns in English past-tense forms are reported for three age groups: preschoolers, 8-10-year-olds, and adults. It is argued that, although irregular forms are rote-learned, speakers make generalizations about such forms. Such a generalization is defined as a SCHEMA which describes general phonological properties of a morphological class, and is used in organizing and accessing the lexicon. Schemas for the English past tense develop and change with age, yielding implications for both acquisitional and diachronic theory.",
        "date": "June 1982",
        "authors": [
            "Joan Bybee",
            "Dan I. Slobin"
        ],
        "references": [
            243750735,
            239588988,
            239456960,
            237127659,
            230876024,
            223422777,
            222997452,
            222740888,
            222630383,
            222315920
        ]
    },
    {
        "id": 222630383,
        "title": "The acquisition of regular and irregular past tense forms",
        "abstract": "The spontaneous speech samples of 15 children were analyzed for appropriate use and inappropriate use and nonuse of the past tense verbal inflection. Using this data base, the following hypotheses were examined: (1) The irregular past tense form is an earlier acquisition than the regular past tense form. (2) The two types of overgeneralization errors (goed vs wented) have acquisitional relevance. (3) Partial regularity blocks overgeneralization errors. (4) The regular rule for the application of -ed is more likely to be overgeneralized to irregular forms such as hit, shut, and put than to other irregular forms. The data provided partial support for the second and third hypotheses, but no support for the first or fourth hypotheses.",
        "date": "October 1977",
        "authors": [
            "Stan Kuczaj"
        ],
        "references": [
            313132242,
            246421749,
            292720207,
            269850349,
            269535800,
            231931967,
            223078573,
            222315920,
            35414906,
            17535988
        ]
    },
    {
        "id": 49552016,
        "title": "Verbal Behavior",
        "abstract": "",
        "date": "January 1959",
        "authors": [
            "Burrhus F. Skinner"
        ],
        "references": []
    },
    {
        "id": 260869405,
        "title": "Optimal perceptual inference",
        "abstract": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth'value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:",
        "date": "January 1983",
        "authors": [
            "Geoffrey E Hinton",
            "Terrence J Sejnowski"
        ],
        "references": [
            292215395,
            269861205,
            242498730,
            239653789,
            234828942,
            228083850,
            224377733,
            222462352,
            222066725,
            220816028
        ]
    },
    {
        "id": 242609671,
        "title": "Learning Internal Representations from Gray-Scale Images: An Example of Extensional Programming",
        "abstract": "",
        "date": "January 1987",
        "authors": [
            "Garrison W. Cottrell",
            "Paul W Munro",
            "D. Zipser"
        ],
        "references": []
    },
    {
        "id": 242509302,
        "title": "Learning and relearning in Boltzmann machines",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "G. E. Hinton",
            "T. J. Sejnowski"
        ],
        "references": []
    },
    {
        "id": 241560014,
        "title": "G-Maximization: an Unsupervised Learning Procedure for Discovering Regularities",
        "abstract": "Hill climbing is used to maximize an information theoretic measure of the difference between the actual behavior of a unit and the behavior that would be predicted by a statistician who knew the first order statistics of the inputs but believed them to be independent. This causes the unit to detect higher order correlations among its inputs. Initial simulations are presented, and seem encouraging. We describe an extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input.",
        "date": "March 1987",
        "authors": [
            "Barak A. Pearlmutter",
            "Geoffrey E. Hinton"
        ],
        "references": [
            226282988,
            18789147,
            243743707,
            242554268,
            242530350,
            242448310,
            222451035,
            222449051,
            200744578,
            50336131
        ]
    },
    {
        "id": 345793666,
        "title": "The evolution of data processing abilities in competing automata",
        "abstract": "",
        "date": "December 1988",
        "authors": [
            "Michel Kerszberg",
            "Aviv Bergman"
        ],
        "references": []
    },
    {
        "id": 313161692,
        "title": "Adaptive Switching Circuits",
        "abstract": "",
        "date": "January 1960",
        "authors": [
            "B. Widrow",
            "M.E. Hoff"
        ],
        "references": []
    },
    {
        "id": 304541496,
        "title": "Holography, associative memory, and inductive generalization",
        "abstract": "",
        "date": "January 1981",
        "authors": [
            "D. Willshaw"
        ],
        "references": []
    },
    {
        "id": 292740784,
        "title": "Mod\u00e8les connexionnistes de l'apprentissage",
        "abstract": "",
        "date": "January 1987",
        "authors": [
            "Yangke Cun"
        ],
        "references": []
    },
    {
        "id": 321325065,
        "title": "Distilling a Neural Network Into a Soft Decision Tree",
        "abstract": "Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.",
        "date": "November 2017",
        "authors": [
            "Nicholas Frosst",
            "Geoffrey Hinton"
        ],
        "references": [
            305196650,
            283762569,
            277411157,
            265022827,
            221653840,
            2524678,
            319770355,
            319770247,
            319769909,
            301847993
        ]
    },
    {
        "id": 320968593,
        "title": "Deep Pyramidal Residual Networks",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Dongyoon Han",
            "Jiwhan Kim",
            "Junmo Kim"
        ],
        "references": [
            305196650,
            303409435,
            301879329,
            269935397,
            265908778,
            265295439,
            259441043,
            2985446,
            319770430,
            319770414
        ]
    },
    {
        "id": 320966701,
        "title": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning",
        "abstract": "",
        "date": "July 2017",
        "authors": [
            "Junho Yim",
            "Donggyu Joo",
            "Ji-Hoon Bae",
            "Junmo Kim"
        ],
        "references": [
            305196650,
            303409435,
            284476548,
            284219622,
            281312423,
            275897099,
            269935397,
            265295439,
            259440750,
            220320806
        ]
    },
    {
        "id": 319135616,
        "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
        "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results with almost no additional computational cost. We also show improved performance in the low-data regime on the STL-10 dataset.",
        "date": "August 2017",
        "authors": [
            "Terrance DeVries",
            "Graham W. Taylor"
        ],
        "references": [
            319770160,
            315655760,
            268451938,
            220320676,
            319770395,
            319770183,
            317194083,
            314521313,
            309076254,
            308277201
        ]
    },
    {
        "id": 318740426,
        "title": "Using the Output Embedding to Improve Language Models",
        "abstract": "",
        "date": "January 2017",
        "authors": [
            "Ofir Press",
            "Lior Wolf"
        ],
        "references": [
            301881141,
            283723770,
            281458937,
            275974238,
            265252627,
            264826516,
            257882504,
            234131319,
            224246503,
            221346280
        ]
    },
    {
        "id": 317615661,
        "title": "Sobolev Training for Neural Networks",
        "abstract": "At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.",
        "date": "June 2017",
        "authors": [
            "Wojciech Marian Czarnecki",
            "Simon Osindero",
            "Max Jaderberg",
            "Grzegorz Swirszcz"
        ],
        "references": [
            316700227,
            315764685,
            309572801,
            308026508,
            292074166,
            258816388,
            221361415,
            215616971,
            2354219,
            319770334
        ]
    },
    {
        "id": 317194083,
        "title": "Wide Residual Networks",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Sergey Zagoruyko",
            "Nikos Komodakis"
        ],
        "references": [
            322539221,
            308278012,
            284579051,
            269935397,
            265908778,
            231556969,
            221345414,
            216792737,
            319770422,
            319770418
        ]
    },
    {
        "id": 317101000,
        "title": "Continual Learning with Deep Generative Replay",
        "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.",
        "date": "May 2017",
        "authors": [
            "Hanul Shin",
            "Jung Kwon Lee",
            "Jaehong Kim",
            "Jiwon Kim"
        ],
        "references": [
            311411551,
            286966779,
            259441025,
            252325755,
            222068807,
            23238493,
            12977135,
            6707827,
            2985446,
            319770430
        ]
    },
    {
        "id": 317061748,
        "title": "Shake-Shake regularization",
        "abstract": "The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https://github.com/xgastaldi/shake-shake.",
        "date": "May 2017",
        "authors": [
            "Xavier Gastaldi"
        ],
        "references": [
            308320871,
            308278012,
            306885833,
            306187421,
            319770414,
            319770236,
            311430372,
            310441005,
            303520977,
            303449354
        ]
    },
    {
        "id": 306093553,
        "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.",
        "date": "March 2016",
        "authors": [
            "Xuezhe Ma",
            "Eduard Hovy"
        ],
        "references": [
            306093561,
            306093067,
            305334469,
            301445941,
            319770272,
            319770184,
            303256841,
            301840112,
            301445808,
            301445784
        ]
    },
    {
        "id": 303521296,
        "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
        "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.",
        "date": "April 2017",
        "authors": [
            "Takeru Miyato",
            "Andrew M. Dai",
            "Ian Goodfellow"
        ],
        "references": [
            301845925,
            301839500,
            319770465,
            319770387,
            313073822,
            312375903,
            311469848,
            307604438,
            301854788,
            301533328
        ]
    },
    {
        "id": 313763061,
        "title": "Training with noise is equivalent to tikhonov regularization",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "C.M. Bishop"
        ],
        "references": []
    },
    {
        "id": 312283342,
        "title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation",
        "abstract": "In this paper, we compare two simple domain adaptation methods for neural machine translation (NMT): (1) We append an artificial token to the source sentences of two parallel corpora (different domains and one of them is resource scarce) to indicate the domain and then mix them to learn a multi domain NMT model; (2) We learn a NMT model on the resource rich domain corpus and then fine tune it using the resource poor domain corpus. We empirically verify fine tuning works better than the artificial token mechanism when the low resource domain corpus is of relatively poor quality (acquired via automatic extraction) but in the case of a high quality (manually created) low resource domain corpus both methods are equally viable.",
        "date": "January 2017",
        "authors": [
            "Chenhui Chu",
            "Raj Dabre",
            "Sadao Kurohashi"
        ],
        "references": [
            336996364,
            311736931,
            273471465,
            265252627,
            319770465,
            310235569,
            306093632,
            306092988,
            305334559,
            301876115
        ]
    },
    {
        "id": 311106875,
        "title": "NewsQA: A Machine Comprehension Dataset",
        "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.",
        "date": "November 2016",
        "authors": [
            "Adam Trischler",
            "Tong Wang",
            "Xingdi Yuan",
            "Justin Harris"
        ],
        "references": [
            306093071,
            304018244,
            303840417,
            311612456,
            308883580,
            308804607,
            307302995,
            306094228,
            306093209,
            303840542
        ]
    },
    {
        "id": 309738279,
        "title": "Words or Characters? Fine-grained Gating for Reading Comprehension",
        "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.",
        "date": "November 2016",
        "authors": [
            "Zhilin Yang",
            "Bhuwan Dhingra",
            "Ye Yuan",
            "Junjie Hu"
        ],
        "references": [
            311990185,
            305388870,
            304226007,
            309572512,
            307302995,
            306094228,
            306093838,
            306093774,
            306093209,
            305388779
        ]
    },
    {
        "id": 327455026,
        "title": "Transferable Representation Learning with Deep Adaptation Networks",
        "abstract": "Domain adaptation generalizes a learning machine across source domain and target domain under different distributions. Recent studies reveal that deep neural networks can learn transferable features generalizing well to similar novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the dataset shift and enhance the feature transferability in task-specific layers, this paper presents a novel framework for deep adaptation networks, which generalizes deep convolutional neural networks to domain adaptation. The framework embeds the deep features of all task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and optimally match different domain distributions. The deep features are made more transferable by exploring low-density separation of target-unlabeled data and very deep architectures, while the domain discrepancy is further reduced using multiple kernel learning for maximal testing power of kernel embedding matching. This leads to a minimax game framework that learns transferable features with statistical guarantees, and scales linearly with unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed networks yield state-of-the-art results on standard visual domain adaptation benchmarks.",
        "date": "September 2018",
        "authors": [
            "Mingsheng Long",
            "Yue Cao",
            "Zhangjie Cao",
            "Jianmin Wang"
        ],
        "references": [
            311715036,
            305196650,
            303448960,
            303448665,
            278413480,
            269636522,
            268079628,
            266201822,
            265295439,
            264979485
        ]
    },
    {
        "id": 311612456,
        "title": "Multi-Perspective Context Matching for Machine Comprehension",
        "abstract": "Previous machine comprehension (MC) datasets are either too small to train end-to-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our model first adjusts each word-embedding vector in the passage by multiplying a relevancy weight computed against the question. Then, we encode the question and weighted passage by using bi-directional LSTMs. For each point in the passage, our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector. Given those matched vectors, we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.",
        "date": "December 2016",
        "authors": [
            "Zhiguo Wang",
            "Haitao Mi",
            "Wael Hamza",
            "Radu Florian"
        ],
        "references": [
            301872750,
            284576917,
            272091377,
            257882504,
            13853244,
            312250707,
            301404729,
            286965813,
            278048614,
            278048272
        ]
    },
    {
        "id": 311067032,
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
        "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension.This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.",
        "date": "November 2016",
        "authors": [
            "Tri Nguyen",
            "Mir Rosenberg",
            "Xia Song",
            "Jianfeng Gao"
        ],
        "references": [
            304018244,
            283659163,
            265252627,
            262289160,
            220422212,
            13853244,
            2588204,
            319770465,
            319770430,
            319769995
        ]
    },
    {
        "id": 309738677,
        "title": "Bidirectional Attention Flow for Machine Comprehension",
        "abstract": "Machine Comprehension (MC), answering questions about a given context, re-quires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these mechanisms use attention to summarize the query and context into a single vectors, couple attentions temporally, and often form a unidirectional attention. In this paper we introduce the Bidirectional Attention Flow (BIDAF) Model, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA(SQuAD) and CNN/DailyMail Cloze Test datasets.",
        "date": "November 2016",
        "authors": [
            "Minjoon Seo",
            "Aniruddha Kembhavi",
            "Ali Farhadi",
            "Hannaneh Hajishirzi"
        ],
        "references": [
            311990185,
            305388870,
            305342143,
            304018244,
            303840417,
            303750057,
            284576917,
            283659163,
            283659096,
            275974823
        ]
    },
    {
        "id": 309738384,
        "title": "Dynamic Coattention Networks For Question Answering",
        "abstract": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.",
        "date": "November 2016",
        "authors": [
            "Caiming Xiong",
            "Victor Zhong",
            "Richard Socher"
        ],
        "references": [
            305388870,
            304018244,
            303840417,
            303750057,
            301404923,
            284576917,
            283659163,
            275897099,
            272091377,
            235639035
        ]
    },
    {
        "id": 309606886,
        "title": "Dual Learning for Machine Translation",
        "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \\emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.",
        "date": "November 2016",
        "authors": [
            "Yingce Xia",
            "Di He",
            "Tao Qin",
            "Liwei Wang"
        ],
        "references": [
            302768576,
            287250272,
            286513565,
            273471465,
            269280447,
            265252627,
            262877889,
            220816913,
            220418899,
            2588204
        ]
    },
    {
        "id": 319770364,
        "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks",
        "abstract": "We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.",
        "date": "November 2015",
        "authors": [
            "Leslie Smith",
            "Emily Hand",
            "Timothy Doster"
        ],
        "references": [
            319770160,
            307747289,
            283433655,
            277411157,
            274730358,
            265787949,
            265295439,
            264979485,
            260126867,
            231556969
        ]
    },
    {
        "id": 311610233,
        "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks",
        "abstract": "",
        "date": "June 2016",
        "authors": [
            "Leslie Smith",
            "Emily Hand",
            "Timothy Doster"
        ],
        "references": [
            319770160,
            307747289,
            305196650,
            283433655,
            277411157,
            274730358,
            269935401,
            269935397,
            265295439,
            264979485
        ]
    },
    {
        "id": 284476470,
        "title": "Towards Principled Unsupervised Learning",
        "abstract": "General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the sought-after supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.",
        "date": "November 2015",
        "authors": [
            "Ilya Sutskever",
            "Rafal Jozefowicz",
            "Karol Gregor",
            "Danilo Rezende"
        ],
        "references": [
            303137904,
            277333816,
            265252627,
            319770439,
            319770395,
            319770355,
            319770229,
            319770163,
            278733352,
            269416972
        ]
    },
    {
        "id": 319770102,
        "title": "Pixel-Level Domain Transfer",
        "abstract": "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.",
        "date": "March 2016",
        "authors": [
            "Donggeun Yoo",
            "Namil Kim",
            "Sunggyun Park",
            "Anthony S. Paek"
        ],
        "references": [
            285648388,
            265295439,
            261100864,
            221346269,
            319770416,
            319770355,
            319770269,
            319770234,
            319770183,
            319770144
        ]
    },
    {
        "id": 302384745,
        "title": "A Hilbert Space Embedding for Distributions",
        "abstract": "While kernel methods are the basis of many popular techniques in supervised learning, they are less commonly used in testing, estimation, and analysis of probability distributions, where information theoretic approaches rule the roost. However it becomes difficult to estimate mutual information or entropy if the data are high dimensional.",
        "date": "October 2007",
        "authors": [
            "Alex Smola",
            "Arthur Gretton",
            "Le Song",
            "Bernhard Sch\u00f6lkopf"
        ],
        "references": []
    },
    {
        "id": 301878493,
        "title": "Generating images with recurrent adversarial networks",
        "abstract": "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual \"canvas\". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.",
        "date": "February 2016",
        "authors": [
            "Daniel Jiwoong Im",
            "Chris Dongjoo Kim",
            "Hui Jiang",
            "Roland Memisevic"
        ],
        "references": [
            281312423,
            278048515,
            263012109,
            319770416,
            319770229,
            284476553,
            284476463,
            278733352,
            272422583,
            272194743
        ]
    },
    {
        "id": 284476553,
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
        "date": "November 2015",
        "authors": [
            "Alec Radford",
            "Luke Metz",
            "Soumith Chintala"
        ],
        "references": [
            279968088,
            278048515,
            266560705,
            263471626,
            257882504,
            236736855,
            221361415,
            221344904,
            220320676,
            2441269
        ]
    },
    {
        "id": 304409658,
        "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
        "abstract": "",
        "date": "December 2015",
        "authors": [
            "Alexey Dosovitskiy",
            "Philipp Fischer",
            "Eddy Ilg",
            "Philip H\u00e4usser"
        ],
        "references": [
            319770430,
            319770355,
            319770269,
            319770183,
            319770168,
            310752533,
            308854455,
            302579452,
            301921832,
            289788943
        ]
    },
    {
        "id": 289588029,
        "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis",
        "abstract": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.",
        "date": "January 2016",
        "authors": [
            "Jimei Yang",
            "Scott Reed",
            "Ming-Hsuan Yang",
            "Honglak Lee"
        ],
        "references": [
            319770160,
            286931946,
            319770269,
            319770183,
            319770168,
            308834606,
            304409693,
            301921832,
            289593486,
            289413058
        ]
    },
    {
        "id": 288889700,
        "title": "Autoencoding beyond pixels using a learned similarity metric",
        "abstract": "We present an autoencoder that leverages the power of learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors that better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that our method outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that our method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.",
        "date": "December 2015",
        "authors": [
            "Anders Boesen Lindbo Larsen",
            "S\u00f8ren Kaae S\u00f8nderby",
            "Ole Winther"
        ],
        "references": [
            319770369,
            319770355,
            319770269,
            319770234,
            319770229,
            319770214,
            319770134,
            319770112,
            308825079,
            308278833
        ]
    },
    {
        "id": 319770234,
        "title": "Deep multi-scale video prediction beyond mean square error",
        "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction is viewed as a promising avenue for unsupervised feature learning. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
        "date": "November 2016",
        "authors": [
            "Michael Mathieu",
            "Camille Couprie",
            "Yann Lecun"
        ],
        "references": []
    },
    {
        "id": 308812843,
        "title": "Rotating your face using multi-task deep neural network",
        "abstract": "",
        "date": "June 2015",
        "authors": [
            "Junho Yim",
            "Heechul Jung",
            "Byungin Yoo",
            "Changkyu Choi"
        ],
        "references": [
            264786906,
            263237688,
            263126809,
            262274394,
            255569350,
            228339739,
            221110585,
            51462810,
            289593486,
            286016734
        ]
    },
    {
        "id": 308278233,
        "title": "Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks",
        "abstract": "As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.",
        "date": "October 2016",
        "authors": [
            "Junyuan Xie",
            "Ross Girshick",
            "Ali Farhadi"
        ],
        "references": [
            304409658,
            304409176,
            286134669,
            279968654,
            275588416,
            269299958,
            268747978,
            258140919,
            224226543,
            220913824
        ]
    },
    {
        "id": 301876917,
        "title": "Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks",
        "abstract": "As 3D movie viewing becomes mainstream and Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks for automatically converting 2D videos and images to stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.",
        "date": "April 2016",
        "authors": [
            "Junyuan Xie",
            "Ross Girshick",
            "Ali Farhadi"
        ],
        "references": [
            286134669,
            279968654,
            269299958,
            268747978,
            258140919,
            224226543,
            220913824,
            319770234,
            308820111,
            284219584
        ]
    },
    {
        "id": 319770345,
        "title": "Exploring Models and Data for Image Question Answering",
        "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.",
        "date": "May 2015",
        "authors": [
            "Mengye Ren",
            "Ryan Kiros",
            "Richard Zemel"
        ],
        "references": []
    },
    {
        "id": 311990784,
        "title": "Character-Level Question Answering with Attention",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Xiaodong He",
            "David Golub"
        ],
        "references": [
            281607724,
            274380525,
            265252627,
            287607356,
            282162224,
            270878664
        ]
    },
    {
        "id": 305334469,
        "title": "Neural Architectures for Named Entity Recognition",
        "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.",
        "date": "March 2016",
        "authors": [
            "Guillaume Lample",
            "Miguel Ballesteros",
            "Sandeep Subramanian",
            "Kazuya Kawakami"
        ],
        "references": [
            301839160,
            301446021,
            281607724,
            319770439,
            319770369,
            311990382,
            301445808,
            285270985,
            283806596,
            281312208
        ]
    },
    {
        "id": 301404452,
        "title": "Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Qiao Qian",
            "Bo Tian",
            "Minlie Huang",
            "Yang Liu"
        ],
        "references": [
            304533937,
            289108770,
            286964721,
            286725696,
            284039049,
            270878508,
            270878336,
            265052545,
            262416109,
            262367926
        ]
    },
    {
        "id": 329978116,
        "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs",
        "abstract": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence\u2019s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https://github.com/yinwenpeng/Answer_Selection .",
        "date": "December 2016",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "references": [
            308845461,
            270283023,
            264978373,
            264003485,
            262484733,
            221012718,
            220817006,
            220320677,
            13853244,
            325933705
        ]
    },
    {
        "id": 301446024,
        "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Duyu Tang",
            "Bing Qin",
            "Ting Liu"
        ],
        "references": [
            284576917,
            275280239,
            273788760,
            273067823,
            272194766,
            272091377,
            266965495,
            266658581,
            263163564,
            262877889
        ]
    },
    {
        "id": 287249782,
        "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs",
        "abstract": "How to model a pair of sentences is a critical issue in many natural language processing (NLP) tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence separately, without considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNN; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNN achieves state-of-the-art performance on AS, PI and TE tasks.",
        "date": "December 2015",
        "authors": [
            "Wenpeng Yin",
            "Hinrich Sch\u00fctze",
            "Bing Xiang",
            "Bowen Zhou"
        ],
        "references": [
            308845461,
            288714023,
            285271115,
            284219105,
            283986649,
            273471942,
            269117118,
            269116878,
            265252627,
            264978373
        ]
    },
    {
        "id": 285648032,
        "title": "Target-Dependent Sentiment Classification with Long Short Term Memory",
        "abstract": "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.",
        "date": "December 2015",
        "authors": [
            "Duyu Tang",
            "Bing Qin",
            "Xiaocheng Feng",
            "Ting Liu"
        ],
        "references": [
            281607465,
            273067823,
            270878040,
            265252627,
            220874961,
            5583935,
            2413241,
            312451745,
            301453221,
            301445943
        ]
    },
    {
        "id": 306376567,
        "title": "RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism",
        "abstract": "Accuracy and interpretation are two goals of any successful predictive models. Most existing works have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN.",
        "date": "August 2016",
        "authors": [
            "Edward Choi",
            "Taha Bahadori",
            "Andy Schuetz",
            "Walter F Stewart"
        ],
        "references": [
            310825152,
            301847126,
            287251087,
            284219155,
            319770416,
            316492087,
            311531625,
            303256841,
            301848337,
            299969701
        ]
    },
    {
        "id": 334115433,
        "title": "Sequence Classification with Human Attention",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Maria Barrett",
            "Joachim Bingel",
            "Nora Hollenstein",
            "Marek Rei"
        ],
        "references": [
            329579355,
            320035887,
            318829585,
            306094030,
            305341989,
            304488244,
            301446107,
            301404440,
            287968328,
            284576917
        ]
    },
    {
        "id": 277959017,
        "title": "Learning to Transduce with Unbounded Memory",
        "abstract": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.",
        "date": "June 2015",
        "authors": [
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ],
        "references": [
            262877889,
            252383413,
            221012648,
            2600158,
            319770465,
            275896989,
            274403822,
            273157787,
            267157056,
            265554383
        ]
    },
    {
        "id": 7647316,
        "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "abstract": "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.",
        "date": "July 2005",
        "authors": [
            "Alex Graves",
            "J\u00fcrgen Schmidhuber"
        ],
        "references": [
            325817538,
            243683010,
            230875873,
            221038923,
            220956616,
            220320057,
            13853244,
            12571895,
            3316656,
            2880936
        ]
    },
    {
        "id": 331214506,
        "title": "Automated rationale generation: a technique for explainable AI and its effects on human perceptions",
        "abstract": "Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations, (b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated rationales with regard to confidence in the autonomous agent, communicating failure and unexpected behavior. Overall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users. Moreover, context permitting, participants preferred detailed rationales to form a stable mental model of the agent's behavior.",
        "date": "March 2019",
        "authors": [
            "Upol Ehsan",
            "Pradyumna Tambwekar",
            "Larry Chan",
            "Brent Harrison"
        ],
        "references": [
            325557708,
            324597789,
            323357380,
            322851636,
            317821828,
            345403059,
            324659154,
            322686862,
            320964595,
            318740995
        ]
    },
    {
        "id": 303499206,
        "title": "MIMIC-III, a freely accessible critical care database",
        "abstract": "MIMIC-III (\u2018Medical Information Mart for Intensive Care\u2019) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.",
        "date": "May 2016",
        "authors": [
            "Alistair Edward William Johnson",
            "Tom Joseph Pollard",
            "Lu Shen",
            "Li-wei H Lehman"
        ],
        "references": [
            300368995,
            259701267,
            259565237,
            233994869,
            51426095,
            281169127,
            259990341,
            235404960,
            224914474,
            49800047
        ]
    },
    {
        "id": 273786930,
        "title": "Pharmacovigilance from social media: Mining adverse drug reaction mentions using sequence labeling with word embedding cluster features",
        "abstract": "Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized for public health monitoring tasks, particularly for pharmacovigilance, via the use of natural language processing (NLP) techniques. However, the language in social media is highly informal, and user-expressed medical concepts are often nontechnical, descriptive, and challenging to extract. There has been limited progress in addressing these challenges, and thus far, advanced machine learning-based NLP techniques have been underutilized. Our objective is to design a machine learning-based approach to extract mentions of adverse drug reactions (ADRs) from highly informal text in social media. We introduce ADRMine, a machine learning-based concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes a variety of features, including a novel feature for modeling words' semantic similarities. The similarities are modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated from unlabeled user posts in social media using a deep learning technique. ADRMine outperforms several strong baseline systems in the ADR extraction task by achieving an F-measure of 0.82. Feature analysis demonstrates that the proposed word cluster features significantly improve extraction performance. It is possible to extract complex medical concepts, with relatively high performance, from informal, user-generated content. Our approach is particularly scalable, suitable for social media mining, as it relies on large volumes of unlabeled data, thus diminishing the need for large, annotated training data sets. \u00a9 The Author 2015. Published by Oxford University Press on behalf of the American Medical Informatics Association.",
        "date": "March 2015",
        "authors": [
            "Azadeh Nikfarjam",
            "Abeed Sarker",
            "Karen Oconnor",
            "Rachel Ginn"
        ],
        "references": [
            327163585,
            280446645,
            280301158,
            342183905,
            339504735,
            319770439,
            291190469,
            289942031,
            288783379,
            285278428
        ]
    },
    {
        "id": 334601988,
        "title": "Generating Token-Level Explanations for Natural Language Inference",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "James Thorne",
            "Andreas Vlachos",
            "Christos Christodoulopoulos",
            "Arpit Mittal"
        ],
        "references": [
            305334469,
            284576917,
            282181875,
            265252627,
            220320677,
            13853244,
            334116978,
            333507939,
            325449053,
            325448957
        ]
    },
    {
        "id": 330933090,
        "title": "Human\u2010centered artificial intelligence and machine learning",
        "abstract": "Humans are increasingly coming into contact with artificial intelligence (AI) and machine learning (ML) systems. Human\u2010centered AI is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human\u2010centered AI can be broken down into two aspects: (a) AI systems that understand humans from a sociocultural perspective, and (b) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.",
        "date": "February 2019",
        "authors": [
            "Mark Riedl"
        ],
        "references": [
            331214506,
            318830044,
            317088026,
            2333743,
            322929400,
            322058942,
            318740995,
            304409761,
            304127930,
            301476343
        ]
    },
    {
        "id": 325448981,
        "title": "Explainable Prediction of Medical Codes from Clinical Text",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "James Mullenbach",
            "Sarah Wiegreffe",
            "Jon Duke",
            "J. Sun"
        ],
        "references": [
            305334401,
            298500299,
            277339708,
            257882504,
            221614652,
            42541470,
            311990853,
            309047233,
            305334391,
            287249782
        ]
    },
    {
        "id": 321069982,
        "title": "Syntax-Directed Attention for Neural Machine Translation",
        "abstract": "Attention mechanism, including global attention and local attention, plays a key role in neural machine translation (NMT). Global attention attends to all source words for word prediction. In comparison, local attention selectively looks at fixed-window source words. However, alignment weights for the current target word often decrease to the left and right by linear distance centering on the aligned source position and neglect syntax-directed distance constraints. In this paper, we extend local attention with syntax-distance constraint, to focus on syntactically related source words with the predicted target word, thus learning a more effective context vector for word prediction. Moreover, we further propose a double context NMT architecture, which consists of a global context vector and a syntax-directed context vector over the global attention, to provide more translation performance for NMT from source representation. The experiments on the large-scale Chinese-to-English and English-to-Germen translation tasks show that the proposed approach achieves a substantial and significant improvement over the baseline system.",
        "date": "November 2017",
        "authors": [
            "Kehai Chen",
            "Rui Wang",
            "Masao Utiyama",
            "Eiichiro Sumita"
        ],
        "references": [
            320867552,
            318741834,
            318740524,
            318737549,
            319770465,
            318829766,
            318742364,
            318742172,
            318741485,
            318740820
        ]
    },
    {
        "id": 335781053,
        "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
        "abstract": "",
        "date": "August 2019",
        "authors": [
            "Elena Voita",
            "David Talbot",
            "Fedor Moiseev",
            "Rico Sennrich"
        ],
        "references": [
            334117983,
            332341730,
            328733327,
            324166896,
            311648037,
            277248670,
            272091377,
            334116565,
            334116154,
            320241826
        ]
    },
    {
        "id": 334600955,
        "title": "Linguistic Knowledge and Transferability of Contextual Representations",
        "abstract": "",
        "date": "January 2019",
        "authors": [
            "Nelson F. Liu",
            "Matt Gardner",
            "Yonatan Belinkov",
            "Matthew E. Peters"
        ],
        "references": []
    },
    {
        "id": 334118517,
        "title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Mario Giulianelli",
            "Jack Harding",
            "Florian Mohnert",
            "Dieuwke Hupkes"
        ],
        "references": [
            321347616,
            306187579,
            301843081,
            260524013,
            13853244,
            334515163,
            334116459,
            334115550,
            325448855,
            323956750
        ]
    },
    {
        "id": 334117413,
        "title": "Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Kelly Zhang",
            "Samuel Bowman"
        ],
        "references": [
            320149277,
            318741956,
            220355331,
            220017637,
            13853244,
            322695114
        ]
    },
    {
        "id": 318084717,
        "title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation",
        "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
        "date": "August 2017",
        "authors": [
            "Daniel Cer",
            "Mona Diab",
            "Eneko Agirre",
            "\u02dc Nigo Lopez-Gazpio"
        ],
        "references": [
            322795703,
            322795602,
            322794723,
            322793748,
            322795601,
            322794591,
            322794513,
            322794489,
            322794155,
            322794004
        ]
    },
    {
        "id": 221366753,
        "title": "The PASCAL recognising textual entailment challenge",
        "abstract": "This paper describes the PASCAL Net- work of Excellence Recognising Textual Entailment (RTE) Challenge benchmark 1. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (en- tailed) from the other. This application- independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, sug- gesting the generic relevance of the task.",
        "date": "January 2005",
        "authors": [
            "Ido Dagan",
            "Oren Glickman",
            "Bernardo Magnini"
        ],
        "references": [
            271439170,
            258232687,
            244447093,
            242132270,
            237320057,
            234819315,
            234811996,
            234784600,
            244469557,
            234803974
        ]
    },
    {
        "id": 325449053,
        "title": "Annotation Artifacts in Natural Language Inference Data",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Suchin Gururangan",
            "Swabha Swayamdipta",
            "Omer Levy",
            "Roy Schwartz"
        ],
        "references": [
            319700831,
            318740062,
            305342142,
            301404715,
            275897099,
            267861512,
            264003485,
            221366753,
            325449126,
            325447336
        ]
    },
    {
        "id": 325447291,
        "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
        "abstract": "",
        "date": "January 2018",
        "authors": [
            "Adina Williams",
            "Nikita Nangia",
            "Samuel Bowman"
        ],
        "references": [
            318741127,
            306094026,
            301586212,
            288713397,
            284576917,
            264003485,
            262484733,
            239061937,
            221618429,
            221366753
        ]
    },
    {
        "id": 320371042,
        "title": "DisSent: Sentence Representation Learning from Explicit Discourse Relations",
        "abstract": "Sentence vectors represent an appealing approach to meaning: learn an embedding that encompasses the meaning of a sentence in a single vector, that can be used for a variety of semantic tasks. Existing models for learning sentence embeddings either require extensive computational resources to train on large corpora, or are trained on costly, manually curated datasets of sentence relations. We observe that humans naturally annotate the relations between their sentences with discourse markers like \"but\" and \"because\". These words are deeply linked to the meanings of the sentences they connect. Using this natural signal, we automatically collect a classification dataset from unannotated text. Training a model to predict these discourse markers yields high quality sentence embeddings. Our model captures complementary information to existing models and achieves comparable generalization performance to state of the art models.",
        "date": "October 2017",
        "authors": [
            "Allen Nie",
            "Erin D. Bennett",
            "Noah D. Goodman"
        ],
        "references": [
            319478810,
            308121314,
            305334586,
            279068396,
            228943262,
            220746875,
            316779617,
            316618221,
            316235248,
            288429582
        ]
    },
    {
        "id": 316779617,
        "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference dataset can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.",
        "date": "May 2017",
        "authors": [
            "Alexis Conneau",
            "Douwe Kiela",
            "Holger Schwenk",
            "Lo\u00efc Barrault"
        ],
        "references": [
            311900760,
            305334586,
            304408934,
            301586212,
            284576917,
            279068396,
            266201822,
            262484733,
            261100864,
            257882504
        ]
    },
    {
        "id": 312250707,
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "abstract": "",
        "date": "January 2016",
        "authors": [
            "Pranav Rajpurkar",
            "Jian Zhang",
            "Konstantin Lopyrev",
            "Percy Liang"
        ],
        "references": [
            301404923,
            284423420,
            283659163,
            221361415,
            220874419,
            220605292,
            220017637,
            2819226,
            2568471,
            2557383
        ]
    },
    {
        "id": 247999003,
        "title": "The second PASCAL recognising textual entailment challenge",
        "abstract": "This paper describes the Second PASCAL Recognising Textual Entailment Chal-lenge (RTE-2)., We describe the RTE-2 dataset and overview the submissions for the challenge. One of the main goals for this year's dataset was to pro-vide more \"realistic\" text-hypothesis ex-amples, based mostly on outputs of ac-tual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year's state of the art.",
        "date": "January 2006",
        "authors": [
            "Roy Bar-Haim",
            "Ido Dagan",
            "Bill Dolan",
            "Lisa Ferro"
        ],
        "references": []
    },
    {
        "id": 221675867,
        "title": "Comparison of the Predicted and Observed Secondary Structure of T4 Phage Lysozime",
        "abstract": "Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both beta-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140-142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.",
        "date": "November 1974",
        "authors": [
            "B.W. Matthews"
        ],
        "references": []
    },
    {
        "id": 313434311,
        "title": "Beam Search Strategies for Neural Machine Translation",
        "abstract": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German-English and Chinese-English without losing any translation quality.",
        "date": "February 2017",
        "authors": [
            "Markus Freitag",
            "Yaser Al-Onaizan"
        ],
        "references": [
            308646556,
            306093632,
            306093359,
            267627635,
            265554383,
            200033861
        ]
    },
    {
        "id": 304470296,
        "title": "Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
        "abstract": "Recurrent neural networks, and in particular long short-term memory networks (LSTMs), are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows a user to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with domain specific structural annotations. We further show several use cases of the tool for analyzing specific hidden state properties on datasets containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis.",
        "date": "June 2016",
        "authors": [
            "Hendrik Strobelt",
            "Sebastian Gehrmann",
            "Bernd Huber",
            "Hanspeter Pfister"
        ],
        "references": [
            319770141,
            317350688,
            306049229,
            301931162,
            301843081,
            301839500,
            312607081,
            311648243,
            311469848,
            305334391
        ]
    },
    {
        "id": 287250272,
        "title": "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation",
        "abstract": "The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that joint training significantly improves both alignment and translation quality over independent training.",
        "date": "December 2015",
        "authors": [
            "Yong Cheng",
            "Shiqi Shen",
            "Zhongjun He",
            "Wei He"
        ],
        "references": [
            303968595,
            280741937,
            272194766,
            269280447,
            319770465,
            301445976,
            289758666,
            281145060,
            266676974,
            265554383
        ]
    },
    {
        "id": 311925796,
        "title": "Understanding Neural Networks through Representation Erasure",
        "abstract": "While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.",
        "date": "December 2016",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Dan Jurafsky"
        ],
        "references": [
            307560165,
            307174609,
            301404398,
            284576917,
            266201822,
            257882504,
            319770439,
            319769995,
            301445927,
            286794765
        ]
    },
    {
        "id": 309738548,
        "title": "Neural Machine Translation with Reconstruction",
        "abstract": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems.",
        "date": "November 2016",
        "authors": [
            "Zhaopeng Tu",
            "Yang Liu",
            "Lifeng Shang",
            "Xiaohua Liu"
        ],
        "references": [
            332669144,
            311990243,
            306226354,
            306093534,
            287250272,
            286513565,
            265252627,
            262877889,
            230875890,
            228102719
        ]
    },
    {
        "id": 291437483,
        "title": "Coverage-based Neural Machine Translation",
        "abstract": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.",
        "date": "January 2016",
        "authors": [
            "Zhaopeng Tu",
            "Zhengdong Lu",
            "Yang Liu",
            "Xiaohua Liu"
        ],
        "references": [
            287250272,
            272194766,
            269280447,
            265385879,
            265252627,
            262877889,
            230875890,
            220874793,
            220874004,
            13853244
        ]
    },
    {
        "id": 277959376,
        "title": "Visualizing and Understanding Recurrent Networks",
        "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.",
        "date": "June 2015",
        "authors": [
            "Andrej Karpathy",
            "Justin Johnson",
            "Fei-Fei Li"
        ],
        "references": [
            319770438,
            319770160,
            273640320,
            272423025,
            269416998,
            265385879,
            265252627,
            259399919,
            233730646,
            228339739
        ]
    },
    {
        "id": 224377747,
        "title": "Decision Making in Context",
        "abstract": "From a Bayesian decision theoretic framework, we show that the reason why the usual statistical approaches do not take context into account is because of the assumptions made on the joint prior probability function and because of the simplistic loss function chosen. We illustrate how the constraints sometimes employed by artificial intelligence researchers constitute a different kind of assumption on the joint prior probability function. We discuss a couple of loss functions which do take context into account and when combined with the joint prior probability constraint create a decision problem requiring a combinatorial state space search. We also give a theory for how probabilistic relaxation works from a Bayesian point of view.",
        "date": "August 1983",
        "authors": [
            "Robert Martin Haralick"
        ],
        "references": [
            284223303,
            264908422,
            263047996,
            256404099,
            256121824,
            243782891,
            239658944,
            239284419,
            236157558,
            235890211
        ]
    },
    {
        "id": 228339739,
        "title": "Viualizing data using t-SNE",
        "abstract": "We present a new technique called \"t-SNE\" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.",
        "date": "November 2008",
        "authors": [
            "Laurens van der Maaten",
            "Geoffrey Hinton"
        ],
        "references": [
            326910915,
            313984423,
            313565837,
            313423296,
            292793744,
            284697832,
            284035345,
            267117901,
            256821877,
            239065741
        ]
    },
    {
        "id": 51140976,
        "title": "Insensitivity of the Human Sentence-Processing System to Hierarchical Structure",
        "abstract": "Although it is generally accepted that hierarchical phrase structures are instrumental in describing human language, their role in cognitive processing is still debated. We investigated the role of hierarchical structure in sentence processing by implementing a range of probabilistic language models, some of which depended on hierarchical structure, and others of which relied on sequential structure only. All models estimated the occurrence probabilities of syntactic categories in sentences for which reading-time data were available. Relating the models' probability estimates to the data showed that the hierarchical-structure models did not account for variance in reading times over and above the amount of variance accounted for by all of the sequential-structure models. This suggests that a sentence's hierarchical structure, unlike many other sources of information, does not noticeably affect the generation of expectations about upcoming words.",
        "date": "June 2011",
        "authors": [
            "Stefan L Frank",
            "Rens Bod"
        ],
        "references": [
            316160803,
            247807796,
            341690965,
            312607081,
            280113549,
            272087448,
            271894730,
            263329889,
            263242011,
            257829035
        ]
    },
    {
        "id": 15421059,
        "title": "Integration of Visual and Linguistic Information in Spoken Language Comprehension",
        "abstract": "Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.",
        "date": "July 1995",
        "authors": [
            "Michael K Tanenhaus",
            "Michael Spivey",
            "Kathleen Eberhard",
            "Julie Sedivy"
        ],
        "references": [
            259703463,
            239549827,
            237386381,
            21297967,
            242430108,
            232540389,
            222166556,
            20589133,
            19941301,
            19872245
        ]
    },
    {
        "id": 281090375,
        "title": "Complexity of SAT Problems, Clone Theory and the Exponential Time Hypothesis",
        "abstract": "The construction of exact exponential-time algorithms for NP-complete problems has for some time been a very active research area. Unfortunately, there is a lack of general methods for studying and comparing the time complexity of algorithms for such problems. We propose such a method based on clone theory and demonstrate it on the SAT problem, Schaefer has completely classified the complexity of SAT with respect to the set of allowed relations and proved that this parameterized problem exhibits a dichotomy: it is either in P or is NP-complete. We show that there is a certain partial order on the NP-complete SAT problems with a close connection to their worst-case time complexities; if a problem SAT(S) is below a problem SAT(S\u2032) in this partial order, then SAT(S\u2032) cannot be solved strictly faster than SAT(S). By using this order, we identify a relation R such that SAT({R}) is the computationally easiest NP-complete SAT(S) problem. This result may be interesting when investigating the borderline between P and NP since one appealing way of studying this borderline is to identify problems that, in some sense, are situated close to it (such as a 'very hard' problem in P or a 'very easy' NP-complete problem). We strengthen the result by showing that SAT({R})-2 (i.e. SAT({R}) restricted to instances where no variable appears more than twice) is NP-complete, too. This is in contrast to, for example, 1-in-3-SAT (or even CNF-SAT), which is in P under the same restriction. We then relate SAT({R})-2 to the exponential-time hypothesis (ETH) and show that ETH holds if and only if SAT({R})-2 is not sub-exponential. This constitutes a strong connection between ETH and the SAT problem under both severe relational and severe structural restrictions, and it may thus serve as a tool for studying the borderline between subexponential and exponential problems. In the process, we also prove a stronger version of Impagliazzo et al.'s sparsification lemma for k-SAT; namely that all finite Boolean constraint languages S and S\u2032 such that SAT(\u00b7) is NP-complete can be sparsified into each other. This should be compared with Santhanam and Srinivasan's recent negative result which states that the same does not hold for all infinite Boolean constraint languages.",
        "date": "January 2013",
        "authors": [
            "Peter Jonsson",
            "Victor Lagerkvist",
            "Gustav Nordh",
            "Bruno Zanuttini"
        ],
        "references": [
            242206661,
            303533432,
            299229870,
            268072213,
            257428713,
            244421153,
            239338339,
            239338215,
            235625105,
            224483578
        ]
    },
    {
        "id": 258818741,
        "title": "The Traveling Salesman Problem for Lines, Balls and Planes",
        "abstract": "We revisit the traveling salesman problem with neighborhoods (TSPN) and propose several new approximation algorithms. These constitute either first approximations (for hyperplanes, lines, and balls in Rd, for d &ges; 3) or improvements over previous approximations achievable in comparable times (for unit disks in the plane). (I) Given a set of n hyperplanes in Rd, a traveling salesman problem (TSP) tour whose length is at most O(1) times the optimal can be computed in O(n) time when d is constant. (II) Given a set of n lines in Rd, a TSP tour whose length is at most O(log \u00b3n) times the optimal can be computed in polynomial time for all d. (III) Given a set of n unit balls in Rd, a TSP tour whose length is at most O(1) times the optimal can be computed in polynomial time when d is constant.",
        "date": "March 2013",
        "authors": [
            "Adrian Dumitrescu",
            "Csaba D. T\u00f3th"
        ],
        "references": [
            265806707,
            262381174,
            259044510,
            267147479,
            265669370,
            246317636,
            243133650,
            242637556,
            242618679,
            238673934
        ]
    },
    {
        "id": 221102165,
        "title": "Random Restarts in Minimum Error Rate Training for Statistical Machine Translation.",
        "abstract": "Och's (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time.",
        "date": "January 2008",
        "authors": [
            "Robert C. Moore",
            "Chris Quirk"
        ],
        "references": [
            234832376,
            221013020,
            220816913,
            246868205,
            236151467,
            31403274,
            2884718
        ]
    },
    {
        "id": 221013126,
        "title": "Dual Decomposition with Many Overlapping Components.",
        "abstract": "Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.",
        "date": "January 2011",
        "authors": [
            "Andr\u00e9 F. T. Martins",
            "Noah A. Smith",
            "M\u00e1rio A T Figueiredo",
            "Pedro M Q Aguiar"
        ],
        "references": [
            237533325,
            303200558,
            288905957,
            268685107,
            260365821,
            245759142,
            242922655,
            238752725,
            234829084,
            234797524
        ]
    },
    {
        "id": 221012876,
        "title": "Incremental Dependency Parsing Using Online Learning.",
        "abstract": "",
        "date": "January 2007",
        "authors": [
            "Richard Johansson",
            "Pierre Nugues"
        ],
        "references": [
            242375525,
            237533325,
            228774597,
            228708442,
            251347641,
            249903651,
            238319783,
            234829084,
            234828464,
            228892601
        ]
    },
    {
        "id": 220816925,
        "title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing.",
        "abstract": "We present a novel deterministic dependency pars- ing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, per- form one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algo- rithm builds a dependency tree by iteratively select- ing the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a determin- istic, best-first, O(nlogn) parser, which is signifi- cantly more accurate than best-first transition based parsers, and nears the performance of globally opti- mized parsing models.",
        "date": "January 2010",
        "authors": [
            "Yoav Goldberg",
            "Michael Elhadad"
        ],
        "references": [
            237533325,
            234814829,
            234804753,
            228916842,
            228882434,
            221013171,
            228644575,
            228524072,
            221013303,
            221013109
        ]
    },
    {
        "id": 311708648,
        "title": "An Introduction to Probability Theory and Its Applications, Volume I",
        "abstract": "",
        "date": "January 1958",
        "authors": [
            "Boyd Harshbarger",
            "William Feller"
        ],
        "references": []
    },
    {
        "id": 284145315,
        "title": "Maximum likelihood from incomplete data from incomplete",
        "abstract": "",
        "date": "January 1977",
        "authors": [
            "A.P. Dempster"
        ],
        "references": []
    },
    {
        "id": 239059509,
        "title": "Computer Analysis of Present-Day American English",
        "abstract": "",
        "date": "January 1967",
        "authors": [
            "S Francis",
            "H Kucera"
        ],
        "references": []
    },
    {
        "id": 232128580,
        "title": "On the Population Frequency of Species and the Estimation of Population Parameters",
        "abstract": "A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N , then r &sol; N is not a good estimate of the population frequency, p , when r is small. Methods are given for estimating p , assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers n r ( r &equals; 1, 2, 3, ...), where n r is the number of distinct species that are each represented r times in the sample. ( n r may be described as \u2018the frequency of the frequency r \u2019.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's \u2018characteristic\u2019 and Shannon's \u2018entropy\u2019. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers n r but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.",
        "date": "December 1953",
        "authors": [
            "I. J. Good"
        ],
        "references": [
            285709032,
            285188902,
            268739367,
            238861835,
            235240901,
            232128972,
            216850117,
            216849882,
            51993442,
            38367690
        ]
    },
    {
        "id": 222479869,
        "title": "Context Based Spelling Correction",
        "abstract": "Some mistakes in spelling and typing produce correct words, such as typing \u201cfig\u201d when \u201cfog\u201d was intended. These errors are undetectable by traditional spelling correction techniques. In this paper we present a statistical technique capable of detecting and correcting some of these errors when they occur in sentences. Experimental results show that this technique is capable of detecting 76% of simple spelling errors and correcting 73%.",
        "date": "January 1990",
        "authors": [
            "Eric Mays",
            "Fred J. Damerau",
            "Robert Mercer"
        ],
        "references": [
            224377724,
            222241903,
            220427626,
            220424167,
            220229413
        ]
    },
    {
        "id": 220049264,
        "title": "An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes",
        "abstract": "",
        "date": "November 1971",
        "authors": [
            "LE Baum"
        ],
        "references": []
    },
    {
        "id": 243645162,
        "title": "Estimation and Annealing for Gibbsian Fields",
        "abstract": "",
        "date": "January 1988",
        "authors": [
            "Laurent Younes"
        ],
        "references": []
    },
    {
        "id": 310606656,
        "title": "Classification and regression trees",
        "abstract": "",
        "date": "January 1984",
        "authors": [
            "L Breiman"
        ],
        "references": []
    },
    {
        "id": 265978139,
        "title": "Information geometry and alternating minimization procedures",
        "abstract": "",
        "date": "January 1984",
        "authors": [
            "Imre Csiszar",
            "G\u00e1bor Tusn\u00e1dy"
        ],
        "references": []
    },
    {
        "id": 240310918,
        "title": "Classification and Regression Trees (CART)",
        "abstract": "",
        "date": "September 1984",
        "authors": [
            "Breiman LI",
            "Jerome H. Friedman",
            "RA Olshen",
            "C.J. Stone"
        ],
        "references": []
    },
    {
        "id": 235961479,
        "title": "Classification and Regression Trees",
        "abstract": "",
        "date": "January 1984",
        "authors": [
            "Leo Breiman",
            "Jerome H. Friedman",
            "Richard A. Olshen",
            "Charles J. Stone"
        ],
        "references": [
            268636561,
            50336265,
            49308789,
            44722827,
            44432461
        ]
    },
    {
        "id": 223013541,
        "title": "An iterative Gibbsian technique for reconstruction of m-ary images",
        "abstract": "The reconstruction of m-ary images corrupted by independent noise is treated. The original image is modeled by a Markov Random Field (MRF) whose parameters are unknown. Likewise, the probabilistic structure of the noise is unknown. This paper presents an iterative procedure which performs the parameter estimation and image reconstruction tasks at the same time. The procedure that we call Gibbsian EM algorithm, is a generalization to the MRF context of a general algorithm, known as the EM algorithm, used to approximate maximum-likelihood estimates for incomplete data problems. A number of experiments are presented in the case of Gaussian noise and binary noise, showing that the Gibbsian EM algorithm is useful and effective for image reconstruction and segmentation.",
        "date": "December 1989",
        "authors": [
            "Bernard Chalmond"
        ],
        "references": [
            227121666,
            37597291,
            3191690,
            291407018,
            262215843,
            247403884,
            246098309,
            244958264,
            242706958,
            237128102
        ]
    },
    {
        "id": 232633438,
        "title": "A tree-trellis based fast search for finding the N Best sentence hypotheses in continuous speech recognition",
        "abstract": "A novel tree-trellis based fast search for finding the N-best sentence hypotheses in continuous speech recognition is presented. The search consists of a forward time-synchronous trellis search and a backward time-asynchronous tree search. The Viterbi algorithm is used for recording the scores of all partial paths in a trellis time synchronously. Then a backward A* algorithm based tree search is used to extend partial paths time asynchronously. Extended partial paths in the backward tree search are rank ordered in a stack by their corresponding best possible scores of the remaining paths which are prerecorded in the forward trellis path map. In each path growing cycle, the current best partial path, which is at the top of the stack, is extended by the best possible one arc (word) extension. The tree-trellis search is different from the traditional time synchronous Viterbi search in its ability to find not just the best but the N best paths of different word content.",
        "date": "April 1991",
        "authors": [
            "Frank K. Soong",
            "Eng-Fong Huang"
        ],
        "references": [
            244459809,
            234788343,
            234772315,
            220049390,
            249632700,
            238160337,
            234803144,
            234793455,
            230876248,
            221491931
        ]
    },
    {
        "id": 270917716,
        "title": "Information Theory and Coding.",
        "abstract": "",
        "date": "January 1964",
        "authors": [
            "I. J. Good",
            "Norman Abramson"
        ],
        "references": []
    },
    {
        "id": 235418978,
        "title": "A Mathematical Theory of Communication",
        "abstract": "Bell System Technical Journal, also pp. 623-656 (October)",
        "date": "July 1948",
        "authors": [
            "Claude Elwood Shannon"
        ],
        "references": []
    },
    {
        "id": 230876473,
        "title": "Prediction and Entropy of Printed English",
        "abstract": "A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed.",
        "date": "January 1951",
        "authors": [
            "C. E. Shannon"
        ],
        "references": []
    },
    {
        "id": 230876022,
        "title": "Frequency Analysis of English Usage",
        "abstract": "",
        "date": "January 1982",
        "authors": [
            "Nelson W Francis",
            "Henry Kuvcera"
        ],
        "references": []
    },
    {
        "id": 50336131,
        "title": "Information Theory And Statistics",
        "abstract": "",
        "date": "March 1960",
        "authors": [
            "Solomon. Kullback"
        ],
        "references": [
            38366795
        ]
    },
    {
        "id": 38359320,
        "title": "A Geometric Interpretation of Darroch and Ratcliff's Generalized Iterative Scaling",
        "abstract": "Darroch and Ratcliff's iterative algorithm for minimizing $I$-divergence subject to linear constraints is equivalent to a cyclic iteration of explicitly performable $I$-projection operations.",
        "date": "September 1989",
        "authors": [
            "Imre Csiszar"
        ],
        "references": [
            38363908,
            38362523
        ]
    },
    {
        "id": 240429666,
        "title": "La erty: Inducing Features of Random Fields",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "Stephen Andrew Della Pietra",
            "Vincent joseph Dellapietra",
            "D John"
        ],
        "references": []
    },
    {
        "id": 234356583,
        "title": "Statistical Mechanics, A Set of Lectures",
        "abstract": "Introduction to statistical mechanics density matrices path integrals classical system of N particles order disorder theory creation and annihilation operators spin waves polaron problem electron gas in metal superconductivity superfluidity.",
        "date": "June 1974",
        "authors": [
            "R. P. Feynman"
        ],
        "references": []
    },
    {
        "id": 220811987,
        "title": "A Method of Computing Generalized Bayesian Probability Values for Expert Systems.",
        "abstract": "This paper presents a new method for calculating the conditional probability of any multi-valued predicate given particular information about the individual case. This calculation is based on the principle of Maximum Entropy (ME), sometimes called the principle of least information, and gives the most unbiased probability estimate given the available evidence. Previous methods for computing maximum entropy values shows that they are either very restrictive in the probabilistic information (constraints) they can use or combinatorially explosive. The computational complexity of the new procedure depends on the inter-connectedness of the constraints, but in practical cases it is small. In addition, the maximum entropy method can give a measure of how accurately a calculated conditional probability is known.",
        "date": "January 1983",
        "authors": [
            "Peter C Cheeseman"
        ],
        "references": [
            268636561,
            244447516,
            242617446,
            225070185,
            221605612,
            221395177,
            220247016,
            220246214,
            17525075,
            3479364
        ]
    },
    {
        "id": 3532137,
        "title": "SWITCHBOARD: telephone speech corpus for research and development",
        "abstract": "SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording",
        "date": "April 1992",
        "authors": [
            "John Godfrey",
            "E.C. Holliman",
            "J. McDaniel"
        ],
        "references": [
            208033632,
            242572658,
            232653286,
            224657044
        ]
    },
    {
        "id": 223255907,
        "title": "Adaptive acquisition of language",
        "abstract": "At present, automatic speech recognition technology is based upon constructing models of the various levels of linguistic structure assumed to compose spoken language. These models are either constructed manually or automatically trained by example. A major impediment is the cost, or even the feasibility, of producing models of sufficient fidelity to enable the desired level of performance.The proposed alternative is to build a device capable of acquiring the necessary linguistic skills in the course of performing its task. We call this learning by doing, and contrast it with learning by example. The purpose of this paper is to describe some basic principles and mechanisms upon which such a device might be based, and to recount a rudimentary experiment evaluating their utility.Spoken language, the original natural language, evolved in order for humans to convey importnat messages to each other. A first principle, then, is that the primary function of language is to communicate. A consequence of this principle is that language acquisition involves gaining the capability of decoding the message. This is in contrast to much of the research on automated language acquisition, which focuses on discovering syntactic structure, often specifically to the exclusion of meaning. Our first principle leads us to investigate a language acquisition mechanism based on connectionist methods, in which the network builds associations between messages and meaningful responses to them.People learn while performing a task by receiving feedback as to the appropriateness of their actions. A second principle, then, is that the actual construction of the mapping from messages to meaning should be governed by a feedback control system where the error signal is at the level of meaning. This is in contrast to some learning research, which is governed by providing input/output pairs, and where the error signal is a parameter-space distortion measure. Our second principle leads us to investigate a mechanism for human-machine interaction based on control-theory methods, where the system input is the message and the error signal is a measure of appropriateness of the machine's response.The utility of these principles is demonstrated and evaluated by applying them to an elementary inward-call-management task, the object of which is to connect a caller to the department of a large organization appropriate to his inquiry. Initially, the system knows nothing about the language for its task, that is no vocabulary, no grammer, and no semantic associations. In the course of directing incoming calls, the system acquires a vocabulary, learns the meaning of words and some rudimentary grammatical relationships relevant to its task. The mechanism used is a particular connectionist network embedded in a feedback control system which adjusts the connection weights of the network based on the success or failure of the machine's behavior, as evaluated by the caller's reaction to it. This mechanism has several intriguing mathematical properties.An experimental evaluation of the system has been conducted using typed rather than spoken input. The system was tested by 12 subjects over a 2-month period. Over 1000 conversations were held, during which the machine acquired a vocabulary of over 1500 words. Subsequent tests showed that the learning was stable, in that it retained 99% of the knowledge it had acquired in the interactions.Although the experiments conducted thus far are of a rudimentary nature, we consider them to be the early stages in a long-term study of automatic acquisition of intelligence by machines through interaction with a complex environment.",
        "date": "April 1991",
        "authors": [
            "Allen L. Gorin",
            "S.E. Levinson",
            "A.N. Gertner",
            "E.R. Goldman"
        ],
        "references": [
            260593130,
            247954418,
            234792985,
            234772315,
            19685105,
            3548300,
            313423098,
            311476487,
            297071700,
            291177922
        ]
    },
    {
        "id": 3178307,
        "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
        "abstract": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.",
        "date": "April 1987",
        "authors": [
            "Slava M. Katz"
        ],
        "references": [
            313201001,
            243784835,
            232128580,
            230876151,
            224737600,
            224737138,
            3177932,
            3177586
        ]
    },
    {
        "id": 3177932,
        "title": "On Turing's formula for word probabilities",
        "abstract": "A. M. Turing, in a 1941 personal communication to I. J. Good, suggested a formula for estimating probabilities of words in text and, more generally, of species in a mixed population of various species. It is remarkable that Turing's formula can be obtained by significantly different statistical methods; we compare three ways to obtain it.",
        "date": "January 1986",
        "authors": [
            "Arthur Nadas"
        ],
        "references": [
            313201001,
            243523993,
            232128580,
            230876151,
            3177586
        ]
    },
    {
        "id": 3177586,
        "title": "Estimation of probabilities in the language model of the IBM speech recognition system",
        "abstract": "The language model probabilities are estimated by an empirical Bayes approach in which a prior distribution for the unknown probabilities is itself estimated through a novel choice of data. The predictive power of the model thus fitted is compared by means of its experimental perplexity [1] to the model as fitted by the Jelinek-Mercer deleted estimator and as fitted by the Turing-Good formulas for probabilities of unseen or rarely seen events.",
        "date": "September 1984",
        "authors": [
            "Arthur Nadas"
        ],
        "references": [
            224377724,
            329476310,
            314733544,
            313201001,
            243082410,
            230876473,
            38365997,
            37796538,
            3177391,
            3177286
        ]
    },
    {
        "id": 3175558,
        "title": "A tree-based statistical language model for natural language speech recognition. IEEE Trans, on ASSP, ASSP-37(7):1001-1008.S",
        "abstract": "The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior",
        "date": "August 1989",
        "authors": [
            "Lalit R. Bahl",
            "Peter F. Brown",
            "Peter V. De Souza",
            "Robert Mercer"
        ],
        "references": [
            224377724,
            220679754,
            6026283,
            250754760,
            235961479,
            230876473,
            224739128,
            220683391,
            220045421,
            3083970
        ]
    },
    {
        "id": 271775547,
        "title": "Full Contingency Tables, Logits, and Split Contingency Tables",
        "abstract": "Three methods of fitting log-linear models to multivariate contingency-table data with one dichotomous variable are discussed. Logit analysis is commonly used when a full contingency table of s dimensions is regarded as a table of rates of dimension s - 1. The split-table method treats the same data as two separate tables each of dimension s - 1. We show that the full contingency-table method can be regarded as a generalized approach: models which can be fitted by it include both the mutually exclusive subsets that can be fitted by the other two methods. Even when the logit method permits the model of choice to be fitted, the full contingency-table method of iterative proportional fitting to the set of sufficient configurations has the advantage of requiring neither matrix inversion nor substitution of an arbitrary value in empty elementary cells.",
        "date": "June 1969",
        "authors": [
            "Yvonne M. M. Bishop"
        ],
        "references": []
    },
    {
        "id": 243081893,
        "title": "Maximum Likelihood Estimation for Generalized Power Series Distributions and Its Application to a Truncated Binomial Distribution",
        "abstract": "",
        "date": "June 1962",
        "authors": [
            "G. P. Patil"
        ],
        "references": [
            270240781,
            243081856,
            229856171,
            229756136,
            47591783,
            38367672,
            31302961,
            30962693,
            5717448
        ]
    },
    {
        "id": 220247016,
        "title": "A Note on Approximations to Discrete Probability Distributions",
        "abstract": "An iterative method is presented which gives an optimum approximationto the joint probability distribution of a set of binary variables given the joint probability distributions of any subsets of the variables (any set of component distributions). The most significant feature of this approximation procedure is that there is no limitation to the number or type of component distributions that can be employed. Each step of the iteration gives an improved approximation, and the procedure converges to give an approximation that is the minimum information (i.e. maximum entropy) extension of the component distributions employed.",
        "date": "December 1959",
        "authors": [
            "David T. Brown"
        ],
        "references": [
            220246214
        ]
    },
    {
        "id": 38366114,
        "title": "Maximum Entropy for Hypothesis Formulation Especially for Multi-Dimensional Contingency Tables",
        "abstract": "The principle of maximum entropy, together with some generalizations, is interpreted as a heuristic principle for the generation of null hypotheses. The main application is to $m$-dimensional population contingency tables, with the marginal totals given down to dimension $m - r$ (\"restraints of the $r$th order\"). The principle then leads to the null hypothesis of no \"$r$th-order interaction.\" Significance tests are given for testing the hypothesis of no $r$th-order or higher-order interaction within the wider hypothesis of no $s$th-order or higher-order interaction, some cases of which have been treated by Bartlett and by Roy and Kastenbaum. It is shown that, if a complete set of $r$th-order restraints are given, then the hypothesis of the vanishing of all $r$th-order and higher-order interactions leads to a unique set of cell probabilities, if the restraints are consistent, but not only just consistent. This confirms and generalizes a recent conjecture due to Darroch. A kind of duality between maximum entropy and maximum likelihood is proved. Some relationships between maximum entropy, interactions, and Markov chains are proved.",
        "date": "September 1963",
        "authors": [
            "I. J. Good"
        ],
        "references": []
    },
    {
        "id": 38364508,
        "title": "Equivalence of Gauss's Principle and Minimum Discrimination Information Estimation of Probabilities",
        "abstract": "",
        "date": "June 1970",
        "authors": [
            "L. L. Campbell"
        ],
        "references": []
    },
    {
        "id": 38364499,
        "title": "An Iterative Procedure for Estimation in Contingency Tables",
        "abstract": "Deming and Stephan (1940) first proposed the use of an iterative proportional fitting procedure to estimate cell probabilities in a contingency table subject to certain marginal constraints. In this paper we first relate this procedure to a variety of sources and a variety of statistical problems. We then describe the procedure geometrically for two-way contingency tables using the concepts presented in Fienberg (1968). This geometrical description leads to a rather simple proof of the convergence of the iterative procedure. We conclude the paper with a discussion of extensions to multi-dimensional tables and to tables with some zero entries.",
        "date": "June 1970",
        "authors": [
            "Stephen E. Fienberg"
        ],
        "references": []
    },
    {
        "id": 18051412,
        "title": "An Iterative Procedure for Analysing Log-Linear Models",
        "abstract": "For a class of log-linear models in multinomial experiments, an iterative procedure is proposed for obtaining maximum likelihood estimates of cell frequencies. A condition is given under which the above estimates also conform to restrictions on the cell frequencies imposed by the sampling scheme. The iterative procedure is applied to a three-way contingency table for obtaining estimates for cells under a hypothesis about some interaction parameters.",
        "date": "October 1971",
        "authors": [
            "D V Gokhale"
        ],
        "references": []
    },
    {
        "id": 221484301,
        "title": "Assessment of smoothing methods and complex stochastic language modeling.",
        "abstract": "This paper studies the overall effect of language modeling on perplexity and word error rate, starting from a trigram model with a standard smoothing method up to complex state--of--the-- art language models: (1) We compare different smoothing methods, namely linear vs. absolute discounting, interpolation vs. backing-off, and back-off functions based on relative frequencies vs. singleton events. (2) We show the effect of complex language model techniques by using distant-trigrams and automatically selected word classes and word phrases using a maximum likelihood criterion (i.e. minimum perplexity). (3) We show the overall gain of the combined application of the above techniques, as opposed to their separate assessment in past publications. (4) We give perplexity and word error rate results on the North American Business corpus (NAB) with a training text of about 240 million words and on the German Verbmobil corpus. 1. INTRODUCTION: LANGUAGE MODELING In this paper, a statistical speec...",
        "date": "January 1999",
        "authors": [
            "Sven C. Martin",
            "Christoph Hamacher",
            "J\u00f6rg Liermann",
            "Frank Wessel"
        ],
        "references": [
            224377724,
            221481293,
            243660797,
            242821088,
            230876357,
            230876148,
            230801973,
            222496856,
            221491919,
            221491341
        ]
    },
    {
        "id": 3703273,
        "title": "Modeling long distance dependence in language: Topic mixtures vs. Dynamic cache models",
        "abstract": "We investigate a new statistical language model which captures topic-related dependencies of words within and across sentences. First, we develop a sentence-level mixture language model that takes advantage of the topic constraints in a sentence or article. Second, we introduce topic-dependent dynamic cache adaptation techniques in the framework of the mixture model. Experiments with the static (or unadapted) mixture model on the 1994 WSJ task indicated a 21% reduction in perplexity and a 3-4% improvement in recognition accuracy over a general n-gram model. The static mixture model also improved recognition performance over an adapted n-gram model. Mixture adaptation techniques contributed a further 14% reduction in perplexity and a small improvement in recognition accuracy",
        "date": "November 1996",
        "authors": [
            "Rukmini Iyer",
            "M. Ostendorf"
        ],
        "references": [
            220817083,
            3618382,
            255563820,
            243773542,
            232641305,
            232623486,
            232621975,
            220817414,
            220816999,
            220182703
        ]
    },
    {
        "id": 2416307,
        "title": "Speech Recognition And The Frequency Of Recently Used Words: A Modified Markov Model For Natural Language",
        "abstract": "Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary. A class of Markov language models identified by Jclinck has achieved considerable success in this domain. A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model. Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English.",
        "date": "January 2003",
        "authors": [
            "Roland Kuhn"
        ],
        "references": [
            224377724,
            246769833,
            242357975,
            238697611,
            235890146,
            230876286,
            230876148,
            230876023,
            224377989,
            3177586
        ]
    },
    {
        "id": 239059370,
        "title": "Correction to: A cache-based natural language model for speech re-production",
        "abstract": "",
        "date": "January 1992",
        "authors": [
            "Roland Kuhn",
            "Renato De Mori"
        ],
        "references": []
    },
    {
        "id": 221102572,
        "title": "Speech recongnition and the frequency of recently used words: a modified Markov model for natural language.",
        "abstract": "",
        "date": "January 1988",
        "authors": [
            "Roland Kuhn"
        ],
        "references": [
            224377724,
            246769833,
            235890146,
            230876286,
            230876023,
            3177586,
            2997331
        ]
    },
    {
        "id": 230876148,
        "title": "Self-organized language modeling for speech recognition",
        "abstract": "",
        "date": "December 1990",
        "authors": [
            "Frederick Jelinek"
        ],
        "references": [
            3083078
        ]
    },
    {
        "id": 3793872,
        "title": "Multi-Class Composite N-gram based on connection direction",
        "abstract": "A new word-clustering technique is proposed to efficiently build statistically salient class 2-grams from language corpora. By splitting word neighboring characteristics into word-preceding and following directions, multiple (two-dimensional) word classes are assigned to each word, In each side, word classes are merged into larger clusters independently according to preceding or following word distributions. This word-clustering can provide more efficient and statistically reliable word clusters. Further, we extend it to a multi-class composite N-gram that unit is a multi-class 2-gram and joined word. The multi-class composite N-gram showed better performance both in perplexity and recognition rates with one thousandth smaller size than conventional word 2-grams",
        "date": "April 1999",
        "authors": [
            "Hirofumi Yamamoto",
            "Yoshinori Sagisaka"
        ],
        "references": [
            220355244,
            3746755,
            243768166,
            232638949,
            232634241,
            3175558,
            2832618
        ]
    },
    {
        "id": 3703274,
        "title": "Scalable backoff language models",
        "abstract": "When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone",
        "date": "November 1996",
        "authors": [
            "K. Seymore",
            "R. Rosenfeld"
        ],
        "references": [
            3178307,
            2598358,
            2272094
        ]
    },
    {
        "id": 2348437,
        "title": "Entropy-based Pruning of Backoff Language Models",
        "abstract": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld [9], and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally bette...",
        "date": "August 2000",
        "authors": [
            "Andreas Stolcke"
        ],
        "references": [
            2327932,
            234776814,
            232128580,
            224773133,
            221480805,
            3703351,
            3703274,
            3178307,
            2779262,
            2620143
        ]
    },
    {
        "id": 38359595,
        "title": "Why Least Squares and Maximum Entropy? An Axiomatic Approach to Inference for Linear Inverse Problems",
        "abstract": "An attempt is made to determine the logically consistent rules for selecting a vector from any feasible set defined by linear constraints, when either all $n$-vectors or those with positive components or the probability vectors are permissible. Some basic postulates are satisfied if and only if the selection rule is to minimize a certain function which, if a \"prior guess\" is available, is a measure of distance from the prior guess. Two further natural postulates restrict the permissible distances to the author's $f$-divergences and Bregman's divergences, respectively. As corollaries, axiomatic characterizations of the methods of least squares and minimum discrimination information are arrived at. Alternatively, the latter are also characterized by a postulate of composition consistency. As a special case, a derivation of the method of maximum entropy from a small set of natural axioms is obtained.",
        "date": "December 1991",
        "authors": [
            "Imre Csiszar"
        ],
        "references": [
            265333961,
            38361537,
            3083558,
            3077514,
            242912961,
            225864379,
            222812623,
            222785720,
            222636658,
            2997753
        ]
    },
    {
        "id": 3908396,
        "title": "Portability of syntactic structure for language modeling",
        "abstract": "Presents a study on the portability of statistical syntactic knowledge in the framework of the structured language model (SLM). We investigate the impact of porting SLM statistics from the Wall Street Journal (WSJ) to the Air Travel Information System (ATIS) domain. We compare this approach to applying the Microsoft rule-based parser (NLP-win) for the ATIS data and to using a small amount of data manually parsed at UPenn for gathering the initial SLM statistics. Surprisingly, despite the fact that it performs modestly in perplexity (PPL), the model initialized on WSJ parses outperforms the other initialization methods based on in-domain annotated data, achieving a significant 0.4% absolute and 7% relative reduction in word error rate (WER) over a baseline system whose word error rate is 5.8%; the improvement measured relative to the minimum WER achievable on the N-best lists we worked with is 12%",
        "date": "February 2001",
        "authors": [
            "Ciprian Chelba"
        ],
        "references": [
            312607081,
            243740434,
            240053209,
            230876742,
            222800591,
            221995817,
            215721461,
            2547815
        ]
    },
    {
        "id": 232643301,
        "title": "Maximum entropy language model integrating N-grams and topic dependencies for conversational speech recognition",
        "abstract": "A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topic-independent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints.",
        "date": "March 1999",
        "authors": [
            "Sanjeev Khudanpur",
            "Jun Wu"
        ],
        "references": [
            221486443,
            221482940,
            38359595,
            3746909,
            3693462,
            3567691,
            2361059,
            238270360,
            38363908,
            3746915
        ]
    },
    {
        "id": 301446330,
        "title": "Enhancing the Inside-Outside Recursive Neural Network Reranker for Dependency Parsing",
        "abstract": "",
        "date": "January 2015",
        "authors": [
            "Phong Le"
        ],
        "references": [
            289176638,
            270878122,
            266201822,
            262217466,
            257882504,
            329974423,
            299487682,
            285432248,
            270878821,
            270878508
        ]
    },
    {
        "id": 270877660,
        "title": "Max-Margin Tensor Neural Network for Chinese Word Segmentation",
        "abstract": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensorbased transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks.",
        "date": "June 2014",
        "authors": [
            "Wenzhe Pei",
            "Tao ge",
            "Baobao Chang"
        ],
        "references": [
            287780998,
            273695736,
            266201822,
            265594447,
            262272503,
            259124658,
            234131319,
            284039049,
            278801374,
            270878508
        ]
    },
    {
        "id": 266376264,
        "title": "Feature Embedding for Dependency Parsing",
        "abstract": "In this paper, we propose an approach to automatically learning feature embeddings to address the feature sparseness problem for dependency parsing. Inspired by word embeddings, feature embeddings are distributed representations of features that are learned from large amounts of auto-parsed data. Our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline.",
        "date": "August 2014",
        "authors": [
            "Wenliang Chen",
            "Yue Zhang",
            "Min Zhang"
        ],
        "references": [
            273695736,
            266376367,
            266201822,
            262402839,
            257882504,
            319770439,
            288676407,
            270878508,
            270878494,
            262355038
        ]
    },
    {
        "id": 286964721,
        "title": "Deep recursive neural networks for compositionality in language",
        "abstract": "Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture - a deep recursive neural network (deep RNN) - constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.",
        "date": "January 2014",
        "authors": [
            "O. Irsoy",
            "Claire Cardie"
        ],
        "references": [
            215616967,
            3652002,
            288619064,
            220500234,
            2818396
        ]
    },
    {
        "id": 319395280,
        "title": "The pascal recognising textual entailment challenge",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "Ido Dagan",
            "Oren Glickman",
            "Bernardo Magnini"
        ],
        "references": [
            221012576
        ]
    },
    {
        "id": 301409030,
        "title": "haLF: Comparing a Pure CDSM Approach with a Standard Machine Learning System for RTE",
        "abstract": "In this paper, we describe our submission to the Shared Task #1. We tried to follow the underlying idea of the task, that is, evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods.",
        "date": "January 2014",
        "authors": [
            "Lorenzo Ferrone",
            "Fabio Massimo Zanzotto"
        ],
        "references": [
            227716404,
            221366753,
            221346102,
            221012893,
            270878419,
            270388843,
            247999003,
            234820797,
            234803974,
            228715647
        ]
    },
    {
        "id": 301408886,
        "title": "ASAP: Automatic Semantic Alignment for Phrases",
        "abstract": "",
        "date": "January 2014",
        "authors": [
            "Ana Oliveira Alves",
            "Adriana Ferrugento",
            "Mariana Louren\u00e7o",
            "Filipe Rodrigues"
        ],
        "references": [
            264003485,
            262484733,
            255820377,
            221900777,
            221620547,
            221345932,
            221012753,
            262367926,
            239066288,
            230854746
        ]
    },
    {
        "id": 301404974,
        "title": "UTexas: Natural Language Semantics using Distributional Semantics and Probabilistic Logic",
        "abstract": "We represent natural language semantics by combining logical and distributional information in probabilistic logic. We use Markov Logic Networks (MLN) for the RTE task, and Probabilistic Soft Logic (PSL) for the STS task. The system is evaluated on the SICK dataset. Our best system achieves 73% accuracy on the RTE task, and a Pearson\u2019s correlation of 0.71 on the STS task.",
        "date": "January 2014",
        "authors": [
            "Islam Beltagy",
            "Stephen Roller",
            "Gemma Boleda",
            "Katrin Erk"
        ],
        "references": [
            301404807,
            269403748,
            269403668,
            266169794,
            264003485,
            262484733,
            262250358,
            262203675,
            255949552,
            254502455
        ]
    },
    {
        "id": 301408981,
        "title": "CECL: a New Baseline and a Non-Compositional Approach for the Sick Benchmark",
        "abstract": "This paper describes the two procedures for determining the semantic similarities between sentences submitted for the SemEval 2014 Task 1. MeanMaxSim, an unsupervised procedure, is proposed as a new baseline to assess the efficiency gain provided by compositional models. It outperforms a number of other baselines by a wide margin. Compared to the wordoverlap baseline, it has the advantage of taking into account the distributional similarity between words that are also involved in compositional models. The second procedure aims at building a predictive model using as predictors MeanMaxSim and (transformed) lexical features describing the differences between each sentence of a pair. It finished sixth out of 17 teams in the textual similarity sub-task and sixth out of 19 in the textual entailment subtask.",
        "date": "January 2014",
        "authors": [
            "Yves Bestgen"
        ],
        "references": [
            264003485,
            262484733,
            262408369,
            259821931,
            258278153,
            221013297,
            220355447,
            200045222,
            200044364,
            37420018
        ]
    },
    {
        "id": 301408980,
        "title": "BUAP: Evaluating Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment",
        "abstract": "The results obtained by the BUAP team at Task 1 of SemEval 2014 are presented in this paper. The run submitted is a supervised version based on two classification models: 1) We used logistic regression for determining the semantic relatedness between a pair of sentences, and 2) We employed support vector machines for identifying textual entailment degree between the two sentences. The behaviour for the second subtask (textual entailment) obtained much better performance than the one evaluated at the first subtask (relatedness), ranking our approach in the 7th position of 18 teams that participated at the competition.",
        "date": "January 2014",
        "authors": [
            "Saul Leon",
            "Darnes Vilari\u00f1o",
            "David Pinto",
            "Tovar Vidal Mireya"
        ],
        "references": [
            200045856,
            1783052
        ]
    },
    {
        "id": 301405051,
        "title": "UIO-Lien: Entailment Recognition using Minimal Recursion Semantics",
        "abstract": "In this paper we present our participation in the Semeval 2014 task \u201cEvaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment\u201d. Our results demonstrate that using generic tools for semantic analysis is a viable option for a system that recognizes textual entailment. The invested effort in developing such tools allows us to build systems for reasoning that do not require training.",
        "date": "January 2014",
        "authors": [
            "Elisabeth Lien",
            "Milen Ognianov Kouylekov"
        ],
        "references": [
            258082235,
            225570828,
            220873969,
            202014156,
            301405224,
            262204364,
            234787760,
            45891078
        ]
    },
    {
        "id": 301404948,
        "title": "SemantiKLUE: Robust Semantic Similarity at Multiple Levels Using Maximum Weight Matching",
        "abstract": "Being able to quantify the semantic similarity between two texts is important for many practical applications. SemantiKLUE combines unsupervised and supervised techniques into a robust system for measuring semantic similarity. At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm. The system participated in three SemEval-2014 shared tasks and the competitive results are evidence for its usability in that broad field of application.",
        "date": "January 2014",
        "authors": [
            "Thomas Proisl",
            "Stefan Evert",
            "Paul Greiner",
            "Besim Kabashi"
        ],
        "references": [
            301408978,
            297768191,
            266050261,
            264003485,
            262484733,
            262203675,
            236407765,
            220873989,
            220355307,
            220147700
        ]
    },
    {
        "id": 220946819,
        "title": "Neural Network Probability Estimation for Broad Coverage Parsing.",
        "abstract": "We present a neural-network-based statistical parser, trained and tested on the Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% F-measure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way.",
        "date": "January 2003",
        "authors": [
            "James Henderson"
        ],
        "references": [
            221153125,
            2619855,
            2617139,
            262349927,
            4355809,
            2476010
        ]
    },
    {
        "id": 221345848,
        "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
        "abstract": "We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.",
        "date": "January 2008",
        "authors": [
            "Ronan Collobert",
            "Jason Weston"
        ],
        "references": [
            262408350,
            241135132,
            221618573,
            220874969,
            220873538,
            220873220,
            220817505,
            3175480,
            2985446,
            2522500
        ]
    },
    {
        "id": 234824998,
        "title": "Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines",
        "abstract": "Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English, such as lexicalization, are effective for German. This paper aims to provide some understanding and solid baseline numbers for the task. We examine the performance of three techniques on three treebanks (Negra, Tiger, and T\u00fcBa-D/Z): (i) Markovization, (ii) lexicalization, and (iii) state splitting. We additionally explore parsing with the inclusion of grammatical function information. Explicit grammatical functions are important to German language understanding, but they are numerous, and na\u00efvely incorporating them into a parser which assumes a small phrasal category inventory causes large performance reductions due to increasing sparsity.",
        "date": "June 2008",
        "authors": [
            "Anna N. Rafferty",
            "Christopher D. Manning"
        ],
        "references": [
            271428967,
            258338765,
            300800753,
            277181011,
            271452992,
            268720443,
            267666517,
            260065124,
            252332293,
            249881248
        ]
    },
    {
        "id": 234804753,
        "title": "Vine parsing and minimum risk reranking for speed and precision",
        "abstract": "We describe our entry in the CoNLL-X shared task. The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of cross-lingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data).",
        "date": "June 2006",
        "authors": [
            "Markus Dreyer",
            "David A. Smith",
            "Noah A. Smith"
        ],
        "references": [
            267797457,
            260291394,
            232031795,
            228916842,
            262349927,
            260239801,
            242367463,
            234799099,
            228644575,
            225740821
        ]
    },
    {
        "id": 228629276,
        "title": "A latent variable model of synchronous parsing for syntactic and semantic dependencies",
        "abstract": "We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntac-tic dependencies LAS, and 73.1% seman-tic dependencies F1.",
        "date": "September 2008",
        "authors": [
            "James Henderson",
            "Paola Merlo",
            "Gabriele Musillo",
            "Ivan Titov"
        ],
        "references": [
            228627119,
            228621519,
            220874688,
            220817478,
            220017637,
            200179364,
            312607081,
            220873714,
            220017689
        ]
    },
    {
        "id": 221102780,
        "title": "Informed ways of improving data-driven dependency parsing for German",
        "abstract": "We investigate a series of targeted modifications to a data-driven dependency parser of German and show that these can be highly effective even for a relatively well studied language like German if they are made on a (linguistically and methodologically) informed basis and with a parser implementation that allows for fast and robust training and application. Making relatively small changes to a range of very different system components, we were able to increase labeled accuracy on a standard test set (from the CoNLL 2009 shared task), ignoring gold standard part-of-speech tags, from 87.64% to 89.40%. The study was conducted in less than five weeks and as a secondary project of all four authors. Effective modifications include the quality and combination of auto-assigned morphosyntactic features entering machine learning, the internal feature handling as well as the inclusion of global constraints and a combination of different parsing strategies.",
        "date": "January 2010",
        "authors": [
            "Wolfgang Seeker",
            "Bernd Bohnet",
            "Lilja \u00d8vrelid",
            "Jonas Kuhn"
        ],
        "references": [
            237533325,
            234809510,
            230876751,
            230876740,
            228974258,
            228916842,
            228657745,
            309532479,
            281263230,
            267448335
        ]
    },
    {
        "id": 221101625,
        "title": "Dependency Forest for Statistical Machine Translation.",
        "abstract": "We propose a structure called dependency forest for statistical machine translation. A dependency forest compactly represents multiple dependency trees. We develop new algorithms for extracting string-to-dependency rules and training dependency language models. Our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets.",
        "date": "August 2010",
        "authors": [
            "Zhaopeng Tu",
            "Yang Liu",
            "Young-Sook Hwang",
            "Qun Liu"
        ],
        "references": [
            221013057,
            221012969,
            221012897,
            220874221,
            312370112,
            310424323,
            268237865,
            221013270,
            221012850,
            220875205
        ]
    },
    {
        "id": 221013153,
        "title": "Is it Really that Difficult to Parse German?",
        "abstract": "Abstract This paper presents a comparative,study of probabilistic treebank parsing of German, using the Negra and T\u00fcBa-D/Z tree- banks.,Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexical- ization of PCFG models,boosts parsing performance,for both treebanks. The experiments also show,that there is a big difference in parsing performance, when trained on the Negra and on the T\u00a8 uBa- D/Z treebanks. Parser performance,for the models trained on T\u00a8 uBa-D/Z are compara- ble to parsing results for English with the Stanford parser, when trained on the Penn treebank. This comparison,at least sug- gests that German,is not harder to parse than its West-Germanic neighbor language English.",
        "date": "January 2006",
        "authors": [
            "Sandra K\u00fcbler",
            "Erhard Hinrichs",
            "Wolfgang Maier"
        ],
        "references": [
            228621065,
            220873665,
            220873648,
            220017613,
            292315429,
            234829591,
            228559411,
            220873570,
            220355517,
            220355435
        ]
    },
    {
        "id": 221013057,
        "title": "The impact of parse quality on syntactically-informed statistical machine translation",
        "abstract": "We investigate the impact of parse quality on a syntactically-informed statistical ma- chine translation system applied to techni- cal text. We vary parse quality by vary- ing the amount of data used to train the parser. As the amount of data increases, parse quality improves, leading to im- provements in machine translation output and results that significantly outperform a state-of-the-art phrasal baseline.",
        "date": "January 2006",
        "authors": [
            "Chris Quirk",
            "Simon Corston-Oliver"
        ],
        "references": [
            228916842,
            221102737,
            220874221,
            220873497,
            312607081,
            307175021,
            246793823,
            222835593,
            221995983,
            221101482
        ]
    },
    {
        "id": 220947009,
        "title": "Large Linguistically-Processed Web Corpora for Multiple Languages.",
        "abstract": "The Web contains vast amounts of linguis- tic data. One key issue for linguists and language technologists is how to access it. Commercial search engines give highly compromised access. An alternative is to crawl the Web ourselves, which also al- lows us to remove duplicates and near- duplicates, navigational material, and a range of other kinds of non-linguistic mat- ter. We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which sup- ports sophisticated linguistic queries. We have now done this for German and Ital- ian, with corpus sizes of over 1 billion words in each case. We provide Web ac- cess to the corpora in our query tool, the Sketch Engine.",
        "date": "January 2006",
        "authors": [
            "Marco Baroni",
            "Adam Kilgarriff"
        ],
        "references": [
            267448270,
            242692098,
            242379316,
            307945930,
            267448182,
            246531444,
            233649548,
            222391121,
            220691324
        ]
    },
    {
        "id": 221152217,
        "title": "Tiered Tagging and Combined Language Models Classifiers",
        "abstract": "We address the problem of morpho-syntactic disambiguation of arbitrary texts in a highly inflectional natural language. We use a large tagset (615 tags), EAGLES and MULTEXT compliant [5]. The large tagset is internally mapped onto a reduced one (82 tags), serving statistical disambiguation, and a text disambiguated in terms of this tagset is subsequently subject to a recovery process of all the information left out from the large tagset. This two step process is called tiered tagging. To further improve the tagging accuracy we use a combined language models classifier, a procedure that interpolates the results of tagging the same text with several register-specific language models.",
        "date": "September 1999",
        "authors": [
            "Dan Ioan Tufis"
        ],
        "references": [
            245801182,
            228608020,
            2643463,
            2458234,
            243756367,
            13540799,
            2778347,
            2350944,
            2248129,
            1782546
        ]
    },
    {
        "id": 220874161,
        "title": "Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset.",
        "abstract": "The paper puts forward a quasi-dependency model for structural analysis of Chinese baseNPs and a MDL-based algorithm for quasi-dependency-strength acquisition. The experiments show that the proposed model is more suitable for Chinese baseNP analysis ...",
        "date": "August 1998",
        "authors": [
            "Jan Haji\u010d",
            "Barbora Hladka"
        ],
        "references": [
            247697163,
            230875928,
            224619517,
            2757051,
            2324402,
            2294814
        ]
    },
    {
        "id": 2578231,
        "title": "MBT: A Memory-Based Part of Speech Tagger-Generator",
        "abstract": "We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably ...",
        "date": "November 1999",
        "authors": [
            "Walter Daelemans",
            "Jakub Zavrel",
            "Peter Berck",
            "Steven Gillis"
        ],
        "references": [
            313224008,
            270877194,
            261344688,
            247113703,
            244956971,
            243764375,
            243646226,
            242620672,
            242565137,
            242494825
        ]
    },
    {
        "id": 247697163,
        "title": "Tagging of Inflective Languages: a Comparison",
        "abstract": "",
        "date": "January 1997",
        "authors": [
            "Jan Haji\u010d"
        ],
        "references": []
    },
    {
        "id": 243697334,
        "title": "Syntactic category disambiguation with neural networks",
        "abstract": "A 560-unit neural network with two layers of modifiable connections was trained by means of back-propagation to disambiguate the syntactic categories of words in samples of text taken from the Brown Corpus. After training, the network was able to successfully disambiguate words in previously unanalyzed text with 95% accuracy, a performance level comparable to the best current computational techniques for the disambiguation of syntactic function. The model incorporates plausible psychological constraints on its input and output representations, and exhibited human-like behavior during parts of the learning process. The network's success suggests that syntactic category disambiguation may be mainly a low-level, bottom-up process with little dependence on the recognition of higher-level syntactic structures. Although the network simulates only a restricted component of the human language processing mechanism, its intrinsic ability to use partially formed data should allow it to be easily integrated into a full-scale language comprehension system. The model's overall performance level, along with its psychological plausibility, indicates that neural networks may be a new and useful approach to building human language processing models.",
        "date": "July 1989",
        "authors": [
            "Julian Benello",
            "Andrew W. Mackie",
            "James A. Anderson"
        ],
        "references": [
            299483779,
            268659044,
            243774647,
            242637483,
            239062388,
            237044580,
            235143479,
            230875963,
            229091480,
            224751624
        ]
    },
    {
        "id": 230875928,
        "title": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text",
        "abstract": "This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query applications. Multimedia answers include videodisc images and heuristically-produced complete sentences ...",
        "date": "January 1988",
        "authors": [
            "Kenneth Ward Church"
        ],
        "references": [
            238760957,
            242574596,
            242382431,
            49492612,
            37599853,
            3488972
        ]
    },
    {
        "id": 215470768,
        "title": "Probabilistic Part-of-Speech Tagging Using Decision Trees",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "Helmut Schmid"
        ],
        "references": []
    },
    {
        "id": 2778347,
        "title": "Classifier Combination for Improved Lexical Disambiguation",
        "abstract": "One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees...",
        "date": "August 1998",
        "authors": [
            "Eric Brill",
            "Jun Wu"
        ],
        "references": [
            234776861,
            321428427,
            312607081,
            279893883,
            247210063,
            230876580,
            230876410,
            230876151,
            200003659
        ]
    },
    {
        "id": 278724277,
        "title": "The best of two worlds: cooperation of statistical and rule-based taggers for Czech",
        "abstract": "",
        "date": "June 2007",
        "authors": [
            "Drahom\u00edra \"johanka\" Spoustov\u00e1",
            "Jan Haji\u010d",
            "Jan Votrubec",
            "Pavel Krbec"
        ],
        "references": [
            269076951,
            263620535,
            240167092,
            230876180,
            221152377,
            220874018,
            263620387,
            243768557,
            225130515,
            221152452
        ]
    },
    {
        "id": 226043864,
        "title": "Tree Induction for Probability-Based Ranking",
        "abstract": "Tree induction is one of the most effective and widely used methods for building classification models. However, many applications require cases to be ranked by the probability of class membership. Probability estimation trees (PETs) have the same attractive features as classification trees (e.g., comprehensibility, accuracy and efficiency in high dimensions and on large data sets). Unfortunately, decision trees have been found to provide poor probability estimates. Several techniques have been proposed to build more accurate PETs, but, to our knowledge, there has not been a systematic experimental analysis of which techniques actually improve the probability-based rankings, and by how much. In this paper we first discuss why the decision-tree representation is not intrinsically inadequate for probability estimation. Inaccurate probabilities are partially the result of decision-tree induction algorithms that focus on maximizing classification accuracy and minimizing tree size (for example via reduced-error pruning). Larger trees can be better for probability estimation, even if the extra size is superfluous for accuracy maximization. We then present the results of a comprehensive set of experiments, testing some straightforward methods for improving probability-based rankings. We show that using a simple, common smoothing method\u2014the Laplace correction\u2014uniformly improves probability-based rankings. In addition, bagging substantially improves the rankings, and is even more effective for this purpose than for improving accuracy. We conclude that PETs, with these simple modifications, should be considered when rankings based on class-membership probability are required.",
        "date": "September 2003",
        "authors": [
            "Foster Provost",
            "Pedro Domingos"
        ],
        "references": [
            290824736,
            327606791,
            313039299,
            310606656,
            303517845,
            285021015,
            275340906,
            262159891,
            260406227,
            256484410
        ]
    },
    {
        "id": 225195066,
        "title": "An Empirical Comparison of Pruning Methods for Decision Tree Induction",
        "abstract": "This paper compares five methods for pruning decision trees, developed from sets of examples. When used with uncertain rather than deterministic data, decision-tree induction involves three main stages\u2014creating a complete tree able to classify all the training examples, pruning this tree to give statistical reliability, and processing the pruned tree to improve understandability. This paper concerns the second stage\u2014pruning. It presents empirical comparisons of the five methods across several domains. The results show that three methods\u2014critical value, error complexity and reduced error\u2014perform well, while the other two may cause problems. They also show that there is no significant interaction between the creation and pruning methods.",
        "date": "January 1989",
        "authors": [
            "John Mingers"
        ],
        "references": [
            238582946,
            288987576,
            275270590,
            270877194,
            270367660,
            246666942,
            243777607,
            243761731,
            241264532,
            240310918
        ]
    },
    {
        "id": 221112612,
        "title": "Improving the AUC of Probabilistic Estimation Trees",
        "abstract": "In this work we investigate several issues in order to improve the per- formance of probabilistic estimation trees (PETs). First, we derive a new prob- ability smoothing that takes into account the class distributions of all the nodes from the root to each leaf. Secondly, we introduce or adapt some new splitting criteria aimed at improving probability estimates rather than improving classifi- cation accuracy, and compare them with other accuracy-aimed splitting criteria. Thirdly, we analyse the effect of pruning methods and we choose a cardinality- based pruning, which is able to significantly reduce the size of the trees without degrading the quality of the estimates. The quality of probability estimates of these three issues is evaluated by the 1-vs-1 multi-class extension of the Area Under the ROC Curve (AUC) measure, which is becoming widespread for evaluating probability estimators, ranking of predictions in particular.",
        "date": "September 2003",
        "authors": [
            "C\u00e8sar Ferri",
            "Peter A. Flach",
            "Jose Hernandez-Orallo"
        ],
        "references": [
            228923735,
            226043864,
            310606656,
            279397521,
            275270590,
            248229770,
            248192679,
            240310918,
            235961479,
            235910409
        ]
    },
    {
        "id": 220874018,
        "title": "Serial Combination of Rules and Statistics: A Case Study in Czech Tagging",
        "abstract": "A hybrid system is described which combines the strength of manual rule- writing and statistical learning, obtain- ing results superior to both methods if applied separately. The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambigua- tion with recall close to 100% is applied first, and a trigram HMM tagger runs on its results. An experiment in Czech tag- ging has been performed with encour- aging results.",
        "date": "January 2001",
        "authors": [
            "Jan Haji\u010d",
            "Pavel Krbec",
            "Pavel Kveton",
            "Karel Oliva"
        ],
        "references": [
            269076951,
            230876180,
            221101514,
            313753822,
            247697163,
            243768557,
            230876410,
            230876149,
            230876094,
            228603421
        ]
    },
    {
        "id": 2882867,
        "title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network",
        "abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.",
        "date": "March 2004",
        "authors": [
            "Kristina Toutanova",
            "Dan Klein",
            "Christopher D. Manning",
            "Yoram Singer"
        ],
        "references": [
            234813710,
            221995760,
            220874539,
            220874417,
            220017637,
            2925361,
            245096052,
            230876291,
            230875928,
            2919269
        ]
    },
    {
        "id": 2539686,
        "title": "Serial Combination of Rules and Statistics: A Case Study in Czech Tagging",
        "abstract": "A hybrid system is described which combines the strength of manual rulewriting and statistical learning, obtaining results superior to both methods if applied separately. The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambiguation with recall close to 100% is applied first, and a trigram HMM tagger runs on its results. An experiment in Czech tagging has been performed with encouraging results.",
        "date": "October 2002",
        "authors": [
            "Jan Haji\u010d",
            "Pavel Krbec",
            "Pavel Kveton",
            "Karel Oliva"
        ],
        "references": [
            269076951,
            230876180,
            226547662,
            220875102,
            220874161,
            313753822,
            230876410,
            230876094,
            226900496,
            34766289
        ]
    },
    {
        "id": 234801831,
        "title": "Evaluation and extension of maximum entropy models with inequality constraints",
        "abstract": "A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension.",
        "date": "January 2003",
        "authors": [
            "Jun'ichi Kazama",
            "Jun'ichi Tsujii"
        ],
        "references": [
            221301327,
            51991698,
            278651717,
            244958007,
            244449401,
            243763580,
            238122176,
            237437436,
            228540070,
            221997066
        ]
    },
    {
        "id": 225121454,
        "title": "Efficient Inference in Large Conditional Random Fields",
        "abstract": "Conditional Random Fields (CRFs) are widely known to scale poorly, particularly for tasks with large numbers of states or with richly connected graphical structures. This is a consequence of inference having a time complexity which is at best quadratic in the number of states. This paper describes a novel parameterisation of the CRF which ties the majority of clique potentials, while allowing individual potentials for a subset of the labellings. This has two beneficial effects: the parameter space of the model (and thus the propensity to over-fit) is reduced, and the time complexity of training and decoding becomes sub-quadratic. On a standard natural language task, we reduce CRF training time four-fold, with no loss in accuracy. We also show how inference can be performed efficiently in richly connected graphs, in which current methods are intractable.",
        "date": "September 2006",
        "authors": [
            "Trevor Cohn"
        ],
        "references": [
            228629735,
            221344783,
            221013041,
            313037830,
            313037696,
            271431029,
            228057950,
            224642153,
            221345388,
            220874707
        ]
    },
    {
        "id": 221345409,
        "title": "Sparse higher order conditional random fields for improved sequence labeling",
        "abstract": "In real sequence labeling tasks, statistics of many higher order features are not sufficient due to the training data sparseness, very few of them are useful. We describe Sparse Higher Order Conditional Random Fields (SHO-CRFs), which are able to handle local features and sparse higher order features together using a novel tractable exact inference algorithm. Our main insight is that states and transitions with same potential functions can be grouped together, and inference is performed on the grouped states and transitions. Though the complexity is not polynomial, SHO-CRFs are still efficient in practice because of the feature sparseness. Experimental results on optical character recognition and Chinese organization name recognition show that with the same higher order feature set, SHO-CRFs significantly outperform previous approaches.",
        "date": "June 2009",
        "authors": [
            "Xian Qian",
            "Xiaoqian Jiang",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "references": [
            309347291,
            252535897,
            237291970,
            221345388,
            221153020,
            220874707,
            2869096,
            2526069
        ]
    },
    {
        "id": 221344720,
        "title": "Accelerated training of conditional random fields with stochastic gradient methods",
        "abstract": "We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the train- ing of Conditional Random Fields (CRFs). On several large data sets, the resulting opti- mizer converges to the same quality of solu- tion over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques.",
        "date": "January 2006",
        "authors": [
            "S. V. N. Vishwanathan",
            "Nicol N. Schraudolph",
            "Mark W. Schmidt",
            "Kevin P. Murphy"
        ],
        "references": [
            282444750,
            270366490,
            266452842,
            243773059,
            235410460,
            233950933,
            230596035,
            228813946,
            228757213,
            228571361
        ]
    },
    {
        "id": 220875020,
        "title": "A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing.",
        "abstract": "This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks. Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estima- tion with L2 regularization, the Averaged Perceptron (AP), and Boosting. We also in- vestigate ME estimation with L1 regularization using a novel optimization algorithm, and BLasso, which is a version of Boosting with Lasso (L1) regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation. Our experiments show that across tasks, three of the estimators \u2014 ME estimation with L1 or L2 regularization, and AP \u2014 are in a near sta- tistical tie for first place.",
        "date": "January 2007",
        "authors": [
            "Jianfeng Gao",
            "Galen Andrew",
            "Mark Johnson",
            "Kristina Toutanova"
        ],
        "references": [
            234801831,
            262349927,
            242358078,
            239295682,
            230876410,
            230873264,
            228685421,
            221013284,
            221012778,
            220875180
        ]
    },
    {
        "id": 220874186,
        "title": "Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty.",
        "abstract": "Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch training algorithms. However, L1-regularization, which is be- coming popular in natural language pro- cessing because of its ability to pro- duce compact models, cannot be effi- ciently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gra- dients. We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty. We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental re- sults demonstrate that our method can pro- duce compact and accurate models much more quickly than a state-of-the-art quasi- Newton method for L1-regularized log- linear models.",
        "date": "January 2009",
        "authors": [
            "Yoshimasa Tsuruoka",
            "Jun'ichi Tsujii",
            "Sophia Ananiadou"
        ],
        "references": [
            234801831,
            234798411,
            228922522,
            228530416,
            297123726,
            264913332,
            234797524,
            228685421,
            228057950,
            221619338
        ]
    },
    {
        "id": 220873150,
        "title": "Efficient Inference of CRFs for Large-Scale Natural Language Data.",
        "abstract": "This paper presents an efficient inference algo- rithm of conditional random fields (CRFs) for large-scale data. Our key idea is to decompose the output label state into an active set and an inactive set in which most unsupported tran- sitions become a constant. Our method uni- fies two previous methods for efficient infer- ence of CRFs, and also derives a simple but robust special case that performs faster than exact inference when the active sets are suffi- ciently small. We demonstrate that our method achieves dramatic speedup on six standard nat- ural language processing problems. small group of the labeling states has sufficient statis- tics. We show that the SFB and the TP are special cases of our method because they derive from our unified al- gorithm with a different setting of parameters. We also present a simple but robust variant algorithm in which CRFs efficiently learn and predict large-scale natural language data. 2 Linear-chain CRFs Many versions of CRFs have been developed for use in natural language processing, computer vision, and machine learning. For simplicity, we concentrate on",
        "date": "January 2009",
        "authors": [
            "Minwoo Jeong",
            "Chin-Yew Lin",
            "Gary Geunbae Lee"
        ],
        "references": [
            225121454,
            220873066,
            313037830,
            235246939,
            224642153,
            221301012,
            2886117,
            2529190
        ]
    },
    {
        "id": 46515750,
        "title": "Regularized Paths for Generalized Linear Models Via Coordinate Descent",
        "abstract": "We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include \u00c3\u00a2\u00c2\u00c2_1 (the lasso), \u00c3\u00a2\u00c2\u00c2_2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.",
        "date": "February 2010",
        "authors": [
            "Rob Tibshirani",
            "Trevor Hastie",
            "Jerome H. Friedman"
        ],
        "references": [
            264955448,
            228354422,
            287823327,
            270244343,
            242443400,
            237519853,
            228855445,
            228612687,
            227604843,
            226608784
        ]
    },
    {
        "id": 230920580,
        "title": "Learning algorithms with optimal stability in neural networks",
        "abstract": "To ensure large basins of attraction in spin-glass-like neural networks of two-state elements xi imu =+or-1. The authors propose to study learning rules with optimal stability Delta , where delta is the largest number satisfying Delta <or=( Sigma j Jij xi jmu ) xi imu ; mu =1. . . . .p: i=1. . . . .N (where N is the number of neurons and p is the number of patterns). They motivate this proposal and provide optimal stability learning rules for two different choices of normalisation for the synaptic matrix (Jij). In addition, numerical work is presented which gives the value of the optimal stability for random uncorrelated patterns.",
        "date": "January 1999",
        "authors": [
            "Werner Krauth",
            "Marc Mezard"
        ],
        "references": [
            283617646,
            45351163,
            41711344,
            16246447,
            6026283,
            234515104,
            13391095,
            13254774
        ]
    },
    {
        "id": 220816928,
        "title": "Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data",
        "abstract": "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines. 1",
        "date": "October 2005",
        "authors": [
            "Yoshimasa Tsuruoka",
            "Jun'ichi Tsujii"
        ],
        "references": [
            234801831,
            221303669,
            220817026,
            220320545,
            220017637,
            2882867,
            2853958,
            228057950,
            221303699,
            2927147
        ]
    },
    {
        "id": 2930135,
        "title": "Ultraconservative Online Algorithms for Multiclass Problems",
        "abstract": "In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarityscore between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li a...",
        "date": "February 2003",
        "authors": [
            "Koby Crammer",
            "Yoram Singer",
            "K. Warmuth"
        ],
        "references": [
            275270590,
            271513141,
            256744905,
            248848344,
            247922844,
            243697670,
            242437158,
            240310918,
            239546749,
            238625098
        ]
    },
    {
        "id": 2799191,
        "title": "Large Margin Classification Using the Perceptron Algorithm",
        "abstract": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt 's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik 's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort. 1 Introduction One of the most influential developments in the theory of machine learning in the last few years is Vapnik's work on supp...",
        "date": "February 1999",
        "authors": [
            "Yoav Freund",
            "Robert E. Schapire"
        ],
        "references": [
            279595537,
            275807826,
            264967033,
            256744905,
            256232217,
            253277969,
            248857140,
            247922844,
            238740116,
            235204619
        ]
    },
    {
        "id": 248208758,
        "title": "Une Approche th'eorique de l''Apprentissage Connexionniste: Applications `a la Reconnaissance de la",
        "abstract": "",
        "date": "January 1991",
        "authors": [
            "L'eon Bottou"
        ],
        "references": []
    },
    {
        "id": 234543089,
        "title": "Speech understanding systems",
        "abstract": "This report describes recent progress of the BBN Speech Understanding Systems project covering the period from May 1976 to July 1976. The BBN Speech Understanding project is an effort to develop a continuous speech understanding system which uses syntactic, semantic and pragmatic support from higher level linguistic knowledge sources to compensate for the inherent acoustic indeterminacies in continuous spoken utterances. These knowledge sources are integrated with sophisticated signal processing and acoustic-phonetic analysis of the input signal, to produce a total system for understanding continuous speech. The system contains components for signal analysis; acoustic parameter extraction; acoustic-phonetic analysis of the signal; phonological expansion of the lexicon; lexical matching and retrieval; syntactic, semantic, and pragmatic analysis and prediction; and inferential fact retrieval and question answering, as well as synthesized text or spoken output.",
        "date": "July 1976",
        "authors": [
            "W. A. Woods",
            "L. Bates",
            "G. Brown",
            "C. C. Cook"
        ],
        "references": []
    },
    {
        "id": 230876410,
        "title": "A Maximum Entropy Part-of-Speech Tagger",
        "abstract": "",
        "date": "May 1996",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "references": []
    },
    {
        "id": 285977158,
        "title": "Negative polarity items: Triggering, scope, and c-command",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "Jack Hoeksema"
        ],
        "references": [
            231920647,
            303721313,
            285837866,
            284323896,
            281022848,
            271650889,
            269424218,
            246699915,
            246684264,
            227037490
        ]
    },
    {
        "id": 285102407,
        "title": "Flaubert triggers, squatitive negation, and other quirks of grammar",
        "abstract": "",
        "date": "January 2001",
        "authors": [
            "Laurence R. Horn"
        ],
        "references": [
            263377713,
            247046187,
            247043144,
            344472769,
            301690356,
            288219269,
            284430590,
            276001054,
            247938511,
            247926072
        ]
    },
    {
        "id": 273014193,
        "title": "Polarity Sensitivity as (Non) Veridical Dependency",
        "abstract": "",
        "date": "March 2000",
        "authors": [
            "Asya Pereltsvaig",
            "Anastasia Giannakidou"
        ],
        "references": [
            319394041,
            344906470,
            344472769,
            321620707,
            319394639,
            319394051,
            319393970,
            313707890,
            313707255,
            303990922
        ]
    },
    {
        "id": 318494453,
        "title": "Metaphors We Live By",
        "abstract": "The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are \"metaphors we live by\"--metaphors that can shape our perceptions and actions without our ever noticing them. In this updated edition of Lakoff and Johnson's influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.",
        "date": "April 1980",
        "authors": [
            "George Lakoff",
            "Mark Johnson"
        ],
        "references": []
    },
    {
        "id": 275999985,
        "title": "Negative Contexts: Collocation, Polarity, and Multiple Negation",
        "abstract": "",
        "date": "September 1999",
        "authors": [
            "Raul Aranovich",
            "Ton van der Wouden"
        ],
        "references": []
    },
    {
        "id": 275998984,
        "title": "Negative and Positive Polarity",
        "abstract": "",
        "date": "September 1995",
        "authors": [
            "Yafei Li",
            "Ljiljana Progovac"
        ],
        "references": []
    },
    {
        "id": 271650889,
        "title": "Some Reasons Why There Can't Be Any Some-Any Rule",
        "abstract": "This paper presents evidence that semantic notions-such as presupposition, speaker's and hearer's beliefs about the world, and previous discourse-must be taken into account in a complete treatment of the distribution of some and any in conditional, negative, and interrogative sentences. Syntactic conditions alone will not account for the fact that, in certain sentence types, the two forms occur with different meanings.",
        "date": "September 1969",
        "authors": [
            "Robin Lakoff"
        ],
        "references": []
    },
    {
        "id": 271213714,
        "title": "Words and the Grammar of Context",
        "abstract": "Acknowledgements Foreword Charles J. Fillmore 1. Regularity and idiomaticity in grammatical construction the case of let alone with Charles J. Fillmore and Mary Catherine O'Connor 2. Even 3. At least 4. Construction grammar 5. Linguistic competence and folk theories of language: two English hedges 6. The kind of/sort of construction 7. Contextual operators: respective, respectively, and vice versa 8. Constructional modus tollens and level of conventionality 9. Three properties of the ideal reader 10. The inheritance of presuppostions References.",
        "date": "January 1996",
        "authors": [
            "Paul Kay"
        ],
        "references": []
    },
    {
        "id": 259810910,
        "title": "Conventional Implicature",
        "abstract": "",
        "date": "January 1979",
        "authors": [
            "Lauri Karttunen",
            "Stanley Peters"
        ],
        "references": []
    },
    {
        "id": 246430630,
        "title": "Implicature, Explicature, and Truth-Theoretic Semantics",
        "abstract": "",
        "date": "January 1988",
        "authors": [
            "Robyn Carston"
        ],
        "references": []
    },
    {
        "id": 243677541,
        "title": "Metalinguistic Negation and Pragmatic Ambiguity",
        "abstract": "When 'marked' or 'external' negation has not been treated as an additional semantic operator alongside the straightforward truth-functional, presupposition-preserving ordinary ('internal') negation, it has been collapsed with internal negation into a unified general logical operator on propositions. Neither of these approaches does justice to the differences and kinships between and within the two principal varieties of negation in natural language. Marked negation is not reducible to a truth-functional one-place connective with the familiar truth-table for negation, nor is it definable as a separate logical operator; it represents, rather, a metalinguistic device for registering objection to a previous utterance (not proposition) on any grounds whatever, including the way it was pronounced.",
        "date": "March 1985",
        "authors": [
            "Laurence R. Horn"
        ],
        "references": [
            284877197,
            243767223,
            338572909,
            284890554,
            274114060,
            268898769,
            245335316,
            243764535,
            242361246,
            228040191
        ]
    },
    {
        "id": 240752088,
        "title": "Between semantics and pragmatics: The two types of \u2018but\u2019\u2014Hebrew \u2018Aval\u2019 and \u2018Ela\u2019",
        "abstract": "Recent proposals for the analysis of the equivalents of Hebrew 'aval,and 'ela\u2019 (in particular, English 'but\u2019 and French 'mais\u2019) are reviewed and it is suggested that they cannot provide a satisfactory account of the full range of uses of 'aval\u2019 and 'ela\u2019 as exemplified in this paper. It is further argued that in order to account for the linguistic facts of 'aval\u2019 and 'ela\u2019 we must allow for a broader definition of 'utterance meaning\u2019 which would include both semantic and pragmatic notions, as these are traditionally classified. Aval,is shown to operate on two different layers of meaning, while 'ela\u2019 always operates on the same layer. However, they perform similar cancellation jobs in all cases. General conditions governing the cancellation function of 'aval\u2019 and 'ela\u2019 are proposed in terms of such a unified treatment of the various uses of these particles. The meaning of an utterance is viewed as made up of hierarchically ordered 'layers of meaning\u2019 (some of which contain partially ordered sublayers), from an inner 'core\u2019 of propositional content to an outer 'shell\u2019 of conversational implicature, via such layers as modality, illocutionary force, felicity conditions, etc.",
        "date": "January 1977",
        "authors": [
            "MARCELO DASCAL",
            "Tamar Katriel"
        ],
        "references": [
            239018979
        ]
    },
    {
        "id": 313005855,
        "title": "Some further notes on logic and conversation",
        "abstract": "",
        "date": "January 1978",
        "authors": [
            "H. Grice"
        ],
        "references": []
    },
    {
        "id": 284046340,
        "title": "Logic and conversation, in P",
        "abstract": "",
        "date": "January 1967",
        "authors": [
            "H. Paul Grice"
        ],
        "references": []
    },
    {
        "id": 271213760,
        "title": "Presuppositions and Non-Truth-Conditional Semantics",
        "abstract": "",
        "date": "January 1975",
        "authors": [
            "Dierdre Wilson"
        ],
        "references": []
    },
    {
        "id": 270151899,
        "title": "Mental Representations: The Interface between Language and Reality",
        "abstract": "",
        "date": "June 1991",
        "authors": [
            "Daniel L. Everett",
            "Ruth Margaret Kempson"
        ],
        "references": []
    },
    {
        "id": 269425203,
        "title": "Pragmatics: Implicature, Presupposition, and Logical Form",
        "abstract": "",
        "date": "December 1980",
        "authors": [
            "Robert Stalnaker",
            "Gerald Gazdar"
        ],
        "references": []
    },
    {
        "id": 256279346,
        "title": "Semantic command over pragmatic priority",
        "abstract": "The coordinating conjunction \u2018and\u2019 is best regarded not as equivalent to logical conjunction but as an autonomous linguistic conjunction whose meaning is captured by the notion of \u2018semantic command\u2019.",
        "date": "June 1980",
        "authors": [
            "Zev bar-Lev",
            "Arthur Palacas"
        ],
        "references": [
            284877197,
            271213840,
            238323754
        ]
    },
    {
        "id": 228882059,
        "title": "Continuous Space Language Models For Statistical Machine Translation",
        "abstract": "This paper describes an open-source implementation of the so-called continuous space lan-guage model and its application to statistical machine translation. The underlying idea of this approach is to attack the data sparseness problem by performing the language model probabil-ity estimation in a continuous space. The projection of the words and the probability estimation are both performed by a multi-layer neural network. This paper describes the theoretical back-ground of the approach, efficient algorithms to handle the computational complexity, and gives implementation details and reports experimental results on a variety of tasks.",
        "date": "February 2010",
        "authors": [
            "Holger Schwenk"
        ],
        "references": [
            253423758,
            252212271,
            244454970,
            237327924,
            228615517,
            221488478,
            240977328,
            225818196,
            222650733,
            222420693
        ]
    },
    {
        "id": 270878029,
        "title": "Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models",
        "abstract": "Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize for a task specific metric, such as BLEU in machine translation. We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion. Furthermore, we tackle the issue of directly integrating a recurrent network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased statistical machine translation system trained onWMT2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a crossentropy trained model by up to 0.6 BLEU in a single reference setup.",
        "date": "June 2014",
        "authors": [
            "Michael Auli",
            "Jianfeng Gao"
        ],
        "references": [
            270877613,
            262410837,
            262272503,
            262246326,
            258245233,
            255564378,
            241637478,
            228940090,
            221489926,
            221012594
        ]
    },
    {
        "id": 262416331,
        "title": "Compositional Morphology for Word Representations and Language Modelling",
        "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.",
        "date": "May 2014",
        "authors": [
            "Jan A. Botha",
            "Phil Blunsom"
        ],
        "references": [
            307174609,
            290650943,
            270267171,
            262483647,
            262272503,
            228882059,
            228763052,
            224246503,
            222099363,
            221442282
        ]
    },
    {
        "id": 228782453,
        "title": "KenLM: Faster and smaller language model queries",
        "abstract": "We present KenLM, a library that imple-ments two data structures for efficient lan-guage model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is de-signed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57% of the mem-ory. The TRIE data structure is a trie with bit-level packing, sorted records, interpola-tion search, and optional quantization aimed at lower memory consumption. TRIE simul-taneously uses less memory than the small-est lossless baseline and less CPU than the fastest baseline. Our code is open-source 1 , thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance tech-niques used and presents benchmarks against alternative implementations.",
        "date": "July 2011",
        "authors": [
            "Kenneth Heafield"
        ],
        "references": [
            262368088,
            221486318,
            221481044,
            221013245,
            220874513,
            220874004,
            2588204,
            228877275,
            228379274,
            220873254
        ]
    },
    {
        "id": 270878323,
        "title": "Hidden Markov tree models for semantic class induction",
        "abstract": "",
        "date": "August 2013",
        "authors": [
            "Edouard Grave",
            "Guillaume Obozinski",
            "Francis Bach"
        ],
        "references": [
            312369894,
            252064877,
            251574306,
            303802749,
            266489133,
            262157385,
            261536894,
            258868648,
            248601167,
            236025174
        ]
    },
    {
        "id": 306160949,
        "title": "A dataset of syntactic-ngrams over time from a very large corpus of english books",
        "abstract": "",
        "date": "January 2013",
        "authors": [
            "Yoav Goldberg",
            "J. Orwant"
        ],
        "references": []
    },
    {
        "id": 270877781,
        "title": "How much do word embeddings encode about syntax?",
        "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
        "date": "June 2014",
        "authors": [
            "Jacob Andreas",
            "Dan Klein"
        ],
        "references": [
            270878122,
            266201822,
            257882504,
            220874045,
            220873681,
            303802749,
            289827888,
            285895924,
            270878508,
            270878336
        ]
    },
    {
        "id": 267448320,
        "title": "Learning Representations for Weakly Supervised Natural Language Processing Tasks",
        "abstract": "Finding the right representations for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce. This paper investigates novel techniques for extracting features from n-gram models, Hidden Markov Models, and other statistical language models, including a novel Partial Lattice Markov Random Field model. Experiments on part-of-speech tagging and information extraction, among other tasks, indicate that features taken from statistical language models, in combination with more traditional features, outperform traditional representations alone, and that graphical model representations outperform n-gram models, especially on sparse and polysemous words.",
        "date": "June 2013",
        "authors": [
            "Fei Huang",
            "Arun Ahuja",
            "Doug Downey",
            "Yi Yang"
        ],
        "references": [
            236157548,
            230876750,
            228348202,
            221618972,
            221618573,
            221618429,
            221344862,
            221013061,
            221013002,
            221012991
        ]
    },
    {
        "id": 324940566,
        "title": "Guidelines for the CLEAR Style Constituent to Dependency Conversion",
        "abstract": "The dependency conversion described here takes the Stanford dependency approach as the core structure and integrates the CoNLL dependency approach to add long-distance dependencies, to enrich important relations like object predicates, and to minimize unclassified dependencies. The Stanford dependency approach is taken for the core structure because it gives more fine-grained dependency labels and is currently used more widely than the CoNLL dependency approach. For our conversion, head-finding rules and heuristics are completely reanalyzed from the previous work to handle constituent tags and relations not introduced by the Penn Treebank. Our conversion has been evaluated with several different constituent Treebanks and showed robust results across these corpora.",
        "date": "January 2012",
        "authors": [
            "Jinho D. Choi",
            "Martha Palmer"
        ],
        "references": [
            266177244,
            230876740,
            230876724,
            301692408,
            285311449,
            268188805,
            267666517,
            262389533,
            251501498,
            230875924
        ]
    },
    {
        "id": 262311224,
        "title": "Improving transition-based dependency parsing with buffer transitions",
        "abstract": "In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre's arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stack-based dependency parser.",
        "date": "July 2012",
        "authors": [
            "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
            "Carlos G\u00f3mez-Rodr\u00edguez"
        ],
        "references": [
            237533325,
            234809510,
            234802992,
            228916842,
            228708442,
            221102476,
            244444395,
            238319783,
            234829084,
            228715647
        ]
    },
    {
        "id": 262240815,
        "title": "Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection",
        "abstract": "This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, left-to-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. We believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further.",
        "date": "July 2012",
        "authors": [
            "Jinho D. Choi",
            "Martha Palmer"
        ],
        "references": [
            262240518,
            230876724,
            221629881,
            220946912,
            215601307,
            288905957,
            228981212,
            220874670,
            220873227,
            45861561
        ]
    },
    {
        "id": 270878151,
        "title": "A Dynamic Oracle for Arc-Eager Dependency Parsing",
        "abstract": "The standard training regime for transition-based dependency parsers makes use of an oracle, which predicts an optimal transition sequence for a sentence and its gold tree. We present an improved oracle for the arc-eager transition system, which provides a set of optimal transitions for every valid parser configuration, including configurations from which the gold tree is not reachable. In such cases, the oracle provides transitions that will lead to the best reachable tree from the given configuration. The oracle is efficient to implement and provably correct. We use the oracle to train a deterministic left-to-right dependency parser that is less sensitive to error propagation, using an online training procedure that also explores parser configurations resulting from non-optimal sequences of transitions. This new parser outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets.",
        "date": "December 2012",
        "authors": [
            "Yoav Goldberg",
            "Joakim Nivre"
        ],
        "references": [
            228774597,
            228708442,
            220874766,
            45860555
        ]
    },
    {
        "id": 262389533,
        "title": "Vine pruning for efficient multi-pass dependency parsing",
        "abstract": "Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.",
        "date": "June 2012",
        "authors": [
            "Alexander M. Rush",
            "Slav Petrov"
        ],
        "references": [
            237533325,
            228916842,
            221101724,
            220874833,
            220873521,
            220873101,
            220320388,
            220320111,
            220017637,
            200044364
        ]
    },
    {
        "id": 262319307,
        "title": "Structured perceptron with inexact search",
        "abstract": "Most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems. This leads to the routine use of approximate inference such as beam search but there is not much theory behind it. Based on the structured perceptron, we propose a general framework of \"violation-fixing\" perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions. This framework subsumes and justifies the popular heuristic \"early-update\" for perceptron with beam search (Collins and Roark, 2004). We also propose several new update methods within this framework, among which the \"max-violation\" method dramatically reduces training time (by 3 fold as compared to early-update) on state-of-the-art part-of-speech tagging and incremental parsing systems.",
        "date": "June 2012",
        "authors": [
            "Liang Huang",
            "Suphan Fayong",
            "Yang Guo"
        ],
        "references": [
            234814829,
            221345875,
            220320388,
            220017637,
            2930135,
            228057950,
            221619279,
            221618559,
            221013109,
            220946924
        ]
    },
    {
        "id": 262294216,
        "title": "Spectral dependency parsing with latent variables",
        "abstract": "Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs. Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods, which can get stuck in local minima. In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-the-art parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker.",
        "date": "July 2012",
        "authors": [
            "Paramveer Dhillon",
            "Jordan Rodu",
            "Michael Collins",
            "Dean P. Foster"
        ],
        "references": [
            266439415,
            262207624,
            221965836,
            221618733,
            220873863,
            220873314,
            220320111,
            220017637,
            262408917,
            245123229
        ]
    },
    {
        "id": 261843006,
        "title": "A graph kernel for protein-protein interaction extraction",
        "abstract": "In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced all-dependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-the-art performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect cross-validation strategies and problems related to comparing F-score results achieved on different evaluation resources.",
        "date": "June 2008",
        "authors": [
            "Antti Airola",
            "Sampo Pyysalo",
            "Jari Bj\u00f6rne",
            "Tapio Pahikkala"
        ],
        "references": [
            221497506,
            221013036,
            275967437,
            275967213,
            271451887,
            246668580,
            245587772,
            221995504,
            221619652,
            221324370
        ]
    },
    {
        "id": 250377014,
        "title": "A Semi-Supervised Approach To Learning Relevant Protein-Protein Interaction Articles",
        "abstract": "This paper describes an Information Extraction system that can be used to identify articles containing protein-protein interactions. The approach relies on the automatic acquisition of depen- dency tree based patterns which can be used to identify these interactions and consequently select relevant documents. Evaluation shows an F-Score performance of approximately 64%.",
        "date": "January 2007",
        "authors": [
            "Mark A. Greenwood",
            "Mark Stevenson"
        ],
        "references": [
            228684325,
            228644054,
            250418362,
            2720890
        ]
    },
    {
        "id": 242080024,
        "title": "Challenges in Mapping of Syntactic Representations for Framework Independent Parser Evaluation",
        "abstract": "We explore some of the issues and chal- lenges created by the incompatibility of di- verse representation schemes for syntactic parsing. In particular, we examine the problem of output format conversion for evaluation of parsers that use different formalisms. We discuss recent related ef- forts, and present an evaluation of different parsers that use representations that vary not only in formalisms, but also in depth of syntactic information. We attempt to com- pare these parsers in a domain widely used for parser evaluation, the Wall Street Jour- nal section of the Penn Treebank, and in the academic biomedical literature, where the use of parsing technologies is expected to contribute in practical applications, such as information extraction and text mining.",
        "date": "January 2008",
        "authors": [
            "Kenji Sagae",
            "Yusuke Miyao",
            "Takuya Matsuzaki",
            "Jun'ichi Tsujii"
        ],
        "references": [
            250298670,
            235990461,
            228873235,
            228707544,
            221324467,
            319394380,
            254681364,
            242635581,
            238606254,
            221324370
        ]
    },
    {
        "id": 228526428,
        "title": "Intrinsic versus extrinsic evaluations of parsing systems",
        "abstract": "A wide range of parser and/or grammar evaluation methods have been reported in the literature. However, in most cases these evaluations take the parsers in-dependently (intrinsic evaluations), and only in a few cases has the effect of different parsers in real applications been measured (extrinsic evaluations). This paper compares two evaluations of the Link Grammar parser and the Conexor Functional Dependency Gram-mar parser. The parsing systems, de-spite both being dependency-based, re-turn different types of dependencies, making a direct comparison impossi-ble. In the intrinsic evaluation, the accu-racy of the parsers is compared indepen-dently by converting the dependencies into grammatical relations and using the methodology of Carroll et al. (1998) for parser comparison. In the extrinsic eval-uation, the parsers' impact in a practi-cal application is compared within the context of answer extraction. The dif-ferences in the results are significant.",
        "date": "January 2003",
        "authors": [
            "Diego Molla Aliod",
            "Ben Hutchinson"
        ],
        "references": [
            235990461,
            224890854,
            220817598,
            220482445,
            319393968,
            266316624,
            243672440,
            234816750,
            234789212,
            220708278
        ]
    },
    {
        "id": 313244022,
        "title": "Tregex and tsurgeon: Tools for querying and manipulating tree data structures",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "R. Levy",
            "G. Andrew"
        ],
        "references": []
    },
    {
        "id": 260065124,
        "title": "Lexical-Functional Syntax",
        "abstract": "\"Lexical-Functional Syntax\" is the definitive text for Lexical-Functional Grammar in the field of syntax.",
        "date": "January 2001",
        "authors": [
            "Joan Bresnan"
        ],
        "references": []
    },
    {
        "id": 234820797,
        "title": "The third PASCAL recognizing textual entailment challenge",
        "abstract": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.",
        "date": "June 2007",
        "authors": [
            "Danilo Giampiccolo",
            "Bernardo Magnini",
            "Ido Dagan",
            "Bill Dolan"
        ],
        "references": [
            254396054,
            243787442,
            237320057,
            237128329,
            228907970,
            228349655,
            220946933,
            220817467,
            220441049,
            2930135
        ]
    },
    {
        "id": 230876748,
        "title": "Background to FrameNet",
        "abstract": "This article presents general background information about the FrameNet project, including an introduction to its basic assumptions and goals, a description of its precursors, and information about its evolution during the six years of the project. The companion articles in this special issue of IJL describe various aspects of the project in greater detail.",
        "date": "September 2003",
        "authors": [
            "Charles Fillmore",
            "Christopher Johnson",
            "Miriam R L Petruck"
        ],
        "references": []
    },
    {
        "id": 228584658,
        "title": "Discovery of Inference Rules for Question-Answering",
        "abstract": "One of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts. While humans appear to use infer-ence rules such as \"X writes Y\" implies \"X is the author of Y\" in answering questions, such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them. In this paper, we present an unsupervised algorithm for discovering inference rules from text. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same con-texts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus. Essentially, if two paths tend to link the same set of words, we hypothesize that their meanings are similar. We use examples to show that our system discovers many inference rules easily missed by humans.",
        "date": "December 2001",
        "authors": [
            "Dekang Lin",
            "Patrick Pantel"
        ],
        "references": [
            313727971,
            234825588,
            221606157,
            221102156,
            221101682,
            37441677,
            2819889,
            2564001,
            2477223,
            2473382
        ]
    },
    {
        "id": 235990461,
        "title": "Parser evaluation: a survey and a new proposal",
        "abstract": "Abstract We present a critical overview of the state-of-the-art in parser evaluation methodologies and metrics. A discussion of their relative strengths and weaknesses motivates a new\u2014and we claim more informative and generally applicable\u2014technique of measuring parser accuracy, based on the use of grammatical relations. We conclude with some preliminary results of experiments in which we use this new scheme to evaluate a robust parser of English.",
        "date": "May 1998",
        "authors": [
            "John Carroll",
            "Ted Briscoe",
            "Antonio Sanfilippo"
        ],
        "references": [
            266452571,
            230876180,
            230876175,
            220873033,
            261344688,
            243784984,
            242635581,
            242402280,
            242396917,
            220874091
        ]
    },
    {
        "id": 232031795,
        "title": "Dependency Parsing with an Extended Finite-State Approach",
        "abstract": "This article presents a dependency parsing scheme using an extended finite-state approach. The parser augments input representation with \"channels\" so that links representing syntactic depen-dency relations among words can be accommodated and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links and no independent items except senten-tial head are filtered via finite-state filters. We have applied the parser to dependency parsing of Turkish.",
        "date": "December 2003",
        "authors": [
            "Kemal Oflazer"
        ],
        "references": [
            230876366,
            230876176,
            247442902,
            242609452,
            230876316,
            230876102,
            230876094,
            230875914,
            228057764,
            225201659
        ]
    },
    {
        "id": 277288972,
        "title": "Memory-Based Language Processing. Introduction to the . . .",
        "abstract": "Memory-Based Language Processing (MBLP) views language processing as being based on the direct reuse of previous experience rather than on the use of rules or other structures extracted from that experience. In such a framework, language acquisition is modeled as the storage of examples in memory, and language processing as analogical or similarity-based reasoning. We briefly discuss the properties and origins of this family of techniques, and provide an overview of current approaches and issues. 1 Empirical Natural Language Processing Natural Language Processing (NLP) studies the knowledge representation and problem solving issues involved in learning, producing, and understanding language. Language Technology, or Language Engineering, uses the formalisms and theories developed within NLP in applications ranging from spelling error correction to machine translation and automatic extraction of knowledge from text. Although the origins of NLP are both logical and statistical, as in o...",
        "date": "January 1999",
        "authors": [
            "Walter Daelemans"
        ],
        "references": []
    },
    {
        "id": 245588735,
        "title": "Bracketing Guidelines for Treebank II Style",
        "abstract": "",
        "date": "January 1995",
        "authors": [
            "Ann Bies",
            "Mark Fergusona",
            "Karen Katz",
            "Robert Macintyre"
        ],
        "references": []
    },
    {
        "id": 239056767,
        "title": "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties",
        "abstract": ": The discrimination problem (two population case) may be defined as follows: e random variable Z, of observed value z, is distributed over some space (say, p-dimensional) either according to distribution F, or according to distribution G. The problem is to decide, on the basis of z, which of the two distributions Z has.",
        "date": "December 1989",
        "authors": [
            "Evelyn Fix",
            "Jr. J. L. Hodges"
        ],
        "references": [
            267170529,
            216300608,
            31102849
        ]
    },
    {
        "id": 238663823,
        "title": "The Danish Dependency Treebank and the DTAG Treebank Tool",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "Matthias Trautner Kromann"
        ],
        "references": [
            208033013,
            2566782
        ]
    },
    {
        "id": 225879575,
        "title": "Dependency-based evaluation of MINIPAR",
        "abstract": "In this paper, we first present a dependency-based method for parser evaluation. We then use the method to evaluate a broad-coverage parser, called MINIPAR, with the SUSANNE corpus. The method allows us to evaluate not only the overall performance of the parser, but also its performance with respect to different grammatical relationships and phenomena. The evaluation results show that MINIPAR is able to cover about 79% of the dependency relationships in the SUSANNE corpus with about 89% precision. KeywordsEnglish\u2013Parser Evaluation\u2013Dependency\u2013Susanne Corpus",
        "date": "January 2003",
        "authors": [
            "Dekang Lin"
        ],
        "references": [
            220873033,
            220817598,
            220017637,
            31441225,
            2297137,
            321594141,
            312607081,
            267666517,
            260239801,
            247643110
        ]
    },
    {
        "id": 230876277,
        "title": "Probabilistic Constraints and Syntactic Ambiguity Resolution",
        "abstract": "Natural languages contain probabilistic constraints that influence the resolution of ambiguities. Current models of sentence processing agree that probabilistic constraints affect syntactic ambiguity resolution, but there has been little investigation of the constraints themselves-what they are, how they differ in their effects on processing, and how they interact with one another. Three different types of probabilistic constraints were investigated: \u201cpre-ambiguity\u201d plausibility information, information about verb argument structure frequencies, and \u201cpost-ambiguity\u201d constraints that arrive after the introduction of the ambiguity but prior to its disambiguation. Reading times for syntactically ambiguous sentences were compared to reading times for unambiguous controls in three self-paced reading experiments. All three kinds of constraints were found to be helpful, and when several constraints converged, ambiguity resolution was facilitated compared to when constraints conflicted. The importance of these constraint interactions for ambiguity resolution models is discussed.",
        "date": "May 1994",
        "authors": [
            "Maryellen Coles MacDonald"
        ],
        "references": [
            263916104,
            230876549,
            313215209,
            297120907,
            257255648,
            243641413,
            237224496,
            232500958,
            232488655,
            232455270
        ]
    },
    {
        "id": 230875913,
        "title": "The rational analysis of inquiry: the case of parsing",
        "abstract": "",
        "date": "January 1998",
        "authors": [
            "Nick Chater",
            "Matthew W Crocker",
            "Martin J Pickering"
        ],
        "references": []
    },
    {
        "id": 308468610,
        "title": "Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition",
        "abstract": "",
        "date": "January 2000",
        "authors": [
            "D. Jurafsky",
            "James H. Martin",
            "A. Kehler",
            "K. Vander Linden"
        ],
        "references": []
    },
    {
        "id": 305263025,
        "title": "An Efficient Context-Free Parsing Algorithm",
        "abstract": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick. \u00a9 1983, ACM. All rights reserved.",
        "date": "January 1983",
        "authors": [
            "J. Earley"
        ],
        "references": [
            220425018,
            243508968,
            234800480,
            222443906,
            222191456,
            220430899,
            4355762,
            3476285
        ]
    },
    {
        "id": 243673325,
        "title": "Structural Change and Reanalysis Difficulty in Language Comprehension",
        "abstract": "Many theories of parsing predict that the difficulty of syntactic reanalysis depends on the type of structural change involved. However, most existing experimental data show that reanalysis difficulty is affected by nonstructural factors like plausibility and verb bias, whereas claims about structural change are typically based on intuition alone. We report two self-paced reading experiments which demonstrate clear differences in the magnitude of garden path effects associated with different types of structural change. However, difficulty of reanalysis was not affected by the position of the head noun within the ambiguous phrase. We interpret these results in terms of theories of structural change such as Sturt and Crocker (1996).",
        "date": "January 1999",
        "authors": [
            "Patrick Sturt",
            "Martin J Pickering",
            "Matthew W Crocker"
        ],
        "references": [
            289598699,
            278608531,
            247514302,
            243700483,
            238663421,
            237386381,
            232530192,
            230876377,
            225537139,
            222337307
        ]
    },
    {
        "id": 239051972,
        "title": "Recovery from misanalyses of garden-path sentences*1",
        "abstract": "In five experiments we examined the way in which readers reanalyze garden-path sentences, using grammaticality judgments as the dependent measure. The stimuli were twoclause sentences containing an ambiguous noun phrase which could function as either the object of the first clause or the subject of the second. Prior research has shown that the former analysis is generally preferred. In the first two experiments, we varied the number of words in the ambiguous phrase and found that reanalysis of garden-path sentences was more difficult with a longer ambiguous phrase. The third experiment established that this effect of phrase length is not attributable to the greater syntactic complexity of longer phrases. The fourth and fifth experiments demonstrated that the effect of phrase length is attributable to increasing the distance from the head of the ambiguous phrase to the disambiguating word of the graden-path sentence: Ambiguous phrases made long by the addition of prenominal adjectives were easy for the parser to reanalyze, but phrases made long by the addition of postnominal modifying prepositional phrases (Experiment 3) or relative clauses (Experiments 4 and 5) were hard for the parser to reanalyze. From these results, we argue that sentence comprehension requires the creation of phrase structure and the assignment of thematic roles to phrases, with the assignment taking place at the phrasal head. Reanalysis is affected by the ease with which thematic roles can be reassigned to misanalyzed phrases.",
        "date": "December 1991",
        "authors": [
            "Fernanda Ferreira",
            "John M Henderson"
        ],
        "references": [
            233460075,
            223408901,
            222472632,
            27401728,
            323826465,
            243762918,
            242510504,
            239059579,
            232569242,
            232524615
        ]
    },
    {
        "id": 228669927,
        "title": "On the complexity of non-projective data-driven dependency parsing",
        "abstract": "In this paper we investigate several non-projective parsing algorithms for depen-dency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for non-local information, and present several hard-ness results. This suggests that it is unlikely that exact non-projective dependency pars-ing is tractable for any model richer than the edge-factored model.",
        "date": "July 2007",
        "authors": [
            "Ryan Mcdonald",
            "Giorgio Satta"
        ],
        "references": [
            265072707,
            237533325,
            228916842,
            228846047,
            228822214,
            228707544,
            221012626,
            220875117,
            220874766,
            220874243
        ]
    },
    {
        "id": 222506919,
        "title": "Principles and Implementation of Deductive Parsing",
        "abstract": "We present a system for generating parsers based directly on the metaphor of parsing as deduction. Parsing algorithms can be represented directly as deduction systems, and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser. The method generalizes easily to parsers for augmented phrase structure formalisms, such as definite-clause grammars and other logic grammar formalisms, and has been used for rapid prototyping of parsing algorithms for a variety of formalisms including variants of tree-adjoining grammars, categorial grammars, and lexicalized context-free grammars.",
        "date": "July 1995",
        "authors": [
            "Stuart M. Shieber",
            "Yves Schabes",
            "Nanediri Chathumi Fernando"
        ],
        "references": [
            234818590,
            234785806,
            230876175,
            220898271,
            45599475,
            45598828,
            2481913,
            319395020,
            319393691,
            305261981
        ]
    },
    {
        "id": 221013277,
        "title": "First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests.",
        "abstract": "Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the ex- pected hypothesis length or feature counts) over packed forests of translations (lat- tices or hypergraphs). We then introduce a novel second-order expectation semir- ing, which computes second-order statis- tics (e.g., the variance of the hypothe- sis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic anneal- ing, active learning, and semi-supervised learning, where gradient descent optimiza- tion requires computing the gradient of en- tropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point.",
        "date": "August 2009",
        "authors": [
            "Zhifei Li",
            "Jason Eisner"
        ],
        "references": [
            234819841,
            221618545,
            221013262,
            220997296,
            220947143,
            220874716,
            220874513,
            220874221,
            220873948,
            2588204
        ]
    },
    {
        "id": 242375525,
        "title": "Building the Italian Syntactic-Semantic Treebank",
        "abstract": "The paper reports on the design and construction of a multi-layered corpus of Italian, annotated at the syntactic and lexico-semantic levels, whose development is supported by dedicated software augmented with an intelligent interface. The issue of evaluating this type of resource is also addressed.",
        "date": "January 2003",
        "authors": [
            "Simonetta Montemagni",
            "Francesco Barsotti",
            "Marco Battista",
            "Nicoletta Calzolari"
        ],
        "references": [
            243771060,
            243765671,
            312607081,
            254893959,
            252332293,
            247921836,
            243765291,
            243647924,
            243526172,
            242267944
        ]
    },
    {
        "id": 220874766,
        "title": "Pseudo-Projective Dependency Parsing.",
        "abstract": "In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective depen- dency structures. We show how a data- driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transforma- tion techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle non- projective constructions with a precision sufficient to yield a significant improve- ment in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech.",
        "date": "January 2005",
        "authors": [
            "Joakim Nivre",
            "Jens Nilsson"
        ],
        "references": [
            234807236,
            232031795,
            228916842,
            228822214,
            228708442,
            228707544,
            221101479,
            238663823,
            230876316,
            230876094
        ]
    },
    {
        "id": 251347641,
        "title": "The Prague Dependency Treebank: A Three-Level Annotation Scenario",
        "abstract": "The availability of annotated data (with as rich and \u201cdeep\u201d annotation as possible) is desirable in any new developments. Textual data are being used for so-called training phase of various empirical methods solving various problems in the field of computational linguistics. While there are many methods that use texts in their plain (or raw) form (in most cases for so-called unsupervised training), more accurate results may be obtained if annotated corpora are available. The data annotation itself is a complex task. While morphologically annotated corpora (pioneered by Henry Ku\u010dera in the 60\u2019s) are now available for English and other languages, syntactically annotated corpora are rare. Inspired by the Penn Treebank, the most widely used syntactically annotated corpus of English, we decided to develop a similarly sized corpus of Czech with a rich annotation scheme.",
        "date": "January 2003",
        "authors": [
            "Alena B\u00f6hmov\u00e1",
            "Jan Haji\u010d",
            "Eva Haji\u010dov\u00e1",
            "Barbora Hladka"
        ],
        "references": [
            265622590,
            226727962,
            220874161,
            220017637,
            2683860,
            2522415,
            312607081,
            307175231,
            307174938,
            307174813
        ]
    },
    {
        "id": 238319783,
        "title": "Sinica Treebank: Design Criteria, Representational Issues and Implementation",
        "abstract": "",
        "date": "August 2003",
        "authors": [
            "Keh-jiann Chen",
            "Chu-Ren Huang",
            "F. Y. Chen",
            "C. C. Luo"
        ],
        "references": []
    },
    {
        "id": 234829084,
        "title": "Multilingual dependency analysis with a two-stage discriminative parser",
        "abstract": "We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.",
        "date": "June 2006",
        "authors": [
            "Ryan McDonald",
            "Kevin Lerman",
            "Fernando Pereira"
        ],
        "references": [
            267797457,
            260291394,
            237533325,
            232031795,
            228916842,
            228774597,
            220875062,
            220873277,
            220873033,
            220500224
        ]
    },
    {
        "id": 228822214,
        "title": "A statistical constraint dependency grammar (CDG) parser",
        "abstract": "CDG represents a sentence's grammatical structure as assignments of dependency relations to func-tional variables associated with each word in the sentence. In this paper, we describe a statistical CDG (SCDG) parser that performs parsing incre-mentally and evaluate it on the Wall Street Jour-nal Penn Treebank. Using a tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves a parsing accuracy comparable to several state-of-the-art context-free grammar (CFG) based statistical parsers using a dependency-based eval-uation metric. Factors contributing to the SCDG parser's performance are analyzed.",
        "date": "July 2004",
        "authors": [
            "Wen Wang",
            "Mary Harper"
        ],
        "references": [
            245458252,
            228817682,
            220873033,
            260730920,
            230876149,
            220355517,
            220343911,
            27233617,
            4087378,
            4084648
        ]
    },
    {
        "id": 221345468,
        "title": "Structured Output Learning with Indirect Supervision",
        "abstract": "We present a novel approach for structure prediction that addresses the difficulty of obtaining labeled structures for training. We observe that structured output problems often have a companion learning problem of determining whether a given input possesses a good structure. For example, the companion problem for the part-ofspeech (POS) tagging task asks whether a given sequence of words has a corresponding sequence of POS tags that is \u201clegitimate\u201d. While obtaining direct supervision for structures is difficult and expensive, it is often very easy to obtain indirect supervision from the companion binary decision problem. In this paper, we develop a large margin framework that jointly learns from both direct and indirect forms of supervision. Our experiments exhibit the significant contribution of the easy-toget indirect binary supervision on three important NLP structure learning problems. 1.",
        "date": "September 2010",
        "authors": [
            "Ming-Wei Chang",
            "Vivek Srikumar",
            "Dan Goldwasser",
            "Dan Roth"
        ],
        "references": []
    },
    {
        "id": 221102262,
        "title": "HMM-Based Word Alignment in Statistical Translation.",
        "abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.",
        "date": "January 1996",
        "authors": [
            "Stephan Vogel",
            "Hermann Ney",
            "Christoph Tillmann"
        ],
        "references": [
            220355417,
            2476616,
            246778890,
            230876149,
            221562673,
            220049264
        ]
    },
    {
        "id": 234802196,
        "title": "Learning Better Monolingual Models with Unannotated Bilingual Text",
        "abstract": "This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the first, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity recognition, using bilingual predictors increases F1 by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F1 by 14.6%. For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.",
        "date": "June 2010",
        "authors": [
            "David Burkett",
            "Slav Petrov",
            "John Blitzer",
            "Dan Klein"
        ],
        "references": [
            221617699,
            221404300,
            220874004,
            220873863,
            220817534,
            220597287,
            220017637,
            215470704,
            4311105,
            3998230
        ]
    },
    {
        "id": 221013260,
        "title": "Bilingual Parsing with Factored Estimation: Using English to Parse Korean.",
        "abstract": "We describe how simple, commonly understood statisti- cal models, such as statistical dependency parsers, proba- bilistic context-free grammars, and word-to-word trans- lation models, can be effectively combined into a uni- fied bilingual parser that jointly searches for the best En- glish parse, Korean parse, and word alignment, where these hidden structures all constrain each other. The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter es- timation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very lim- ited labeled data.",
        "date": "January 2004",
        "authors": [
            "David A. Smith",
            "Noah A. Smith"
        ],
        "references": [
            230875890,
            220874539,
            220817534,
            220017613,
            2927500,
            2876877,
            2839498,
            2537796,
            2537544,
            2476940
        ]
    },
    {
        "id": 226080200,
        "title": "A Latent Variable Model for Generative Dependency Parsing",
        "abstract": "Dependency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-2006 shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of Nivre et al. (2006) and the minimum spanning tree parser of McDonald et al.",
        "date": "October 2010",
        "authors": [
            "Ivan Titov",
            "James Henderson"
        ],
        "references": [
            313037830,
            312607081,
            262349927,
            262184200,
            248208758,
            247921836,
            244444395,
            244150117,
            242370758,
            238319783
        ]
    },
    {
        "id": 228824508,
        "title": "Chinese Word Segmentation with Maximum Entropy and N-gram Language Model",
        "abstract": "This paper presents the Chinese word seg-mentation systems developed by Speech and Hearing Research Group of Na-tional Laboratory on Machine Perception (NLMP) at Peking University, which were evaluated in the third International Chi-nese Word Segmentation Bakeoff held by SIGHAN. The Chinese character-based maximum entropy model, which switches the word segmentation task to a classi-fication task, is adopted in system de-veloping. To integrate more linguistics information, an n-gram language model as well as several post processing strate-gies are also employed. Both the closed and open tracks regarding to all four cor-pora MSRA, UPUC, CITYU, CKIP are involved in our systems' evaluation, and good performance are achieved. Espe-cially, in the closed track on MSRA, our system ranks 1st.",
        "date": "August 2006",
        "authors": [
            "Wang Xinhao",
            "Lin Xiaojun",
            "Dianhai Yu",
            "Tian Hao"
        ],
        "references": [
            228642175,
            221013087
        ]
    },
    {
        "id": 221012972,
        "title": "Experiments with a Higher-Order Projective Dependency Parser.",
        "abstract": "We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech. 1",
        "date": "January 2007",
        "authors": [
            "Xavier Carreras"
        ],
        "references": [
            242375525,
            228774597,
            220873101,
            220017637,
            200179371,
            47465198,
            2799191,
            2551651,
            251347641,
            238319783
        ]
    },
    {
        "id": 221012880,
        "title": "Dependency Parsing by Belief Propagation.",
        "abstract": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and em- pirically efficient. Even with second-order features or la- tent variables, which would make exact parsing consider- ably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features sig- nificantly improve parse accuracy over exact first-order methods. Incorporating additional features would in- crease the runtime additively rather than multiplicatively.",
        "date": "January 2008",
        "authors": [
            "David A. Smith",
            "Jason Eisner"
        ],
        "references": [
            270451739,
            221013324,
            221012626,
            220875081,
            220873229,
            220873101,
            220813650,
            220482445,
            220345624,
            200179371
        ]
    },
    {
        "id": 221618388,
        "title": "Automatic Acquisition and Efficient Representation of Syntactic Structures.",
        "abstract": "The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category (1) has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by us- ing mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity.",
        "date": "January 2002",
        "authors": [
            "Zach Solan",
            "Eytan Ruppin",
            "David Horn",
            "Shimon Edelman"
        ],
        "references": [
            19141547,
            2540157,
            230876241,
            226967854,
            222449846,
            32228181,
            2363157,
            2361343
        ]
    },
    {
        "id": 221102754,
        "title": "ABL: Alignment-based learning",
        "abstract": "This paper introduces a new type of grammar learning algorithm, inspired by string edit distance (Wagner and Fischer, 1974). The algorithm takes a corpus of flat sentences as input and returns a corpus of labelled, bracketed sentences. The method works on pairs of unstructured sentences that have one or more words in common. When two sentences are divided into parts that are the same in both sentences and parts that are different, this information is used to find parts that are interchangeable. These parts are taken as possible constituents of the same type. After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents. This method was used to bootstrap structure on the ATIS corpus (Marcus et al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). While the results are encouraging (we obtained up to 89.25 % non-crossing brackets precision), this paper will point out some of the shortcomings of our approach and will suggest possible solutions.",
        "date": "January 2000",
        "authors": [
            "Menno van Zaanen"
        ],
        "references": [
            220814601,
            37705036,
            312607081,
            270114538,
            248721580,
            230875831,
            222491221,
            220480255,
            220431230,
            200806382
        ]
    },
    {
        "id": 289827888,
        "title": "Distributional Part-of-Speech Tagging",
        "abstract": "This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.",
        "date": "March 1995",
        "authors": [
            "Hinrich Sch\u00fctze"
        ],
        "references": [
            3505297,
            327221114,
            243776118,
            242382431,
            230875928,
            222635695,
            222449846,
            220874901,
            220355497,
            35044178
        ]
    },
    {
        "id": 246981102,
        "title": "The Noun Phrase in its Sentential Aspect",
        "abstract": "",
        "date": "January 1987",
        "authors": [
            "Steven Abney"
        ],
        "references": []
    },
    {
        "id": 239535201,
        "title": "Finding structure in language",
        "abstract": "",
        "date": "January 1993",
        "authors": [
            "S. P. Finch"
        ],
        "references": []
    },
    {
        "id": 230875831,
        "title": "Trainable grammars for speech recognition",
        "abstract": "Algorithms which are based on modelingspeech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component.",
        "date": "June 1979",
        "authors": [
            "J. K. Baker"
        ],
        "references": []
    },
    {
        "id": 221523376,
        "title": "Inducing Probabilistic Grammars by Bayesian Model Merging.",
        "abstract": "",
        "date": "January 1994",
        "authors": [
            "Andreas Stolcke",
            "Stephen M. Omohundro"
        ],
        "references": [
            242394905,
            3192081,
            243764358,
            234776814,
            230875831,
            228057764,
            222274266,
            221995817,
            220565789,
            220312496
        ]
    },
    {
        "id": 307915669,
        "title": "Semantic Tagging of Medical Narratives with Top Level Concepts from SNOMED CT Healthcare Data Standard",
        "abstract": "",
        "date": "March 2012",
        "authors": [
            "Saman Hina",
            "Eric Atwell",
            "Owen Johnson"
        ],
        "references": [
            228857931,
            220482445,
            11535553,
            8520140,
            226020357,
            221606983,
            26744274,
            11535434,
            8915994,
            7593534
        ]
    },
    {
        "id": 286920377,
        "title": "Multiword Expressions in NLP: General Survey and a Special Case of Verb-Noun Constructions",
        "abstract": "This chapter presents a survey of contemporary NLP research on Multiword Expressions (MWEs). MWEs pose a huge problem to precise language processing due to their idiosyncratic nature and diversity of their semantic, lexical, and syntactical properties. The chapter begins by considering MWEs definitions, describes some MWEs classes, indicates problems MWEs generate in language applications and their possible solutions, presents methods of MWE encoding in dictionaries and their automatic detection in corpora. The chapter goes into more detail on a particular MWE class called Verb-Noun Constructions (VNCs). Due to their frequency in corpus and unique characteristics, VNCs present a research problem in their own right. Having outlined several approaches to VNC representation in lexicons, the chapter explains the formalism of Lexical Function as a possible VNC representation. Such representation may serve as a tool for VNCs automatic detection in a corpus. The latter is illustrated on Spanish material applying some supervised learning methods commonly used for NLP tasks.",
        "date": "August 2013",
        "authors": [
            "Alexander Gelbukh",
            "Olga Kolesnikova"
        ],
        "references": [
            295636765,
            295636700,
            290987024,
            262252069,
            319394956,
            279395438,
            269953350,
            248021977,
            247790062,
            247405710
        ]
    },
    {
        "id": 267379415,
        "title": "Comprendre les effets des erreurs d'annotations des plates-formes de TAL",
        "abstract": "Les r\u00e9sultats des analyses des outils de TAL sont souvent des annotations qui carac-t\u00e9risent les s\u00e9quences des textes analys\u00e9s. Ces annotations sont dites erron\u00e9es lorsque leurs valeurs diff\u00e8rent des valeurs attribu\u00e9es par un expert. Des architectures innovantes sont aujour-d'hui propos\u00e9es pour annoter et corriger simultan\u00e9ment des annotations de diff\u00e9rentes cat\u00e9go-ries. Mais la complexit\u00e9 des calculs requis limite le nombre d'annotations r\u00e9ellement int\u00e9gr\u00e9es. Nous \u00e9tudions ici une alternative conservant l'architecture standard de traitement en cascade. Nous montrons, sur la r\u00e9solution des anaphores, que la mod\u00e9lisation de l'incertitude des annotations permet de limiter l'impact des annotations erron\u00e9es, d'int\u00e9grer toutes les annotations n\u00e9cessaires \u00e0 l'inf\u00e9rence et de diff\u00e9rer la r\u00e9vision des erreurs \u00e0 un post-traitement. ABSTRACT. Outputs of NLP tools can be regarded as sets of annotations of predefined characters included in a processed document. These annotations are said erroneous if they disagree with the annotations given by the experts. Innovative architectures have been proposed to annotate and revise annotations by processing errors in various levels of linguistic annotations simultaneously. But the computational complexity limits the number of the annotations they can effectively handle. We study an alternative way which keeps the standard cascade pipeline architecture. We show, on the anaphora resolution, that modeling the reliability of the annotations helps to attenuate the impact of the noisy ones, to integrate into the pipeline all annotations needed to fulfill the tasks, and to postpone the correction of errors in a latter stage. MOTS-CL\u00c9S : anaphore, r\u00e9seau bay\u00e9sien, annotation erron\u00e9e, plate-forme d'annotation.",
        "date": "January 2012",
        "authors": [
            "Davy Weissenbacher",
            "Adeline Nazarenko"
        ],
        "references": [
            255673170,
            253291509,
            226852999,
            301689917,
            268041802,
            243765278,
            228828447,
            222700598,
            221345848,
            221314705
        ]
    },
    {
        "id": 262368557,
        "title": "A bottom-up approach to sentence ordering for multi-document summarization",
        "abstract": "Ordering information is a difficult but important task for applications generating natural-language text. We present a bottom-up approach to arranging sentences extracted for multi-document summarization. To capture the association and order of two textual segments (eg, sentences), we define four criteria, chronology, topical-closeness, precedence, and succession. These criteria are integrated into a criterion by a supervised learning approach. We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged. Our experimental results show a significant improvement over existing sentence ordering strategies.",
        "date": "July 2006",
        "authors": [
            "Danushka Bollegala",
            "Naoaki Okazaki",
            "Mitsuru Ishizuka"
        ],
        "references": [
            229060015,
            228694198,
            2588204,
            2537003,
            2458461,
            239546749,
            224890483,
            2927472,
            2924027,
            2882915
        ]
    },
    {
        "id": 317444544,
        "title": "Non-continuous Syntactic N-grams",
        "abstract": "In this paper, we present the concept of non-continuous syntactic n-grams. In our previous works we introduced the general concept of syntactic n-grams, i.e., n-grams that are constructed by following paths in syntactic trees. Their great advantage is that they allow introducing of the merely linguistic (syntactic) information into machine learning methods. Certain disadvantage is that previous parsing is required. We also proved that their application in the authorship attribution task gives better results than using traditional n-grams. Still, in those works we considered only continuous syntactic n-grams, i.e., the paths in syntactic trees are not allowed to have bifurcations. In this paper, we propose to remove this limitation, so we consider all sub-trees of length \u00ab of a syntactic tree as non-continuous syntactic n-grams. Note that continuous syntactic n-grams are the particular case of non-continuous syntactic n-grams. Further research should show which n-grams are more useful and in which NLP tasks. We also propose a formal manner of writing down (representing) non-continuous syntactic n-grams using parenthesis and commas, for example, \"a b [c [d, e], f]. In this paper, we also present examples of construction of non-continuous syntactic n-grams on the basis of the syntactic tree of the FreeLing and the Stanford parser.",
        "date": "December 2013",
        "authors": [
            "Grigori Sidorov"
        ],
        "references": [
            320911934,
            319043618,
            318824103,
            313562704,
            311087712,
            308073726,
            305724415,
            304849110,
            303443999,
            301404533
        ]
    },
    {
        "id": 309347291,
        "title": "Semi-markov conditional random fields for information extraction",
        "abstract": "",
        "date": "January 2004",
        "authors": [
            "S. Sarawagi",
            "W.W. Cohen"
        ],
        "references": []
    },
    {
        "id": 296425392,
        "title": "Automatic detection of mis-spelled Japanese expressions using a new method for automatic extraction of negative examples based on positive examples",
        "abstract": "We developed a method for extracting negative examples when only positive examples are given as supervised data. This method calculates the probability of occurrence of an input example, which should be judged to be positive or negative. It considers an input example that has a high probability of occurrence but does not appear in the set of positive examples as a negative example. We used this method for one of important tasks in natural language processing: automatic detection of misspelled Japanese expressions. The results showed that the method is effective. In this study, we also described two other methods we developed for the detection of misspelled expressions: a combined method and a \"leaving-one-out\" method. In our experiments, we found that these methods are also effective.",
        "date": "September 2002",
        "authors": [
            "M. Murata",
            "Hitoshi Isahara"
        ],
        "references": [
            222445347
        ]
    },
    {
        "id": 262397696,
        "title": "Linguistic structure prediction with the sparseptron",
        "abstract": "Recent advances in natural language processing bring together rich representations and scalable machine learning algorithms.",
        "date": "March 2013",
        "authors": [
            "Noah A. Smith",
            "Andr\u00e9 F. T. Martins"
        ],
        "references": [
            237533325,
            234801831,
            221996079,
            221013174,
            221013126,
            220320111,
            220017637,
            200179371,
            329475599,
            313024706
        ]
    },
    {
        "id": 262366271,
        "title": "Ontology-Driven Construction of Domain Corpus with Frame Semantics Annotations",
        "abstract": "Semantic Role Labeling plays a key role in many text mining applications. The development of SRL systems for the biomedical domain is frustrated by the lack of large domain specific corpora that are labeled with semantic roles. In this paper we proposed a method for building corpus that are labeled with semantic roles for the domain of biomedicine. The method is based on the theory of frame semantics, and uses domain knowledge provided by ontologies. By using the method, we have built a corpus for transport events strictly following the domain knowledge provided by GO biological process ontology. We compared one of our frames to a BioFrameNet frame. We also examined the gaps between the semantic classification of the target words in this domain-specific corpus and in FrameNet and PropBank/VerbNet data. The successful corpus construction demonstrates that ontologies, as a formal representation of domain knowledge, can instruct us and ease all the tasks in building this kind of corpus. Furthermore, ontological domain knowledge leads to well-defined semantics exposed on the corpus, which will be very valuable in text mining applications.",
        "date": "March 2012",
        "authors": [
            "He Tan",
            "Rajaram Kaliyaperumal",
            "Nirupama Benis"
        ],
        "references": [
            285668548,
            242582900,
            228729642,
            228342169,
            221235022,
            221234966,
            220605075,
            51403968,
            23245640,
            14727320
        ]
    },
    {
        "id": 252868319,
        "title": "Improving English-Spanish Statistical Machine Translation: Experiments in Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing",
        "abstract": "We describe the experiments of the UC Berke- ley team on improving English-Spanish ma- chine translation of news text, as part of the WMT'08 Shared Translation Task. We ex- periment with domain adaptation, combin- ing a small in-domain news bi-text and a large out-of-domain one from the Europarl corpus, building two separate phrase transla- tion models and two separate language mod- els. We further add a third phrase transla- tion model trained on a version of the news bi-text augmented with monolingual sentence- level syntactic paraphrases on the source- language side, and we combine all models in a log-linear model using minimum error rate training. Finally, we experiment with differ- ent tokenization and recasing rules, achieving 35.09% Bleu score on the WMT'07 news test data when translating from English to Span- ish, which is a sizable improvement over the highest Bleu score achieved on that dataset at WMT'07: 33.10% (in fact, by our sys- tem). On the WMT'08 English to Spanish news translation, we achieve 21.92%, which makes our team the second best on Bleu score.",
        "date": "January 2008",
        "authors": [
            "Preslav Nakov"
        ],
        "references": [
            267401278,
            262408072,
            234832376,
            228922819,
            228355180,
            2588204,
            221013037,
            2884718
        ]
    },
    {
        "id": 228846991,
        "title": "Method of selecting training data to build a compact and efficient translation model",
        "abstract": "Target task matched parallel corpora are re-quired for statistical translation model train-ing. However, training corpora sometimes include both target task matched and un-matched sentences. In such a case, train-ing set selection can reduce the size of the translation model. In this paper, we propose a training set selection method for transla-tion model training using linear translation model interpolation and a language model technique. According to the experimental results, the proposed method reduces the translation model size by 50% and improves BLEU score by 1.76% in comparison with a baseline training corpus usage.",
        "date": "January 2008",
        "authors": [
            "Keiji Yasuda",
            "Ruiqiang Zhang",
            "Hirofumi Yamamoto",
            "Eiichiro Sumita"
        ],
        "references": [
            228391161,
            221013146,
            220817420,
            220816913,
            307174896,
            232128580,
            221013037,
            220875255,
            220655257,
            2884718
        ]
    },
    {
        "id": 228740822,
        "title": "Language model adaptation for statistical machine translation based on information retrieval",
        "abstract": "Language modeling is an important part for both speech recognition and machine translation systems. Adaptation has been successfully applied to language models for speech recognition. In this paper we present experiments concerning language model adaptation for statistical machine translation. We develop a method to adapt language models using information retrieval methods. The adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.",
        "date": "January 2004",
        "authors": [
            "Matthias Eck",
            "Stephan Vogel",
            "Alex Waibel"
        ],
        "references": [
            232623883,
            3908401,
            3703273,
            221482926,
            3746915,
            2590522,
            2369987
        ]
    },
    {
        "id": 228361002,
        "title": "Using Word Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation",
        "abstract": "In this paper, we present a Bayesian Learn-ing based method to train word dependent transition models for HMM based word alignment. We present word alignment re-sults on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also con-ducted machine translation (MT) experi-ments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based ma-chine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.",
        "date": "July 2007",
        "authors": [
            "Xiaodong He"
        ],
        "references": [
            230875890,
            221102262,
            221013020,
            220874741,
            220873497,
            303517922,
            280940018,
            246324585,
            245123229,
            243770992
        ]
    },
    {
        "id": 228355180,
        "title": "Experiments in domain adaptation for statistical machine translation",
        "abstract": "The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical ma-chine translation systems to a special do-main (here: news commentary), when most of the training data is from a dif-ferent domain (here: European Parliament speeches). This paper also gives a descrip-tion of the submission of the University of Edinburgh to the shared task.",
        "date": "July 2007",
        "authors": [
            "Philipp Koehn",
            "Josh Schroeder"
        ],
        "references": [
            228355130,
            228347509,
            220874793,
            220816913,
            2884718,
            2602251
        ]
    },
    {
        "id": 228347509,
        "title": "CCG supertags in factored statistical machine translation",
        "abstract": "Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this informa-tion into the translation process. Factored translation models allow the inclusion of su-pertags as a factor in the source or target lan-guage. We show that this results in an im-provement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.",
        "date": "July 2007",
        "authors": [
            "Alexandra Birch",
            "Miles Osborne",
            "Philipp Koehn"
        ],
        "references": [
            229023614,
            228350461,
            256744933,
            238210713,
            238167802,
            234795520,
            228557160,
            228379274,
            221012595,
            220947104
        ]
    },
    {
        "id": 221013146,
        "title": "Improving Statistical Machine Translation Performance by Training Data Selection and Optimization.",
        "abstract": "Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly.",
        "date": "January 2007",
        "authors": [
            "Yajuan L\u00fc",
            "Jin Huang",
            "Qun Liu"
        ],
        "references": [
            252622163,
            228740822,
            228634956,
            228629034,
            228528075,
            228343353,
            221562550,
            221299698,
            307174896,
            221149685
        ]
    },
    {
        "id": 240911908,
        "title": "Factored Language Models for Statistical Machine Translation",
        "abstract": "Abstract Machine translation systems, as a whole, are currently not able to use the output of linguistic tools, such as part-of-speech taggers, to effectively improve translation performance. However, a new language modeling technique, Factored Language Models can incorporate the additional linguistic information that is produced by these tools. In the field of automatic speech recognition, Factored Language Models smoothed with Generalized Parallel Backoff have been shown,to significantly reduce language model perplexity. However, Factored Language Models have previously only been applied to statistical machine,translation as part of a second-pass rescoring system. In this thesis, we show that a state-of-the-art phrase-based system using factored language models with generalized parallel backoff can improve performance,over an identical system using trigram language models. These improvements,can be seen both with the use of additional word features and without. The relative gain from the Factored Language Models increases with smaller training corpora, making this approach especially useful for domains with limited data. iii Acknowledgements",
        "date": "January 2006",
        "authors": [
            "Amittai E. Axelrod"
        ],
        "references": [
            220947155
        ]
    },
    {
        "id": 221013211,
        "title": "Discriminative Corpus Weight Estimation for Machine Translation",
        "abstract": "Current statistical machine translation (SMT) systems are trained on sentence- aligned and word-aligned parallel text col- lected from various sources. Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parame- ter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down-weighing certain parts of the train- ing corpus by assigning a weight to each sentence in the training bitext so as to op- timize a discriminative objective function on a designated tuning set. This way, the proposed method can limit the negative ef- fects of low quality training data, and can adapt the translation model to the domain of interest. It is shown that such discrim- inative corpus weights can provide sig- nificant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system.",
        "date": "January 2009",
        "authors": [
            "Spyros Matsoukas",
            "Antti-Veikko I. Rosti",
            "Bing Zhang"
        ],
        "references": [
            228846991,
            228668187,
            228634956,
            228346240,
            224382272,
            221013146,
            221012593,
            220816779,
            2876877,
            2588204
        ]
    },
    {
        "id": 221364406,
        "title": "Global Training of Document Processing Systems Using Graph Transformer Networks.",
        "abstract": "We propose a new machine learning paradigm called Graph Transformer Networks that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concepts is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provides record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks per month. 1. Introduction The most common technique for building document processing systems is to partition the task into manageable subtasks, such as field detection, word segmentation, or character recognition, and to build a separate module for each one. Typically, each module is tr...",
        "date": "January 1997",
        "authors": [
            "L\u00e9on Bottou",
            "Y. Bengio",
            "Yann Lecun"
        ],
        "references": [
            220817372,
            216792874,
            247089825,
            245578967,
            245123229,
            224619451,
            221620264,
            221619988,
            220181601,
            216792834
        ]
    },
    {
        "id": 2562741,
        "title": "Long Short-Term Memory in Recurrent Neural Networks",
        "abstract": "For a long time, recurrent neural networks (RNNs) were thought to be theoretically fascinating. Unlike standard feed-forward networks RNNs can deal with arbitrary input sequences instead of static input data only. This combined with the ability to memorize relevant events over time makes recurrent networks in principal more powerful than standard feed-forward networks. The set of potential applications is enormous: any task that requires to learn how to use memory is a potential task for recurrent networks. Potential application areas include time series prediction, motor control in non-Markovian environments and rhythm detection (in music and speech). Previous successes in real world applications, with recurrent networks were limited, however, due to practical problems when long time lags between relevant events make learning dicult. For these applications conventional gradient-based recurrent network algorithms for learning to store information over extended time intervals take too long. The main reason for this failure is the rapid decay of back-propagated error. The Long Short Term Memory\" (LSTM) algorithm overcomes this and related problems by enforcing constant error ow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In this thesis we extend, analyze, and apply the LSTM algorithm. In particular, we identify two weaknesses of LSTM, oer solutions and modify the algorithm accordingly: (1) We recognize a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indenitely and eventually cause the network to break down. Our remedy is a novel, adaptiv...",
        "date": "May 2001",
        "authors": [
            "D Epartement D'informatique",
            "Ese N",
            "Pr Esent",
            "Ee Au"
        ],
        "references": []
    },
    {
        "id": 238769089,
        "title": "Findings of the 2009 Workshop on Statistical Machine Translation Chris Callison-Burch",
        "abstract": "This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.",
        "date": "January 2009",
        "authors": [
            "Chris Callison-Burch",
            "Philipp Koehn",
            "Christof Monz",
            "Josh Schroeder"
        ],
        "references": [
            262396068,
            262389001,
            262214020,
            253485955,
            253485251,
            252666388,
            228902012,
            254602794,
            252044237,
            228715647
        ]
    },
    {
        "id": 228668187,
        "title": "A study of translation edit rate with targeted human annotation",
        "abstract": "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judg-ments. Translation Edit Rate (TER) mea-sures the amount of editing that a hu-man would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU\u2014even when BLEU is given human-targeted references. Our results in-dicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judg-ments as well as\u2014or better than\u2014a sec-ond human judgment does.",
        "date": "January 2006",
        "authors": [
            "Matthew Snover",
            "Bonnie Dorr",
            "Richard Schwartz",
            "Linnea Micciulla"
        ],
        "references": [
            231901086,
            245329863,
            244435287,
            234812513,
            228979589,
            225917194,
            221314102,
            220427293,
            2936280,
            2913503
        ]
    },
    {
        "id": 228346240,
        "title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
        "abstract": "Meteor is an automatic metric for Ma-chine Translation evaluation which has been demonstrated to have high levels of corre-lation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 work-shop. This paper recaps the technical de-tails underlying the metric and describes re-cent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evalua-tion of MT output in Spanish, French and German, in addition to English.",
        "date": "July 2007",
        "authors": [
            "Alon Lavie",
            "Abhaya Agarwal"
        ],
        "references": [
            228668187,
            220947076,
            2938266,
            2588204,
            244435287,
            220839711,
            49488795,
            2900215,
            2884718
        ]
    },
    {
        "id": 228341145,
        "title": "Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation",
        "abstract": "This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, 1 which included a translation task, a system combination task, and an eval-uation task. We conducted a large-scale manual evaluation of 104 machine trans-lation systems and 41 system combina-tion entries. We used the ranking of these systems to measure how strongly auto-matic metrics correlate with human judg-ments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk.",
        "date": "August 2010",
        "authors": [
            "Chris Callison-Burch",
            "Philipp Koehn",
            "Christof Monz",
            "Kay Peterson"
        ],
        "references": [
            262317479,
            242639553,
            242639442,
            239598038,
            238769089,
            234830723,
            234828193,
            234827876,
            261853661,
            234830134
        ]
    },
    {
        "id": 221013242,
        "title": "Re-evaluating Machine Translation Results with Paraphrase Support.",
        "abstract": "In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations. Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching. ParaEval addresses three important issues: support for para- phrase/synonym matching, recall meas- urement, and correlation with human judgments. We show that ParaEval corre- lates significantly better than BLEU with human assessment in measurements for both fluency and adequacy.",
        "date": "January 2006",
        "authors": [
            "Liang Zhou",
            "Chin-Yew Lin",
            "Eduard Hovy"
        ],
        "references": [
            228415375,
            220873702,
            220355462,
            2876877,
            238624214,
            224890498,
            220947104,
            220875040,
            220817071,
            2913503
        ]
    },
    {
        "id": 220874816,
        "title": "Better Word Alignments with Supervised ITG Models.",
        "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.",
        "date": "January 2009",
        "authors": [
            "Aria Haghighi",
            "John Blitzer",
            "John DeNero",
            "Dan Klein"
        ],
        "references": [
            221012875,
            220874741,
            237136150,
            230875959,
            228621909,
            222808307,
            221996415,
            221013045,
            221012709,
            220874577
        ]
    },
    {
        "id": 220874513,
        "title": "Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation",
        "abstract": "We describe Joshua, an open source toolkit for statistical machine transla- tion. Joshua implements all of the algo- rithms required for synchronous context free grammars (SCFGs): chart-parsing, n- gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and dis- tributed computing techniques for scala- bility. We demonstrate that the toolkit achieves state of the art translation per- formance on the WMT09 French-English translation task.",
        "date": "March 2009",
        "authors": [
            "Zhifei Li",
            "Chris Callison-Burch",
            "Chris Dyer",
            "Juri Ganitkevitch"
        ],
        "references": [
            241185825,
            238769089,
            220874716,
            268237865,
            252044237,
            240977328,
            238627205,
            237136150,
            228628171,
            221013045
        ]
    },
    {
        "id": 220816978,
        "title": "The Best Lexical Metric for Phrase-Based Statistical MT System Optimization.",
        "abstract": "Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the re- sulting model. We train a state-of-the-art MT system using MERT on many parameteriza- tions of each metric and evaluate the result- ing models on the other metrics and also us- ing human judges. In accordance with popular wisdom, we find that it's important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model's struc- ture and features allow it to take advantage of the metric. Contrasting with TER's good cor- relation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human prefer- ences for METEOR trained models varies de- pending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and per- form well in human judgments, we conclude they are still the best choice for training.",
        "date": "January 2010",
        "authors": [
            "Daniel Cer",
            "Christopher D. Manning",
            "Daniel Jurafsky"
        ],
        "references": [
            252666388,
            239051244,
            237561591,
            228875053,
            228668187,
            228355560,
            228340500,
            285905347,
            245329863,
            234812513
        ]
    },
    {
        "id": 220419015,
        "title": "Measuring machine translation quality as semantic equivalence: A metric based on entailment features",
        "abstract": "Current evaluation metrics for machine translation have increasing diculty in distinguishing good from merely fair translations. We believe the main problem to be their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that assesses the quality of MT output through its semantic equivalence to the reference translation, based on a rich set of match and mismatch features motivated by textual entailment. We rst evaluate this metric in an evaluation setting against a combination metric of four state-of-the-art scores. Our metric predicts human judgments better than the combination metric. Combining the entailment and traditional features yields further improvements. Then, we demonstrate that the entailment metric can also be used as learning criterion in minimum error rate training (MERT) to improve parameter estimation in MT system training. A manual evaluation of the resulting translations indicates that the new model obtains a signicant improvement in translation quality.",
        "date": "September 2009",
        "authors": [
            "Sebastian Pad\u00f3",
            "Daniel Cer",
            "Michel Galley",
            "Dan Jurafsky"
        ],
        "references": [
            319395280,
            241185825,
            285704200,
            269033639,
            267953871,
            257467254,
            244435287,
            242354127,
            241662987,
            238650200
        ]
    },
    {
        "id": 2588204,
        "title": "BLEU: a Method for Automatic Evaluation of Machine Translation",
        "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused.",
        "date": "October 2002",
        "authors": [
            "Kishore Papineni",
            "Salim Roukos",
            "Todd Ward",
            "Wei Jing Zhu"
        ],
        "references": [
            228981081,
            228762954,
            248126684,
            245212015
        ]
    },
    {
        "id": 246929280,
        "title": "Neural-network and K-nearest-neighbor Classifiers",
        "abstract": "The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance.",
        "date": "January 1991",
        "authors": [
            "Jane Bromley"
        ],
        "references": []
    },
    {
        "id": 260869480,
        "title": "The Use of Multiple Measurements in Taxonomic Problems",
        "abstract": "",
        "date": "January 1936",
        "authors": [
            "R.A. Fisher"
        ],
        "references": []
    },
    {
        "id": 236736807,
        "title": "Estimation of Dependences Based on Empirical Data",
        "abstract": "",
        "date": "January 2006",
        "authors": [
            "Vladimir Vapnik",
            "Samuel Kotz"
        ],
        "references": [
            243649607,
            220566165,
            47805733,
            3697812,
            314848772,
            281328020,
            247706437,
            221945139,
            37596515,
            13249901
        ]
    },
    {
        "id": 38366305,
        "title": "Classification into two Multivariate Normal Distributions with Different Covariance Matrices",
        "abstract": "Linear procedures for classifying an observation as coming from one of two multivariate normal distributions are studied in the case that the two distributions differ both in mean vectors and covariance matrices. We find the class of admissible linear procedures, which is the minimal complete class of linear procedures. It is shown how to construct the linear procedure which minimizes one probability of misclassification given the other and how to obtain the minimax linear procedure; Bayes linear procedures are also discussed.",
        "date": "June 1962",
        "authors": [
            "T. W. Anderson",
            "R. R. Bahadur"
        ],
        "references": [
            243495668,
            243081927
        ]
    },
    {
        "id": 2439558,
        "title": "Handwritten Digit Recognition with a Back-Propagation Network",
        "abstract": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service. 1 INTRODUCTION The main point of this paper is to show that large back-propagation (BP) networks can be applied to real image-recognition problems without a large, complex preprocessing stage requiring detailed engineering. Unlike most previous work on the subject (Denker et al., 1989), the learning network is directly fed with images, rather than feature vectors, thus demonstrating the ability of BP networks to deal with large amounts of low level information. Previous work performed on simple digit images (Le Cun, 1989) showed that the architecture of the network s...",
        "date": "November 1997",
        "authors": [
            "Yann Lecun",
            "Bernhard E. Boser",
            "John Denker",
            "Don Henderson"
        ],
        "references": []
    },
    {
        "id": 243652967,
        "title": "The Proper Treatment of Connectionism",
        "abstract": "A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models. Higher-level analyses of these connectionist models reveal subtle relations to symbolic models. Parallel connectionist memory and linguistic processes are hypothesized to give rise to processes that are describable at a higher level as sequential rule application. At the lower level, computation has the character of massively parallel satisfaction of soft numerical constraints; at the higher level, this can lead to competence characterizable by hard rules. Performance will typically deviate from this competence since behavior is achieved not by interpreting hard rules but by satisfying soft constraints. The result is a picture in which traditional and connectionist theoretical constructs collaborate intimately to provide an understanding of cognition.",
        "date": "March 1988",
        "authors": [
            "Paul Smolensky"
        ],
        "references": [
            345402887,
            344118947,
            329652982,
            313083612,
            313063903,
            311869775,
            311256399,
            301064084,
            297181812,
            285007805
        ]
    },
    {
        "id": 221617942,
        "title": "Discovering the Structure of a Reactive Environment by Exploration",
        "abstract": "Consider a robot wandering around an unfamiliar environment, performing actions and observing the consequences. The robot's task is to construct an internal model of its environment, a model that will allow it to predict the effects of its actions and to determine what sequences of actions to take to reach particular goal states. Rivest and Schapire (1987a,b; Schapire 1988) have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of \u201cfinite state\u201d environments. The heart of this algorithm is a clever representation of the environment called an update graph. We have developed a connectionist implementation of the update graph using a highly specialized network architecture. With backpropagation learning and a trivial exploration strategy \u2014 choosing random actions \u2014 the connectionist network can outperform the Rivest and Schapire algorithm on simple problems. Our approach has additional virtues, including the fact that the network can accommodate stochastic environments and that it suggests generalizations of the update graph representation that do not arise from a traditional, symbolic perspective.",
        "date": "January 1989",
        "authors": [
            "Michael C. Mozer",
            "Jonathan Bachrach"
        ],
        "references": []
    },
    {
        "id": 313125996,
        "title": "Discovering the structure of a reactive environment by exploration",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "M. Mozer",
            "J. Bachrach"
        ],
        "references": []
    },
    {
        "id": 311569941,
        "title": "Faster-Learning Variations on Back-Propagation: An Empirical Study",
        "abstract": "",
        "date": "January 1988",
        "authors": [
            "Scott E. Fahlman"
        ],
        "references": []
    },
    {
        "id": 243786282,
        "title": "Serial order: A parallel distributed processing approach",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "Michael Jordan"
        ],
        "references": []
    },
    {
        "id": 243764382,
        "title": "Encoding input/output representations in connectionist cognitive systems",
        "abstract": "",
        "date": "January 1989",
        "authors": [
            "R. Miikkulainen",
            "M. G. Dyer"
        ],
        "references": []
    },
    {
        "id": 239059649,
        "title": "The Connectionist Construction of Concepts",
        "abstract": "",
        "date": "January 1990",
        "authors": [
            "Adrian Cussins"
        ],
        "references": []
    },
    {
        "id": 256446199,
        "title": "Training Neural Networks with Implicit Variance",
        "abstract": "We present a novel method to train predictive Gaussian dis-tributions p(z|x) for regression problems with neural networks. While most approaches either ignore or explicitly model the variance as an-other response variable, it is trained implicitly in our case. Establishing stochasticty by the injection of noise into the input and hidden units, the outputs are approximated with a Gaussian distribution by the forward propagation method introduced for fast dropout [1]. We have designed our method to respect that probabilistic interpretation of the output units in the loss function. The method is evaluated on a synthetic and a inverse robot dynamics task, yielding superior performance to plain neu-ral networks, Gaussian processes and LWPR in terms of mean squared error and likelihood.",
        "date": "November 2013",
        "authors": [
            "Justin Bayer",
            "Christian Osendorfer",
            "Sebastian Urban",
            "Patrick van der Smagt"
        ],
        "references": [
            329650090,
            319770183,
            312623183,
            312370418,
            303269133,
            291735318,
            287034172,
            286271944,
            280940018,
            279351140
        ]
    },
    {
        "id": 261159435,
        "title": "On rectified linear units for speech processing",
        "abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",
        "date": "May 2013",
        "authors": [
            "M.D. Zeiler",
            "M. Ranzato",
            "R. Monga",
            "M. Mao"
        ],
        "references": [
            266225209,
            255564382,
            228871422,
            220320677,
            215616967,
            47438419,
            319770387,
            304109482,
            288188248,
            267960550
        ]
    },
    {
        "id": 319770174,
        "title": "Practical recommendations for gradient-based training of deep architectures",
        "abstract": "Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.",
        "date": "June 2012",
        "authors": [
            "Y. Bengio"
        ],
        "references": []
    },
    {
        "id": 288023219,
        "title": "A Practical Guide to Support Vector Classification",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "C.-W. Hsu",
            "C.-C. Chang",
            "C.-J. Lin"
        ],
        "references": []
    },
    {
        "id": 271528918,
        "title": "The Selfish Gene",
        "abstract": "",
        "date": "January 1976",
        "authors": [
            "R Dawkins"
        ],
        "references": []
    },
    {
        "id": 270663502,
        "title": "Random Forests",
        "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, \u2217\u2217\u2217, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
        "date": "January 2001",
        "authors": [
            "Leo Breiman"
        ],
        "references": []
    },
    {
        "id": 258304769,
        "title": "Machine Learning, Volume 45, Number 1 - SpringerLink",
        "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
        "date": "October 2001",
        "authors": [
            "Leo Breiman"
        ],
        "references": [
            290824736,
            220499641,
            2463897,
            265444019,
            263453249,
            248256880,
            244498683,
            226270700,
            3193103,
            3192880
        ]
    },
    {
        "id": 313060232,
        "title": "Learning Internal Representations by Error Propagation",
        "abstract": "",
        "date": "January 1986",
        "authors": [
            "D.E. Rummelhart",
            "G.E. Hinton",
            "R.J. Williams"
        ],
        "references": []
    },
    {
        "id": 261466508,
        "title": "Polyphonic piano note transcription with recurrent neural networks",
        "abstract": "In this paper a new approach for polyphonic piano note onset transcription is presented. It is based on a recurrent neural network to simultaneously detect the onsets and the pitches of the notes from spectral features. Long Short-Term Memory units are used in a bidirectional neural network to model the context of the notes. The use of a single regression output layer instead of the often used one-versus-all classification approach enables the system to significantly lower the number of erroneous note detections. Evaluation is based on common test sets and shows exceptional temporal precision combined with a significant boost in note transcription performance compared to current state-of-the-art approaches. The system is trained jointly with various synthesized piano instruments and real piano recordings and thus generalizes much better than existing systems.",
        "date": "May 2012",
        "authors": [
            "Sebastian B\u00f6ck",
            "Markus Schedl"
        ],
        "references": [
            264871781,
            248876741,
            243055458,
            236476076,
            224929665,
            224091857,
            26620182,
            13853244,
            3457682,
            3424207
        ]
    },
    {
        "id": 230876435,
        "title": "Learning Internal Representation by Error Propagation",
        "abstract": "This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords: Learning; networks; Perceptrons; Adaptive systems; Learning machines; and Back propagation",
        "date": "January 1986",
        "authors": [
            "David E. Rumelhart",
            "Geoffrey E. Hinton",
            "Ronald J. Williams"
        ],
        "references": []
    },
    {
        "id": 224576777,
        "title": "Note onset detection for the transcription of polyphonic piano music",
        "abstract": "Transcription of music is the process of generating a symbolic representation such as a score sheet or a MIDI file from an audio recording of a piece of music. A statistical machine learning approach for detecting note onsets in polyphonic piano music is presented. An area from the spectrogram of the sound is concatenated into one feature vector. A cascade of boosted classifiers is used for dimensionality reduction and classification in an one-versus-all manner. The presented system achieves an accuracy of 87.4% in onset detection outperforming the best comparison system by 25.1 %.",
        "date": "August 2009",
        "authors": [
            "C.G.v.d. Boogaart",
            "R. Lienhart"
        ],
        "references": [
            228776646,
            26620182,
            4105194,
            3457995,
            3424207,
            2552483,
            245123229,
            224641478,
            221114425,
            200806158
        ]
    },
    {
        "id": 3422935,
        "title": "Python for Scientific Computing",
        "abstract": "Python is an interpreted language with expressive syntax, which transforms itself into a high-level language suited for scientific and engineering code. Some of its features include a liberal open source license, ability to run on many platforms, powerful interactive interpreter, ability to expand with earlier compiled code, ability to interact with a wide variety of other software, and a large number of library modules. An important factor in the utility of Python as a computing language is its clear syntax, which can make code easy to understand and maintain. This language also contributes to the construction of maintainable code by separating code into logical groups such as modules, class, and functions, in addition to offering clean syntax. Python can be easily extended with a large C-API for calling Python functionality from C programming language, connecting to non-Python compiled code, and extending the language itself by creating new Python types in C language.",
        "date": "June 2007",
        "authors": [
            "Travis Oliphant"
        ],
        "references": [
            265236067,
            213877848
        ]
    },
    {
        "id": 221344668,
        "title": "Learning Recurrent Neural Networks with Hessian-Free Optimization",
        "abstract": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-theart method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens. 1.",
        "date": "January 2011",
        "authors": [
            "James Martens",
            "Ilya Sutskever"
        ],
        "references": [
            277298117,
            243781690,
            224685591,
            221620610,
            13853244,
            12292425,
            11294744,
            5583935,
            2822332,
            285906939
        ]
    },
    {
        "id": 213877848,
        "title": "SciPy: Open Source Scientific Tools for Python",
        "abstract": "SciPy is a Python-based ecosystem of open-source software for mathematics, science, and engineering. See http://www.scipy.org/ .",
        "date": "January 2001",
        "authors": [
            "Eric Jones",
            "Travis Oliphant",
            "Pearu Peterson"
        ],
        "references": [
            328775640,
            326888435,
            320438372,
            312035008,
            310329000,
            308715485,
            293886905,
            267157482,
            266044565,
            264197576
        ]
    },
    {
        "id": 3422938,
        "title": "IPython: A System for Interactive Scientific Computing",
        "abstract": "The IPython project provides an enhanced interactive environment for scientific computing, with features including support data visualization and facilities for distributed and parallel computation. The most important characteristic of scientific computing is a collection of high-performance code written in FORTRAN, C language, and C++ that runs in batch mode on large systems, clusters, and superconductors. The IPython project aims to provide a greatly enhanced Python shell, facilities for interactive distributed and parallel computing, and comprehensive set of tools for building special-purpose interactive environments for scientific computing. This project has been providing tools to extend Python's interactive capabilities and continues to be developed as a base layer for new interactive environments. It offers a set of control commands designed to improve Python's usability in an interactive environment.",
        "date": "June 2007",
        "authors": [
            "Fernando Huitron Perez",
            "Brian E. Granger"
        ],
        "references": [
            252460722,
            234238535,
            220439974,
            278915721,
            228630085,
            228348645,
            213877848,
            3734423,
            2386149
        ]
    },
    {
        "id": 221618747,
        "title": "Non-Local Manifold Tangent Learning.",
        "abstract": "We claim and present arguments to the effect that a large class of man- ifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation sug- gests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to general- ize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails.",
        "date": "January 2004",
        "authors": [
            "Y. Bengio",
            "Martin Monperrus"
        ],
        "references": [
            221618279,
            220500224,
            285599267,
            284035345,
            224881862,
            221620483,
            221618315,
            221345182,
            216792842,
            12204039
        ]
    },
    {
        "id": 285599267,
        "title": "Charting a Manifold",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "M Brand"
        ],
        "references": []
    },
    {
        "id": 228338321,
        "title": "Algorithms for manifold learning",
        "abstract": "Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensional-ity of many data sets is only artificially high; though each data point consists of perhaps thousands of fea-tures, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Man-ifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional repre-sentation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embed-ding, Laplacian Eigenmaps, Semidefinite Embedding, and a host of variants of these algorithms are exam-ined.",
        "date": "July 2005",
        "authors": [
            "Lawrence Cayton"
        ],
        "references": [
            256821709,
            285599320,
            270151773,
            246365307,
            243768003,
            2931928
        ]
    },
    {
        "id": 223450457,
        "title": "Application of distributed SVM architectures in classifying forest data cover types",
        "abstract": "In many \u2018real-world\u2019 applications, a classification of large data sets, which are often also imbalanced, is difficult due to the small, but usually more interesting classes. In this study, a large data set, forest cover type classes, which is actually multi-class classification defined with seven imbalanced classes and used as a resource inventory information was analyzed and evaluated. The data set was transformed into seven new data sets and a support vector machine (SVM) was employed to solve a binary classification problem of balanced and imbalanced data sets with various sizes. In the two approaches considered, the use of distributed SVM architectures, which basically reduces the complexity of the quadratic optimization problem of very large data sets, and the use of two sampling approaches for classification of imbalanced data sets were combined and results presented. The experimental results of distributed SVM architectures show the improvement of the accuracy for larger data sets in comparison to a single SVM classifier and their ability to improve the correct classification of the minority class.",
        "date": "October 2008",
        "authors": [
            "Mira Trebar",
            "Nigel Steele"
        ],
        "references": [
            243763293,
            234786663,
            228084517,
            222499246,
            221619261,
            220894899,
            220520061,
            220520041,
            37432989,
            3974361
        ]
    },
    {
        "id": 221619959,
        "title": "Sample Complexity of Testing the Manifold Hypothesis.",
        "abstract": "",
        "date": "January 2010",
        "authors": [
            "Hariharan Narayanan",
            "Sanjoy Mitter"
        ],
        "references": []
    },
    {
        "id": 238835966,
        "title": "Neural Network Music Composition by Prediction: Exploring the Benefits of Psychoacoustic Constraints and Multi-scale Processing",
        "abstract": "In algorithmic music composition, a simple technique involves selecting notes sequentially according to a transition table that specifies the probability of the next note as a function of the previous context. An extension of this transition-table approach is described, using a recurrent autopredictive connectionist network called CONCERT. CONCERT is trained on a set of pieces with the aim of extracting stylistic regularities. CONCERT can then be used to compose new pieces. A central ingredient of CONCERT is the incorporation of psychologically grounded representations of pitch, duration and harmonic structure. CONCERT was tested on sets of examples artificially generated according to simple rules and was shown to learn the underlying structure, even where other approaches failed. In larger experiments, CONCERT was trained on sets of J. S. Bach pieces and traditional European folk melodies and was then allowed to compose novel melodies. Although the compositions are occasionally pleasant, and are preferred over compositions generated by a third-order transition table, the compositions suffer from a lack of global coherence. To overcome this limitation, several methods are explored to permit CONCERT to induce structure at both fine and coarse scales. In experiments with a training set of waltzes, these methods yielded limited success, but the overall results cast doubt on the promise of note-by-note prediction for composition.",
        "date": "January 1994",
        "authors": [
            "Michael C. Mozer"
        ],
        "references": [
            265896254,
            345402887,
            324296605,
            300332864,
            284393430,
            280300059,
            275848245,
            272161465,
            270819579,
            270819327
        ]
    },
    {
        "id": 221664805,
        "title": "Conditional Restricted Boltzmann Machines for Structured Output Prediction",
        "abstract": "Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.",
        "date": "February 2012",
        "authors": [
            "Volodymyr Mnih",
            "Hugo Larochelle",
            "Geoffrey E. Hinton"
        ],
        "references": [
            221346359,
            242793084,
            222451035,
            222430151,
            221620295,
            221619845,
            221619768,
            221346352,
            221346268,
            221346249
        ]
    },
    {
        "id": 314823957,
        "title": "Polyphonic music modeling with random fields",
        "abstract": "",
        "date": "January 2003",
        "authors": [
            "Victor Lavrenko",
            "Jeremy Pickens"
        ],
        "references": []
    }
]